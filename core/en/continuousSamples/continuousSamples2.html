<!DOCTYPE HTML>
<html>
<head>
	<title>Distribution of sample sum and mean</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="random sample, mean of random sample, sum of random sample, central limit theorem">
</head>


<body>
<script type="text/javascript">writePageStart();</script>

<p class="heading">Properties of combinations of random variables</p>
<p>We now repeat a few results that were <a href="javascript:showNamedPage('sec_randomSample')">given earlier</a> for discrete random variables â€” they actually hold for all variables, whether discrete or continuous. Formal proofs for continuous variables are harder so we simply repeat the results in their more general form without proof.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Linear combination of independent variables</p>
		<p>If the means of two independent  random variables, \(X\) and \(Y\), are \(\mu_X\) and \(\mu_Y\) and their variances are \(\sigma_X^2\) and \(\sigma_Y^2\), then the linear combination \((aX + bY)\) has mean and variance</p>
		\[ \begin {align}
		E[aX + bY] &amp; = a\mu_X + b\mu_Y \\[0.4em]
		\Var(aX + bY) &amp; = a^2\sigma_X^2 + b^2\sigma_Y^2
		\end {align} \] </div>
</div>
<p>We next give formulae for the mean and variance of the sum of a random sample.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Sum of a random sample</p>
		<p>If \(\{X_1, X_2, ..., X_n\}\) is a random sample of <em>n</em> values from any  distribution with mean \(\mu\) and variance \(\sigma^2\), then the sum of the values has mean and variance</p>
		\[\begin{aligned}
		E\left[\sum_{i=1}^n {X_i}\right] &amp; \;=\; n\mu \\
		\Var\left(\sum_{i=1}^n {X_i}\right) &amp; \;=\; n\sigma^2
	\end{aligned} \] </div>
</div>
<p>The earlier results for the <strong>mean</strong> of a random sample also hold for samples from both discrete and continuous distributions.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Sample mean</p>
		<p>If \(\{X_1, X_2, ..., X_n\}\) is a random sample of <em>n</em> values from any  distribution with mean \(\mu\) and variance \(\sigma^2\), then the sample mean has a distribution with mean and variance</p>
		\[\begin{aligned}
		E\big[\overline{X}\big] &amp; \;=\; \mu \\
		\Var\big(\overline{X}\big) &amp; \;=\; \frac {\sigma^2} n
		\end{aligned} \] </div>
</div>
<p>Finally, the <a href="javascript:showNamedPage('continuousDistns7')">Central Limit Theorem</a> holds for random samples from <strong>all</strong> distributions, whether discrete or continuous. It is repeated here, again without proof.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Central Limit Theorem (informal)</p>
		<p>If \(\{X_1, X_2, ..., X_n\}\) is a random sample of <em>n</em> values from any distribution with mean \(\mu\) and variance \(\sigma^2\),</p>
		\[\begin{aligned}
		\sum_{i=1}^n {X_i} &amp; \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \NormalDistn(n\mu, \;\;\sigma_{\Sigma X}^2=n\sigma^2) \\
		\overline{X} &amp; \;\; \xrightarrow[n \rightarrow \infty]{} \; \; \NormalDistn(\mu, \;\;\sigma_{\overline X}^2 = \frac {\sigma^2} n)
		\end{aligned} \] </div>
</div>
<script type='text/javascript'>writePageEnd();</script>

</body>
</html>
