<!DOCTYPE HTML>
<html>
<head>
	<title>Independence and random samples</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="joint probability density function, independence, random sample">
</head>


<body>
<script type="text/javascript">writePageStart();</script>
<p>Two random variables, \(X\) and \(Y\), are independent when all events about \(X\) are independent of all events about \(Y\). The following result for continuous random variables is similar to that for discrete variables.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Independence</p>
		<p><span class="definition">Two continuous random variables,<em> X</em>, and<em> Y</em>, are independent if</span> and only if</p>
		\[
		f(x, y) = f_X(x) \times f_Y(y) \qquad \text{ for all } x \text{ and } y
		\] </div>
</div>
<p>If \(X\) and \(Y\) are independent, then the conditional pdf of \(X\), given that \(Y = y\) is equal to its marginal pdf.</p>
\[
f_{X\mid Y=y}(x) \;\;=\;\; \frac {f(x,y)}{f_Y(y)} \;\;=\;\;  f_X(x)
\]
<p>If the variables are independent,  knowing the value of \(Y\) gives no information about the distribution of \(X\). Similarly,</p>
\[
f_{Y\mid X=x}(y) \;\;=\;\;  f_Y(y)
\]

<p class="heading">Determining independence</p>
<p>We can sometimes deduce mathematically that two variables are independent by factorising their joint pdf, but independence is more often justified by the context from which the two variables were defined.</p>
<div class="example">
	<p class="exampleHeading">Failure of light bulbs</p>
	<p>If two light bulbs are tested at 80ºC until failure, their failure times \(X\) and \(Y\) can be assumed to be independent — failure of one bulb would not influence when the other failed. If the distribution for a single light bulb is \(\ExponDistn(\lambda)\), there joint pdf would therefore be</p>
\[
		f(x, y) = f_X(x) \times f_Y(y) = \left(\lambda e^{\lambda x}\right)\left(\lambda e^{\lambda y}\right) = \lambda^2 e^{\lambda(x+y)}\qquad \text{ if } x \ge 0\text{ and } y \ge 0
\]
</div>

<p class="heading">Extensions to 3 or more variables</p>
<p>The idea of a joint probability density function for three or more continuous random variables  \(\{X_1,X_2,\dots, X_n\}\) is a simple extension of that for two variables,</p>
\[
f(x_1, \dots, x_n) 
\]
<p>Probabilities can be obtained from the joint pdf as multiple integrals over the corresponding values of the variables, \((x_1, \dots, x_n)\), but we will not go into the details  here.</p>
<p class="heading">Random samples</p>
<p>A collection of \(n\) <strong>independent</strong> random variables with the <strong>same</strong> distribution is a <strong>random sample</strong> from the distribution. The joint pdf of the variables is then</p>
\[
f(x_1, \dots, x_n) \;=\; \prod_{i=1}^n f(x_i)
\]
<p>where \(f(\cdot)\) is the pdf of the distribution from which the random sample is taken. When treated as a function of unknown parameters, this is the likelihood function for the sample data.</p>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
