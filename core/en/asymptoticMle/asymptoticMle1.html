<!DOCTYPE HTML>
<html>
<head>
	<title>Bias, variance and normality</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="bias, variance, normal distribution, maximum likelihood">
</head>


<body>
<script type="text/javascript">writePageStart();</script>
<p class="heading">Asymptotic properties of maximum likelihood estimators</p>
<p>An  important reason for the importance of maximum likelihood estimators is that they have very good large-sample properties.</p>

<p>We now describe some properties of  the maximum likelihood estimator, \(\hat {\theta} \), of a parameter, \(\theta\), that is based on a random sample of size \(n\) from a distribution with probability function \(p(x \mid \theta)\).</p>

<div class="boxed">
<p>The  properties strictly require certain &quot;regularity conditions&quot; to be satisfied. We will omit the conditions since they hold for the kinds of model that we will consider in this e-book.</p>
</div>

<p>Proofs of the  properties are complex and will also be  omitted.</p>

<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Bias</p>
		<p>A maximum likelihood estimator, \(\hat {\theta} \), of a parameter, \(\theta\), is asymptotically unbiased, </p>
\[
		E[\hat {\theta}] \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \theta
\]
</div>
</div>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Variance and consistency</p>
		<p>A maximum likelihood estimator, \(\hat {\theta} \), of a parameter, \(\theta\), asymptotically has variance,</p>
\[
\Var(\hat {\theta}) \;\; \xrightarrow[n \rightarrow \infty]{} \;\; - \frac 1 {n \times E\left[\large\frac {d^2 \log\left(p(X \;|\; \theta)\right)} {d\theta^2} \right]}
\]
		<p>Since this tends to zero as \(n \rightarrow \infty\) and the bias is asymptotically zero, a maximum likelihood estimator is also consistent.</p>

</div>
</div>
<div class="theoremProof">
	<div class="theorem">
<p class="theoremTitle">Asymptotic normality</p>
<p>A maximum likelihood estimator, \(\hat {\theta} \), of a parameter, \(\theta\), asymptotically has a normal distribution, </p>

\[
\hat {\theta} \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \text{a normal distribution}
\]</div>
</div>


<p>We now express these three properties together in a slightly more formal way that is clearer about what is meant by the limits above.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">All together</p>
		<p>If \(\hat {\theta} \) is the  maximum likelihood estimator of a parameter, \(\theta\), based on a random sample of size \(n\),</p>
		\[
		(\hat {\theta} - \theta) \times \sqrt {-n \times E\left[\frac {d^2\; \log\left(p(X \;|\; \theta)\right)} {d\;\theta^2} \right]} \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \NormalDistn(0, 1)
		\] </div>
</div>
<p>A final result states that a maximum likelihood estimator cannot be beaten in large samples.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Asymptotically "best"</p>
		<p>Other estimators of a parameter, \(\theta\),  may have lower mean squared errors in small samples, but none have  lower mean squared error than the maximum likelihood estimator if the sample size is large enough.</p>
	</div>
</div>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
