<!DOCTYPE HTML>
<html>
<head>
	<title>Bias, variance and normality</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="bias, variance, normal distribution, maximum likelihood">
</head>


<body>
<script type="text/javascript">writePageStart();</script>
<p>Maximum likelihood estimators  have very good <strong>large-sample</strong> properties.</p>
<div class="boxed">
	<p>These strictly require certain &quot;regularity conditions&quot; to be satisfied. We will avoid them in this e-book — they almost always hold.</p>
</div>

<p>The following results apply to the maximum likelihood estimator, \(\hat {\theta}\) of a parameter \(\theta\), based on a random sample of size \(n\) when \(n \to \infty\) — i.e. its <strong>asymptotic</strong> properties.</p>

<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Bias</p>
		<p>It is asymptotically unbiased, </p>
\[
		E[\hat {\theta}] \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \theta
\]
</div>
</div>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Variance and consistency</p>
		<p>It asymptotically has variance,</p>
\[
\Var(\hat {\theta}) \;\; \xrightarrow[n \rightarrow \infty]{} \;\; - \frac 1 {n \times E\left[\large\frac {d^2 \log\left(p(X \;|\; \theta)\right)} {d\theta^2} \right]}
\]
		<p>Since this tends to zero as \(n \rightarrow \infty\) and the bias is asymptotically zero, a maximum likelihood estimator is also consistent.</p>

</div>
</div>
<div class="theoremProof">
	<div class="theorem">
<p class="theoremTitle">Asymptotic normality</p>
<p>It asymptotically has a normal distribution, </p>

\[
\hat {\theta} \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \text{a normal distribution}
\]</div>
</div>


<p>We now express these three properties together in a slightly more formal way.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">All together</p>
		<p>If \(\hat {\theta} \) is the  maximum likelihood estimator of a parameter, \(\theta\), based on a random sample of size \(n\),</p>
		\[
		(\hat {\theta} - \theta) \times \sqrt {-n \times E\left[\frac {d^2\; \log\left(p(X \;|\; \theta)\right)} {d\;\theta^2} \right]} \;\; \xrightarrow[n \rightarrow \infty]{} \;\; \NormalDistn(0, 1)
		\] </div>
</div>
<p>A final result states that a maximum likelihood estimator cannot be beaten in large samples.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Asymptotically "best"</p>
		<p>Other estimators of a parameter, \(\theta\),  may have lower mean squared errors in small samples, but none have  lower mean squared error than the maximum likelihood estimator if the sample size is large enough.</p>
	</div>
</div>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
