<!DOCTYPE HTML>
<html>
<head>
	<title>Maximising the likelihood</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="maximum likelihood, log-likelihood, binomial distribution">
</head>


<body>
<script type="text/javascript">writePageStart();</script>

<p class="heading">Maximum likelihood estimate</p>

<p>The likelihood function, \(L(\theta \; | \; x_1, x_2, \dots, x_n) = p(x_1, x_2, \dots, x_n \;| \; \theta)\), gives the probability of getting the data that were recorded for different values of the unknown parameter \(\theta\). A value of \(\theta\) that gives the observed data high probability is more likely to be correct than one that would make the observed data unlikely.</p>

<div class="definition">
<p class='definitionTitle'>Definition</p>
<p>The <strong>maximum likelihood estimate</strong> of a parameter \(\theta\) is the value that maximises the likelihood function,</p>
\[
L(\theta \; | \; x_1, x_2, \dots, x_n) = p(x_1, x_2, \dots, x_n \;| \; \theta)
\]</div>

<p>Finding a maximum likelihood estimate (MLE) is therefore the mathematical problem of maximising a function of the parameter \(\theta\). This is usually a &quot;turning point&quot; of the likelihood function and calculus provides a method to find the maximum likelihood estimator.</p>
<div class="theoremProof">
	<div class="theorem">
<p class="theoremTitle">Finding the maximum likelihood estimate</p>
<p>The maximum likelihood estimate of a parameter \(\theta\) can normally be obtained as a solution to the equation</p>
\[
\frac {d\; L(\theta \; | \; x_1, x_2, \dots, x_n)} {d\; \theta} \;\; = \;\; 0
\]	</div>
</div>

<p>Although this equation can be solved to find a maximum likelihood estimate, it is often easier mathematically to maximise the logarithm of the likelihood function rather than the likelihood function itself.</p>
\[
\ell(\theta) \;=\; \log L(\theta)
\]
<p>The following result explains why this is equivalent.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Maximising the log-likelihood</p>
		<p>The maximum likelihood estimate of a parameter \(\theta\) can normally be found by solving the equation</p>
		\[
		\frac {d\; \log L(\theta \; | \; x_1, x_2, \dots, x_n)} {d\; \theta} \;\; = \;\; 0
		\] 
		</div>
		<div class="proof">
			<p>Using the chain rule to differentiate \(\log L(\theta)\),</p>
\[
		\frac {d\; \log L(\theta)} {d\; \theta}  \;\;=\;\; \frac 1 {L(\theta)} \times \frac {d\; L(\theta)} {d\; \theta}
		\]
			<p>Since the likelihood \(L(\theta)\) is a probability, its value is between 0 and 1. Therefore \({d\; L(\theta)} / {d\; \theta}\) can only be zero if \({d\; \log L(\theta)} / {d\; \theta}\) is zero.</p>
	</div>
</div>
<p>We will now give a simple example.</p>


<div class="example" title="Example: Maximum likelihood for binomial distribution">
	<p class="exampleHeading">A simple binomial example</p>
	<p>Consider a random variable \(X\) that is the number of successes in \(n=20\) independent trials, each of which has probability \(\pi\) of success. If  the experiment resulted in \(x=6\) successes, the likelihood function would be</p>
	\[
	L(\pi) = {{20} \choose 6} \pi^6(1-\pi)^{20-6} \;\; = \;\;38,760 \; \times 
	\pi^6(1-\pi)^{14} \]
	<p>The maximum likelihood estimator of \(\pi\) is the solution to the equation</p>
\[
	\frac {d \; L(\pi)} {d\; \pi} \;\; = \;\;38,760 \;\times \frac {d} {d\; \pi} \left(\pi^6(1-\pi)^{14} \right) = 0\]
	<p>This differentiation is relatively difficult, but the method can be simplified if we instead differentiate the log-likelihood,</p>
\[
	\ell(\pi) \;\; = \;\; \log L(\pi) \;\; = \;\; 6 \log(\pi) + 14 \log(1 - \pi) + K\]
	<p>where \(K\) is a constant that does not depend on \(\pi\). The maximum likelihood estimate can therefore be found by solving</p>
\[
	\frac {d \; \ell(\pi)} {d\; \pi} \;\; = \;\; \frac 6 {\pi} - \frac {14} {1 - \pi} \;\; = \;\; 0 \\[0.5em]
	6(1-\pi) = 14\pi \\[0.5em]
	6 = 20\pi
\]
	<p>The maximum likelihood estimate of \(\pi\) is therefore \(
	\hat {\pi} = \frac 6 {20} \), the sample proportion of successes.</p>
	
	<hr width="75%">
	
	<p>This can be generalised to a binomial experiment in which \(x\) successes are observed in \(n\) trials, so</p>
\[
	\ell(\pi) \; = \; \log L(\pi) \; = \; x \log(\pi) + (n-x) \log(1 - \pi) + K(n, x) \\[0.4em]
	\frac {d \; \ell(\pi)} {d\; \pi} \; = \; \frac x {\pi} - \frac {n-x} {1 - \pi} \; = \; 0
\]
	<p>which can be solved to give</p>
\[
	\hat {\pi} \;=\; \frac x n
\]


<hr width="75%">

<p>The diagram below illustrates the method. The top half shows the likelihood function, whereas the bottom half shows the log-likelihood.</p>

<div class="centred"> 
<applet codebase="../../java" code="dataView.CastApplet.class" archive="coreCAST.jar" width="500" height="500">
<script type="text/javascript">writeAppletParams();</script>
<param name="appletName" value="estimationProg.LogLikelihoodApplet">
<param name="backgroundColor" value="FFFBF3">
<param name="binom" value="20 6 0.400">
<param name="likelihoodAxis" value="0 0.2 0 0.05">
<param name="loglikelihoodAxis" value="-6 -1.6 -6 1">
<param name="customText" value="Likelihood=Likelihood#Log-likelihood=Log-likelihood#Unknown parameter=Unknown parameter#Probability of success=Probability of success#Max likelihood=Max likelihood">
</applet> 
</div>

<p><strong>Drag the slider</strong> and observe that the value of \(\pi\) that maximises the likelihood is the same as the one that maximises the log-likelihood.</p>
<p>The green lines show the slopes of the two curves at the currently selected value of \(\pi\) â€” the derivatives of the two functions. Click <strong>Max likelihood</strong> to display the maximum likelihood estimator and observe that this is the value corresponding to zero slope (and first derivative) for both the likelihood function and the log-likelihood.</p>
</div>

<p>Note that we are using <strong>natural </strong> logarithms (base-e) here, not logarithms to the base 10. Some textbooks use the notation &quot;\(\ln (x)\)&quot; to denote a natural logarithm and Excel uses the function &quot;=LN(x)&quot;, but we will use &quot;\(\log (x)\)&quot; here.</p>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
