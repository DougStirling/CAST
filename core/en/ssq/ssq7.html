<!DOCTYPE HTML>
<html>
<head>
	<title>Summary of anova distributions</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>

	<meta name="index" content="chi-squared distribution, f ratio, f test, f distribution, analysis of variance, anova, sum of squares, mean sum of squares">
</head>


<body>
<script type="text/javascript">writePageStart();</script>

<p>This page summarises the most important results from this section.</p>
  
<div class="centred"><div class="boxed">
<p>The terminology and results below underly many 
            statistical methods in this and later chapters.</p>
</div></div>

<p class="heading">Sums of squares</p>
  <p><strong>Sums of squares</strong> have distributions that are proportional 
    to chi-squared distributions. The shapes of these chi-squared distributions 
    depend on constants called their <strong>degrees of freedom</strong>.</p>
<p class="eqn"><img src="images/ssqGeneralDistn.gif" width="163" height="26"></p>
<p>For example, the sum of squares round µ 
has a chi-squared distribution with <em>n</em> degrees of freedom whereas the 
sum of squares round the sample mean has (<em>n</em>&nbsp;-&nbsp;1) degrees of 
freedom.</p>

<p class="heading">Mean sums of squares</p>
<p>Since a raw sum of squares has a chi-squared distribution whose mean is proportional 
to its degrees of freedom, we define the <strong>mean sum of squares</strong> 
by dividing the sum of squares by its degrees of freedom,</p>
<p class="eqn"><img src="images/msqGeneralDistn.gif" width="228" height="36"></p>
<p>The mean sum of squares has a distribution whose shape is also proportional 
to a chi-squared distribution, but its mean is now σ<sup>2</sup>,</p>
<p class="eqn"><img src="images/msqGeneralMean.gif" width="90" height="27"></p>
<p>The mean sum of squares is therefore an <strong>unbiased</strong> estimator 
of σ<sup>2</sup>.</p>

<div class="centred"><div class="boxed">
<p>The sample variance is an example of a mean sum 
of squares.</p>
</div></div>

<p class="heading">F ratio</p>
  <p>If we have two independent mean sums of squares involving the same σ<sup>2</sup>, 
    their ratio has an F distribution whose degrees of freedom are those of the 
    numerator and denominator. Note that this F distribution does not involve 
    any unknown parameters.</p>
<p class="eqn"><img src="images/fGeneralDistn.gif" width="292" height="64"></p>

<div class="centred"><div class="boxed">
<p>The ratio of two sample variances is an example, 
provided the two samples are from normal distributions with the same σ<sup>2</sup>.</p>
</div></div>

  <p class="heading">Hypothesis testing</p>
  <p>Since F ratios have distributions that do not involve unknown parameters 
    (at least when some assumptions are made about the model underlying the data), 
    they can be used as test statistics.</p>
  <p>The observed value of the F ratio can be compared to this F distribution 
    to assess a hypothesis about the underlying model &mdash; were the assumptions 
    valid?</p>
  
<script type='text/javascript'>writePageEnd();</script>

</body>
</html>
