<!DOCTYPE HTML>
<html>
<head>
	<title>Maximum likelihood</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="estimation, likelihood function, maximum likelihood">
</head>


<body>
<script type="text/javascript">writePageStart();</script>

<p class="heading">Likelihood function</p>

<p>Unlike the method of moments, the concept of maximum likelihood estimation can be used in the same way with models that have <strong>any</strong> number of parameters. We will describe the method for a model with two unknown parameter, \(\theta\) and \(\phi\), but it should be clear how to extend it to three or more parameters.</p>
<p>The concept of the <a href="javascript:showNamedPage('maxLikelihood1')">likelihood function</a> is the same as for models with a single unknown parameter. If \(\{x_1, x_2, \dots, x_n\}\) is a random sample from a discrete distribution with probability function \(p(x\;|\; \theta, \phi)\), the likelihood function gives the probability of getting the observed data for any values of the parameters.</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; p(x_1, x_2, \dots, x_n \;| \; \theta, \phi) \;\;=\;\; \prod_{i=1}^n {p(x_i\;|\; \theta, \phi)}
\]
<p>For a random sample from a continuous distribution, the corresponding definition is</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; \prod_{i=1}^n {f(x_i\;|\; \theta, \phi)}
\]
<p class="heading">Maximising the likelihood</p>
<p>Maximum likelihood again chooses the parameter values for which the observed data have the highest probability of being observed.</p>

<div class="definition">
<p class='definitionTitle'>Definition</p>
<p>If a random variable \(X\) has a distribution that involves two unknown parameters, \(\theta\) and \(\phi\), the <strong>maximum likelihood</strong> estimates of the parameters are the values that maximise the likelihood function.</p>
</div>
<p>The likelihood function is usually maximised at a turning point of the likelihood function and could therefore be found by setting the partial derivatives of \(L(\theta, \phi)\) with respect to \(\theta\) and \(\phi\) to zero.</p>


\[
\frac{\partial L(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial L(\theta, \phi)}{\partial \phi} = 0
\]
<p>giving two equations that can be solved for \(\theta\) and \(\phi\). The parameter values that maximise the likelihood also maximise its logarithm and it is usually easier to work with the log-likelihood, \(\ell(\theta, \phi) = \log L(\theta, \phi)\), so we usually solve the equations</p>
\[
\frac{\partial \ell(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial \ell(\theta, \phi)}{\partial \phi} = 0
\]
<p>instead, giving identical parameter estimates.</p>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
