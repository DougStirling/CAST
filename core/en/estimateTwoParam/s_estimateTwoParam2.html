<!DOCTYPE HTML>
<html>
<head>
	<title>Maximum likelihood</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="stylesheet" href="../../pageStyles.css" type="text/css">
	<script src="../../releaseInfo.js"></script>
	<script src="../../structure/pageSetup.js"></script>
	
	<link rel='stylesheet' href='../../structure/maths/mathStyles.css' type='text/css'>
	<script src='../../structure/videoControls/jquery.js'></script>
	<script src='../../structure/maths/theorems.js'></script>
	<script src='../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js'></script>

	<meta name="index" content="estimation, likelihood function, maximum likelihood">
</head>


<body>
<script type="text/javascript">writePageStart();</script>
<p>Unlike the method of moments,  maximum likelihood estimation can be used for models with <strong>any</strong> number of unknown parameters. We will describe the method for a model with two unknown parameter, \(\theta\) and \(\phi\), but it should be clear how to extend it to three or more parameters.</p>
<p>If \(\{x_1, x_2, \dots, x_n\}\) is a random sample from a discrete distribution with probability function \(p(x\;|\; \theta, \phi)\), the likelihood function is again the probability of getting the observed data for any values of the parameters.</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; p(x_1, x_2, \dots, x_n \;| \; \theta, \phi) \;\;=\;\; \prod_{i=1}^n {p(x_i\;|\; \theta, \phi)}
\]
<p>For a continuous distribution, the corresponding definition is</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; \prod_{i=1}^n {f(x_i\;|\; \theta, \phi)}
\]
<p class="heading">Maximising the likelihood</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
<p>If a random variable \(X\) has a distribution that involves two unknown parameters, \(\theta\) and \(\phi\), the <strong>maximum likelihood</strong> estimates of the parameters are the values that maximise the likelihood function.</p>
</div>
<p>This is usually at a <strong>turning point</strong> of the likelihood function â€” where the partial derivatives of \(L(\theta, \phi)\) with respect to \(\theta\) and \(\phi\) are zero,</p>


\[
\frac{\partial L(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial L(\theta, \phi)}{\partial \phi} = 0
\]
<p>Solving these equations give MLEs  for \(\theta\) and \(\phi\). Equivalently, writing \(\ell(\theta, \phi) = \log L(\theta, \phi)\), we can solve the equations</p>
\[
\frac{\partial \ell(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial \ell(\theta, \phi)}{\partial \phi} = 0
\]
<p>This is usually easier and gives identical parameter estimates.</p>
<script type='text/javascript'>writePageEnd();</script>
</body>
</html>
