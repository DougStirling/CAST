<!DOCTYPE HTML>
<html>
<head>
  <title>15. Comparing Means</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 15 &nbsp; Comparing Means</h1>
<h1 class="sectionName">15.1 &nbsp; Models for two groups</h1>
<h2 class="pageName">15.1.1 &nbsp; Interest in underlying population</h2>

<p class="heading notPrinted">Data from two groups</p>
<p>When data are  collected from two groups, we are usually interested in  differences
between the groups
<strong>in general</strong>. The <strong>specific</strong> individuals  are of
less interest. Questions are therefore about the characteristics of the populations
or processes that we assume <strong>underlie</strong> the
data.</p>
<p class="heading">Example</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/hypnosis.gif" width="523" height="299" class="summaryPict"></p>
<p>The  questions do not refer to the 16 specific subjects &mdash; they ask about
whether anticipation of hypnosis affects the ventilation rate <strong>in general</strong>.
We
would like to use the answers to predict what will happen to other people.</p>




<h2 class="pageName">15.1.2 &nbsp; Model for two groups</h2>

<p class="heading notPrinted">Data and model</p>
<p>Data from two groups can be displayed with two histograms:</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_histos.gif" width="325" height="281"> </p>
<p>The diagram below illustrates a possible model for the data above.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_pdfs.gif" width="321" height="251"></p>




<h2 class="pageName">15.1.3 &nbsp; Parameters of the normal model</h2>

<p class="heading notPrinted">Parameters</p>
<p>A normal model for two groups has four unknown parameters (the mean and standard
deviation for each normal distribution). These parameters give considerable flexibility
and allow the model to be used for a variety of different data sets.</p>
<p>(The number of parameters can be reduced to three if it is assumed that the two
standard deviations are the same, but we will not consider this type of model here.)</p>




<h2 class="pageName">15.1.4 &nbsp; Parameter estimates</h2>

<p class="heading notPrinted">Parameter estimates</p>
<p>A normal model for 2-group data involves 4 unknown parameters, µ<sub>1</sub>,
µ<sub>2</sub>, σ<sub>1</sub> and σ<sub>2</sub>. The means and standard deviations
in the two samples provide objective estimates of the four parameters.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_bestFit.gif" width="330" height="289"></p>




<h2 class="pageName">15.1.5 &nbsp; Difference between means</h2>

<p class="heading notPrinted">Comparing the populations</p>
<p>Although  standard deviations in the two populations may also differ, we are usually
most interested in the difference between the population means. Differences between
the means can be expressed in terms of the model parameters with the following questions.</p>
<ul>
<li>Is µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> = 0?</li>
<li>What is the value of µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub>?</li>
</ul>
<p class="heading">Randomness of sample difference</p>
<p>These questions  are about µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> and the
best estimate of it is <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
However, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span> cannot
give  definitive answers <sub></sub> since
it is  random   &mdash; it varies from sample to sample.</p>
<p class="eqn"><img class="gif" src="../../../en/twoGroupModel/images/birthWt2.gif" width="523" height="296"></p>
<p>Without an understanding of the distribution of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
it is impossible to properly interpret what the sample difference, 0.104&nbsp;kg,
tells you about the difference between the underlying population means.</p>




<h1 class="sectionName breakBefore">15.2 &nbsp; Distn of sums and differences</h1>
<h2 class="pageName">15.2.1 &nbsp; Means and sums of samples (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Sample mean and sum</p>
<p>The mean of a random
sample, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">,
has a distribution that is approximately normal if the sample size, <em>n</em>, is
large and alway has a
mean and standard deviation that depend on the population mean, µ, and standard deviation,
σ,</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></td>
			<td valign="middle">&nbsp;<span class="black">=&nbsp; &mu;</span></td>
		</tr>
	</table>
</div>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></td>
			<td valign="middle">&nbsp;<span class="black">=</span>&nbsp; </td>
			<td valign="middle"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></td>
		</tr>
	</table>
</div>
<p>Occasionally the sum of values in a random sample values is more useful than the
mean,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sampleSum.gif" width="220" height="18"></p>
<p>Its distribution is
a scaled version of the distribution of the mean &mdash; the same shape but different
mean and standard deviation.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumMean.gif" width="75" height="14"></p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumSD.gif" width="89" height="21"></p>
<p class="heading">Mean vs Sum</p>
<p>As the sample size increases,</p>
<ul>
<li>the standard deviation of the mean decreases, but</li>
<li>the standard deviation of the sum <strong>increases</strong>.</li>
</ul>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sdInequality.gif" width="130" height="16"></p>




<h2 class="pageName">15.2.2 &nbsp; Sum and difference (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Sum and difference of two  variables</p>
<p>Applying the result about the sum of a random sample to a sample of size <em>n</em> = 2, <em>X</em><sub>1</sub>
and <em>X</em><sub>2</sub>,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn.gif" width="119" height="50"></p>
<p>If we generalise by allowing <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> to
have different means, µ<sub>1</sub> and µ<sub>2</sub>, but the same σ,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn2.gif" width="146" height="47"> </p>
<p>A similar result holds for the difference between <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>:</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/diff2Distn2.gif" width="146" height="47"></p>
<p>If <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> are independent and have
normal distributions, their sum and difference are also normally distributed.</p>




<h2 class="pageName">15.2.3 &nbsp; Sum and difference (cont) (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">General result</p>
<p>The results generalise further to independent variables that may have different
means <strong>and</strong> standard deviations.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumDiffSD.gif" width="250" height="161"></p>
<p>The formulae for the standard deviations are more easily remembered in terms of
the <strong>variances</strong> of
the  quantities. For example,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Variance.gif" width="151" height="25"></p>




<h2 class="pageName">15.2.4 &nbsp; Probabilities for sums and differences (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Finding probabilities</p>
<p>To find the probability that a sum or difference satisfies an inequality, the
inequality should be translated into ones about a z-score, using the mean and standard
deviation of the quantity,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/standardiseEqn.gif" width="95" height="31"></p>
<p>The standard normal distribution can then be used to find the  probabilities. The
examples below illustrate the method. </p>
<p class="heading">Example (total of several variables)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/totalExample.gif" width="496" height="516"> </p>
<p class="heading">Example (sum of two variables with different sd)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumExample.gif" width="496" height="516"> </p>




<h1 class="sectionName breakBefore">15.3 &nbsp; Comparing means in two groups</h1>
<h2 class="pageName">15.3.1 &nbsp; Distn of difference between means</h2>

<p class="heading notPrinted">Difference between  means</p>
<p>The difference between <strong>any</strong> two independent
quantities <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> has a distribution
with</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/meanSDDiff2.gif" width="113" height="51"> </p>
<p>Applying this to  the difference between the
means of two random samples,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffMeanSD.gif" width="340" height="163"> </p>
<dl>
<dt>If the distributions are normal in each group,&nbsp;...</dt>
<dd>... the  sample means are normal, so their difference
also has a normal distribution.</dd>
<dt>Otherwise,&nbsp;...</dt>
<dd>... the two sample means are approximately normal if the sample sizes are
large,  so their difference is also close to normal.</dd>
</dl>

<div class="centred"><div class="boxed">
<p>Irrespective of the distributions within the
two groups, <br>
<img class="gif" src="../../../en/twoGroupInf/images/normalDistn.gif" width="331" height="44"></p>
</div></div>





<h2 class="pageName">15.3.2 &nbsp; SE of difference between means</h2>

<p class="heading notPrinted">Estimation error</p>
<p>The difference between the sample means, <span class="eqn"><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span></span>,
is a point estimate of the difference between the means of the underlying populations, <span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span>.
In order to properly interpret it, we must understand the distribution of
the estimation error.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/errorTwoDistn.gif" width="372" height="42"></p>
<p>Replacing σ<sub>1</sub><sup>2</sup> and σ<sub>2</sub><sup>2</sup> by <em>s</em><sub>1</sub><sup>2</sup> and
<em>s</em><sub>2</sub><sup>2</sup> gives an approximate error distribution,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/estErrorDistn2.gif" width="172" height="42"> </p>
<p>The standard deviation of these errors is the <strong>standard error</strong> of
the estimator.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisError.gif" width="514" height="417" class="summaryPict"></p>
<p>Our best estimate is that anticipation of hypnosis results in a mean ventilation
rate that is 0.491 higher than the control group.
From the error distribution, the error in this estimate is unlikely to be more
than about 0.6.</p>




<h2 class="pageName">15.3.3 &nbsp; CI for difference between means</h2>

<p class="heading">If <span class="black">σ<sub>1</sub></span> and <span class="black">σ<sub>2</sub></span> were known...</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><strong>Prob</strong> ( </td>
			<td valign="middle"><img src="../../../en/../images/symbol.xBarDiffRed.png" width="42" height="21" align="baseline"></td>
			<td valign="middle">&nbsp;<strong>is within</strong> &nbsp; &plusmn; &nbsp;1.96 &nbsp;</td>
			<td valign="middle"><img src="../../../en/../images/symbol.sdDiffGreen.png" width="41" height="26" align="baseline"></td>
			<td valign="middle">&nbsp; of &nbsp; <span style="color:#00F">&mu;<sub>2</sub>&nbsp;-&nbsp;&mu;<sub>1</sub></span>) &nbsp; = &nbsp; 0.95</td>
		</tr>
	</table>
</div>
<p>so a 95% confidence interval for <span class="eqn"><span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span></span> would
be</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.xBarDiffRed.png" width="42" height="21" align="baseline"></td>
			<td valign="middle">&nbsp; &plusmn; &nbsp; 1.96 &nbsp;</td>
			<td valign="middle"><img src="../../../en/../images/symbol.sdDiffGreen.png" width="41" height="26" align="baseline"></td>
		</tr>
	</table>
</div>
<p class="heading">When <span class="black">σ<sub>1</sub></span> and <span class="black">σ<sub>2</sub></span> are
unknown...</p>
<p>We must replace <span class="black">σ<sub>1</sub> and <span class="black">σ<sub>2</sub></span></span> by <span class="black"><em>s</em><sub>1</sub> and <span class="black"><em>s</em><sub>2</sub></span></span> in
the confidence interval, and the constant '1.96' must  be replaced by a slightly
larger value from t-tables, </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffCIWithT.gif" width="257" height="70"></p>
<p>where the degrees of freedom for the t-value are </p>
<p class=eqn><span class="black">&nu; &nbsp; = &nbsp; min (<em>n</em><sub>1</sub>&minus;1, &nbsp;<em>n</em><sub>2</sub>&minus;1)</span> </p>
<p class="gray">(A more complex formula is available that gives a higher
value for &nu;. It is slightly better but the difference is usually
negligible.)</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisCI.gif" width="514" height="401" class="summaryPict"></p>




<h2 class="pageName">15.3.4 &nbsp; Testing a hypothesis</h2>

<p class="heading">Testing for a difference between  two  means</p>
<p>The difference between two groups that is of most practical
importance is a difference between their <strong>means</strong>. </p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;&ne;&nbsp; 0</strong></span></p>
<p>The summary statistic that throws most light on these hypotheses is the difference
between the sample means, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
Testing therefore involves assessment of whether this difference is unusually
far from zero. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/pValue.gif" width="454" height="266"> </p>
<p>As with all other hypothesis tests, a p-value near zero gives evidence that
the null hypothesis does not hold &mdash; evidence of a difference between the group
means. </p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">General properties of p-values</p>
<p>A statistical hypothesis test cannot provide
a definitive answer about whether two groups have different means. The randomness
of sample data means that p-values are also random quantities.</p>
<p>It is possible to get a small p-value (supporting H<sub>A</sub>) when H<sub>0</sub> is
true, and it is possible to get a large p-value (consistent with H<sub>0</sub>)
when H<sub>A</sub> is true.</p>
<div class="centred"><div class="boxed"><p>There is some chance of being misled by an 'unlucky sample.</p></div></div>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>All p-values between 0 and 1 are equally likely. For example, there is a
5% probability of getting a p-value less than 0.05.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The p-value is more likely to be near zero, though there is still some chance
of a larger p-value.</dd>
</dl>
<p class="heading">Effect of increasing the sample size</p>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>The p-values remain equally likely between 0 and 1.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The distribution of p-values becomes more concentrated near zero, so you
are more likely to conclude that the population means are really different.</dd>
</dl>




<h2 class="pageName">15.3.5 &nbsp; One-tailed tests for differences</h2>

<p class="heading notPrinted">One- and two-tailed tests for differences</p>
<p>In a <strong>two-tailed test</strong>, the alternative hypothesis is that the two population
means are different. A <strong>one-tailed test</strong> arises when we want to test whether one
mean is <strong>higher</strong> than the other (or <strong>lower</strong> than the other).</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/hypotheses.gif" width="441" height="84"> </p>
<p class="heading">Test statistic, p-value and conclusion</p>
<p>Consider a test for the hypotheses,</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&mu;<strong><sub>2</sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;&gt;&nbsp; </strong>&mu;<strong><sub>2</sub></strong></span> </p>
<p>The alternative hypothesis is only supported by very small values of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
This also corresponds to small values of the test statistic <span class="em black">t</span> ,
so the p-value is the <strong>lower</strong> tail probability of the t distribution. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/oneTailedP.gif" width="453" height="265"> </p>
<p>A small  p-value is interpreted as giving evidence that H<sub>0</sub> is false, in a
similar way to all other kinds of hypothesis test.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/bacteriaCarpetsTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">Properties of p-values</p>
<p>We again stress that a statistical hypothesis test cannot provide a definitive
answer. The randomness of sample data means that p-values are also random quantities,
so there is some chance of us being misled by an 'unlucky' sample:</p>
<ul>
<li>If µ<sub>1</sub> = µ<sub>2</sub>, it is still possible to get a small p-value
(e.g. a 5% probability of getting a p-value less than 0.05).</li>
<li>If µ<sub>1</sub> and µ<sub>2</sub> are different, large p-values are still
possible (though less likely than small p-values).</li>
</ul>




<h1 class="sectionName breakBefore">15.4 &nbsp; Paired t test</h1>
<h2 class="pageName">15.4.1 &nbsp; Paired data</h2>

<p class="heading notPrinted">Paired data</p>
<p>When two types measurements, <em>X</em> and <em>Y</em>, are made from each
individual (or other unit), the data are called <strong>bivariate</strong>. Sometimes
the two measurements are of  closely related quantities and
may even describe the same quantity at different times. </p>
<div class="centred"><div class="boxed"><p>When the sum or difference of <em>X</em> and <em>Y</em> is a meaningful quantity,
the data are called <strong>paired data</strong>.</p></div></div>
<p class="heading">Hypotheses of interest</p>
<p>For paired data, We often want to test whether the means of the two variables
are equal,</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>Sometimes a one-tailed test is required, such as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &gt; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p class="heading">Examples</p>
<dl>
<dt>Pre-test, post-test data</dt>
<dd>This arises when a measurement is made from each individual, then a second
measurement of the same type is made after some kind of intervention (e.g. training
or medication). Has the intervention &quot;improved&quot; the measurement?</dd>
<dt>Twin studies</dt>
<dd>Some experiments or other studies are conducted with
identical twins, either human or animal. The  members
of each pair experience different environments &mdash; either two different experimental
treatments or two other differences. Are there differences between the two treatments?</dd>
<dt>Other types of pairing</dt>
<dd>For example, damaged cars may each be taken to two garages for estimates
of the cost of repair. The two estimates for each car are paired data. Does one
garage overcharge?</dd>
</dl>





<h2 class="pageName">15.4.2 &nbsp; Analysis of differences</h2>

<p class="heading notPrinted">Differences</p>
<p>Information about the difference between the means of <em>X</em> and <em>Y</em> is
contained in the values <em>D</em> = (<em>Y</em> - <em>X</em>)  for
each individual. The hypotheses</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>can then be expressed as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> = 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> &ne; 0</strong></span></p>
<p>This reduces the paired data set to a <strong>univariate</strong> data set of
differences,
<em>D</em>, and reduces questions about (µ<sub><em>Y</em></sub> - µ<sub><em>X</em></sub>)
to questions about the mean of <em>D</em>.</p>
<p class="heading">Analysis of paired data</p>
<p>By taking differences between <em>Y</em> and <em>X</em>, much of the variability
between the individuals is eliminated, making it easier to see whether their means
are different. The example below shows paired data on the left with blue lines
joining the x- and y-values in each pair. The differences on the right make it clearer
that the y-values are usually higher than the corresponding x-values.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_pairing.gif" width="404" height="322"></p>





<h2 class="pageName">15.4.3 &nbsp; Paired t-test</h2>

<p class="heading notPrinted">Approach (paired t-test)</p>
<p>Testing whether two paired measurements, <em>X</em> and <em>Y</em>,
have equal means is done in terms of the differences</p>
<p class="eqn"><em><strong>D</strong></em><strong> = <em>Y</em> - <em>X</em></strong></p>
<p>The test is then expressed as</p>
<p class="eqn"><strong>H<sub>0</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> =
0</p>
<p class="eqn"><strong>H<sub>A</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> ≠
0</p>
<p>or a one-tailed variant. The hypotheses are therefore assessed with a standard
univariate t-test using test statistic</p>
<p class="eqn"><img class="gif" src="../../../en/testPaired/images/testStat.gif" width="86" height="58"></p>
<p>This is compared to a t distribution with <em>n</em>&nbsp;-&nbsp;1
degrees of freedom to find the p-value.</p>
<p class="heading">Example</p>
<p>The diagram below illustrates a 2-tailed test for equal means, based on <em>n</em> = 15
paired observations.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_example.gif" width="475" height="384"></p>
<p>From the p-value, we conclude that there is very strong evidence that the
means for <em>Y</em> and <em>X</em> are different.</p>





<h2 class="pageName">15.4.4 &nbsp; Pairing and experimental design</h2>

<p class="heading notPrinted">Choice between paired data or two independent samples</p>
<p>It is sometimes possible to answer questions about the difference
between two means by collecting two alternative types of data.</p>
<dl>
<dt>Two independent samples</dt>
<dd>Measurements are made from two samples of individuals from the groups whose
means are to be compared. A 2-sample t-test can be used to compare the means.</dd>
<dt>One paired sample</dt>
<dd>The 'individuals' can be re-defined as pairs of related values from the two
groups and a single sample of these pairs can be collected. A paired t-test
can be performed on the differences to compare the means.</dd>
</dl>
<div class="centred"><div class="boxed"><p>If the individuals in the 2 groups can be paired so that the pairs
are relatively similar, a paired design gives more accurate results.</p></div></div>
<p class="heading">Matched pairs in experiments</p>
<p>In experiments to compare two treatments, it may be possible to group the
experimental units into pairs that are similar in some way. These are called <strong>matched
pairs</strong>.
If the two experimental units in each pair are randomly assigned to the two
treatments, the data can be analysed as described in this section.</p>
<p>The difference between the treatments is estimated more accurately than in
a completely randomised experiment.</p>




<h1 class="sectionName breakBefore">15.5 &nbsp; Comparing several means</h1>
<h2 class="pageName">15.5.1 &nbsp; Model</h2>

<p class="heading notPrinted">Data</p>
<p>In this section, we examine data that may arise as:</p>
<ul>
<li>A separate sample of numerical measurements from each of <em>g</em> groups,
or </li>
<li>Bivariate data with a numerical response and a categorical 'explanatory'
variable with <em>g</em> levels. This categorical variable can be used to define
'groups' of individuals.</li>
</ul>
<p>We will model the data in terms of <em>g</em> groups. The data often arise
from <a href="javascript:showNamedPage('designIntro4')">completely randomised
experiments with <em>g</em> treatments</a>.</p>
<p class="heading">Model</p>
<p>The model that was used for 2 groups can be easily extended to to <em>g</em>&nbsp;&gt;&nbsp;2
groups, allowing different means and standard deviations in all groups.</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ<sub><em>i</em></sub>)</span></td>
</tr>
</table></div>
<p class="eqn"><img src="../../../en/multiGroup/images/s_multiModel.gif" width="401" height="315"></p>
<p>However to develop a test for equal group means with <em>g</em>&nbsp;&gt;&nbsp;2
groups, we must make an extra assumption that the
standard deviations in all groups are the same.</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p class="eqn"><img src="../../../en/multiGroup/images/s_multiModel2.gif" width="401" height="286"></p>
<p>If there are <i>g</i> groups, this model has <i>g</i>&nbsp;+&nbsp;1 unknown
parameters &mdash; the <i>g</i> group means and the common standard deviation, σ.
It is flexible enough to be useful for many data sets.</p>
<div class="centred"><div class="boxed"><p>If the assumptions of a normal distribution and constant variance
do not hold, a nonlinear transformation of the response may result in data for
which the model is appropriate.</p></div></div>




<h2 class="pageName">15.5.2 &nbsp; Parameter estimates</h2>

<p class="heading">Estimating the group means</p>
<p>We now assume a  normal model with the same standard deviation
in each group,</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p>The sample means provide estimates of the {µ<sub><em>i</em></sub>}:</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/groupMeanEst.gif" width="52" height="22"> </p>
<p class="heading">Estimating σ<sup>2</sup></p>
<p>The sample standard deviation in any single group, <em>s<sub>i</sub></em>,
is a valid estimate of σ, but we need to combine these <em>g</em> separate estimates
in some way.</p>
<p>It is easier to describe estimation of σ<sup>2</sup> rather than σ. If the sample
sizes are the same in all groups, a <strong>pooled</strong> estimate
of σ<sup>2</sup> is the average of the group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst3.gif" width="105" height="38"> </p>
<p>If the sample sizes are <strong>not</strong> equal in all groups, this is
generalised by adding the numerators and denominators of the formulae for the <em>g</em> separate
group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst.gif" width="444" height="67"> </p>
<p>More mathematically,  <em>y<sub>ij</sub></em> denotes the <span class="em black">j</span> 'th
of the <span class="em black">n<sub>i</sub></span>  values in group <span class="em black">i</span> ,
for <span class="em black">i</span> &nbsp;=&nbsp;1 to <span class="em black">g</span> .
The pooled estimate of σ<sup>2</sup> can then be written as</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst2.gif" width="210" height="80"> </p>
<p>The pooled variance is influenced most by the sample variances in the groups
with biggest sample sizes.</p>




<h2 class="pageName">15.5.3 &nbsp; Inference about two groups (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Revisiting the difference between <span class="red">two</span> group
means</p>
<p>In an earlier section, we described  confidence intervals and tests about the
difference between two group means, µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub>.
They can be improved if we can assume that</p>
<p class=eqn><span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span> </p>
<p>Inference is still based on <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
but the equation for its standard deviation can be simplified</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/diffMeanSD.gif" width="261" height="45"> </p>
<p class="heading">Confidence interval</p>
<p>A 95% confidence interval for µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub> has
the same general form as before,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/generalCI.gif" width="160" height="22"> </p>
<p>but the standard deviation and the degrees of freedom
for the t-value, &nu;, are different.</p>
<div class="centred">
	<table border="0" class="centred" cellpadding="10" cellspacing="0">
		<tr>
			<td style="border-bottom:1px solid #999999;">&nbsp;</td>
			<td align="center" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/sdEst.gif" width="42" height="22"></td>
			<th align="center" style="border-bottom:1px solid #999999;"><span class="black">degrees of freedom</span></th>
		</tr>
		<tr>
			<td style="border-bottom:1px solid #999999;">Allowing <span class="black">&sigma;<sub>1</sub> &ne; &sigma;<sub>2</sub></span></td>
			<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/sdEst1.gif" width="85" height="45"></td>
			<td align="center" bgcolor="#FFFFFF" class="black" style="border-bottom:1px solid #999999;">min( <em>n</em><sub>1</sub> - 1, <em>n</em><sub>2</sub> - 1)</td>
		</tr>
		<tr>
			<td style="border-bottom:1px solid #999999;">Assuming <span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span></td>
			<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/diffMeanSDEst.gif" width="109" height="39"></td>
			<td align="center" bgcolor="#FFFFFF" class="black" style="border-bottom:1px solid #999999;"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> - 2</td>
		</tr>
	</table>
</div>
<p>If it can be assumed that &sigma;<sub>1</sub> = &sigma;<sub>2</sub>,  the
confidence interval is usually narrower.</p>
<p class="heading">Example</p>
<p>The diagram below shows 95% confidence intervals obtained by the two methods.</p>
<p align="center"><img src="../../../en/multiGroup/images/hypnosisPooled.gif" width="550" height="340"> </p>
<p>The p-value for this test is found from the tail area of the t distribution
with (<span class="black"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> -
2</span>) degrees of freedom.</p>




<h2 class="pageName">15.5.4 &nbsp; Variation between and within groups</h2>

<p class="heading notPrinted">Comparing several groups</p>
<p>A new approach is needed to compare the means of three or more groups &mdash; the
 methods for two groups cannot be extended. We again assume a normal model with equal
standard deviations,</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p>Testing whether there are differences between the groups involves the hypotheses,</p>

<p style="margin-left:200px; color:#000000"><font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>0</sub></strong></font> : &nbsp; µ<sub><em>i</em></sub> &nbsp;=&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
all i and j</em><br>
<font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>A</sub></strong></font>: &nbsp; µ<sub><em>i</em></sub> &nbsp;≠&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
at least some i, j</em></p>

<p class="heading">Variation between and within groups</p>
<p>Testing whether the model means, {µ<sub><em>i</em></sub>}, are equal is done
by assessing the <strong>variation between the group means</strong> in the data.
However, because of randomness in sample data, the means are unlikely be the same,
even if <strong>H<sub>0</sub></strong> is true.</p>
<p>In the example on the left below, the group means vary so much that the 
{µ<sub><em>i</em></sub>} are almost certainly not equal. However the group means
on the right are relatively similar and their differences may simply be randomness.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_variationBetween.gif" width="518" height="295"></p>
<p>To assess whether the means are 'unusually different',
we must also take account of the <strong>variation within
the groups</strong>. The data set on the left below gives much stronger evidence
of group differences than that on the right, even though the group means are
the same in both data sets.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_variationWithin.gif" width="518" height="295"></p>
<p>The evidence against <strong>H<sub>0</sub></strong> depends on the <strong>relative size</strong> of the
variation within groups and between groups.</p>




<h2 class="pageName">15.5.5 &nbsp; Sums of squares</h2>

<p class="heading notPrinted">Notation</p>
<p>In the formulae in this page, the values in the <em>i</em>'th group are denoted
by <span class="em black">y<sub>i</sub></span> <span class="black"><sub>1</sub></span>, <span class="em black">y<sub>i</sub></span> <span class="black"><sub>2</sub></span>,
... . More generally, the <i>j</i>'th
value in the <i>i</i>'th group is called <span class="em black">y<sub>ij</sub></span>  and
the
mean of the values in the <i>i</i>'th group is  <span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yiBar.png" width="12" height="14" align="baseline"></span>.</p>
<p class="heading">Total variation</p>
<div class="centred">
	<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
		<tr>
			<td width="95"><img class="gif" src="../../../en/multiGroup/images/totalSsq.gif" width="79" height="27"></td>
			<td class="green" style="text-align:left;">The <strong>total sum of squares</strong> reflects the 
				total variability of the response.</td>
		</tr>
	</table>
</div>
<p>The overall variance of all values (ignoring 
groups) is the total sum of squares divided by (<em>n</em>&nbsp;-&nbsp;1).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_totalSsq.gif" width="413" height="277"></td>

<div class="centred">
	<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
		<tr>
			<td width="95"><img class="gif" src="../../../en/multiGroup/images/regnSsq.gif" width="75" height="26"></td>
			<td class="red" style="text-align:left;">The <strong> sum of squares between groups</strong> measures 
				the variability of the group means.</td>
		</tr>
	</table>
</div>
<p>Variation between groups is summarised by the differences between the group
means and the overall mean. Note that the summation  is <span class="black bold">over
all observations in the data set</span>.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_betweenSsq.gif" width="413" height="277"></td>
	<div class="centred">
		<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
			<tr>
				<td width="95"><img class="gif" src="../../../en/multiGroup/images/residSsq.gif" width="83" height="27"></td>
				<td class="blue" style="text-align:left;">The <strong> sum of squares within groups</strong> quantifies 
					the spread of values within each group.</td>
			</tr>
		</table>
	</div>
<p>This is also called the <strong>residual</strong> sum of squares since it
describes variability that is unexplained by differences between the groups.
Note that the pooled estimate of the common variance, σ<sup>2</sup>, is the sum
of squares within groups divided by (<em>n</em>&nbsp;-&nbsp;<em>g</em>).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_withinSsq.gif" width="413" height="277"> </p>




<h2 class="pageName">15.5.6 &nbsp; Coefficient of determination (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Sums of squares</p>
<div class="centred"><table border="0" class="centred" cellpadding="4" cellspacing="0">
<tr>
<th align="left" style="border-bottom:1px solid #999999;">Sum of squares</th>
<th style="border-bottom:1px solid #999999;">Interpretation</th>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/totalSsq2.gif" width="141" height="25">
<td>Overall variability of <em>Y</em>, taking no account of the groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residSsq2.gif" width="154" height="25">
<td>Variability
that <strong>cannot be explained</strong> by the model.</td>
</tr>
<tr>
<td style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/regnSsq2.gif" width="149" height="24">
<td style="border-bottom:1px solid #999999;">Variability that is <strong>explained</strong> by
the model.</td>
</tr>
</table></div>
<p class="heading">Coefficient of determination</p>
<p>The <strong>proportion</strong> of
the total sum of squares that is explained by the model is called
the <strong>coefficient of determination</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/rSquaredDefn.gif" width="97" height="38"> </p>
<ul>
<li>0  ≤  R<sup>2</sup>  ≤  1</li>
<li>The proportion of <strong>unexplained</strong> variation is (1&nbsp;-&nbsp;R<sup>2</sup>)</li>
<li>When R<sup>2</sup> &asymp; 0, the group means are similar
to each other.</li>
<li>If R<sup>2</sup> &asymp; 1, the individual values must be  close
to their group means.</li>
</ul>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/multiGroup/images/radiocarbonR2.gif" width="555" height="344" class="summaryPict"></p>





<h2 class="pageName">15.5.7 &nbsp; Test for differences between groups</h2>

<p class="heading notPrinted">Hypothesis test</p>
<p>The following hypotheses are used to test whether the group means are all
equal:</p>
<p style="margin-left:200px;" class="black"><span class="arial"><strong><span class="bigger">H</span><sub>0</sub></strong></span> : &nbsp; µ<sub><em>i</em></sub> &nbsp;=&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
all i and j</em><br>
<span class="arial"><strong><span class="bigger">H</span><sub>A</sub></strong></span>: &nbsp; µ<sub><em>i</em></sub> &nbsp;≠&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
at least some i, j</em></p>
<p>We will describe some of the steps for this test,  but cannot
justify them here.</p>
<p class="heading">Mean sums of squares</p>
<p>The three sums of squares are first divided by  values called their <strong>degrees
of freedom</strong>:</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/totalMss.gif" width="139" height="43"></td>
<td class="green">The <strong>mean total sum of squares</strong> is the
sample variance of the response (ignoring groups).</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residMss.gif" width="143" height="43"></td>
<td class="blue">The <strong>mean within-group sum of squares</strong> is
the pooled estimate of the variance within groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/regnMss.gif" width="162" height="43"></td>
<td class="red">The <strong>mean between-group sum of squares</strong> is
harder to directly interpret.</td>
</tr>
</table></div>
<p>The numerators in these ratios add up:</p>
<p class="eqn"><span class="green">SS<sub>Total</sub></span>  =  <span class="red">SS<sub>Between</sub></span>  +  <span class="blue">SS<sub>Within</sub></span></p>
<p>and the same relationship holds for their denominators (degrees of freedom):</p>
<p class="eqn"><span class="green">df<sub>Total</sub></span>  =  <span class="red">df<sub>Between</sub></span>  +  <span class="blue">df<sub>Within</sub></span></p>
<p class="heading">F ratio and p-value</p>
<p>The test statistic is  an <strong>F-ratio</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/fRatio.gif" width="114" height="37"> </p>
<p>This test statistic compares between- and within-group variation. The further
apart the group means, the larger <span class="red">SS<sub>Between</sub></span> and the larger the F-ratio.<br>
</p>

<div class="centred"><div class="boxed">
<p>Large values of F suggest that H<sub>0</sub> does
not hold &mdash; that the group means are not the same.</p>
</div></div>

<p>The p-value for the test is the probability of such a high F ratio if <strong>H<sub>0</sub></strong> is
true (all group means are the same). It is based on a standard distribution called
an <strong>F distribution</strong> and is interpreted in the same way as other
p-values. </p>

<div class="centred"><div class="boxed">
<p>The closer the p-value to zero, the stronger the evidence
that H<sub>0</sub> does not hold.</p>
</div></div>

<p class="heading">Analysis of variance table</p>
<p>An <strong>analysis
of variance table</strong> (<strong>anova table</strong>) describes some of the calculations
above:</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/anovaTable2.gif" width="596" height="192"></p>




<h2 class="pageName">15.5.8 &nbsp; Examples</h2>

<p align="center"><img src="../../../en/multiGroup/images/radiocarbonF.gif" width="550" height="434" class="summaryPict"></p>
<p align="center"><img src="../../../en/multiGroup/images/teachingF.gif" width="550" height="434" class="summaryPict"></p>




<h1 class="sectionName breakBefore">15.6 &nbsp; Exercises</h1>
</body>
</html>
