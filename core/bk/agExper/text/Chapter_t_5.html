<html>
<head>
<title>5. More about treatments</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 5 &nbsp; More about treatments</h1>
<h2>5.1 &nbsp; Contrasts and constraints</h2>
<h3>5.1.1 &nbsp; Treatments and factors</h3>
<p>The combinations of factor levels used are called treatments. The sum of squares explained by the treatments (treated as a single factor) is closely related to those of the main effects and interaction between the actual factors.</p>
<h3>5.1.2 &nbsp; Defining models with constraints</h3>
<p>The model with no interaction between two factors can be expressed with constraints on the parameters of the most general model. Similarly, models without main effects can be expressed in terms of constraints.</p>
<h3>5.1.3 &nbsp; Comparing a control to other treatments</h3>
<p>Models where the control level of a single factor equals the average for the other treatments, and where all non-control levels are equal, can both be expressed in terms of constraints.</p>
<h3>5.1.4 &nbsp; Analysis of variance for constraints</h3>
<p>The treatment sum of squares for an experiment with a control level can be split into a sum of squares explained by the difference between the control and other levels, and a sum of squares explained by differences between the non-control levels. This allows both constraints to be tested.</p>
<h3>5.1.5 &nbsp; Comparing groups of treatments</h3>
<p>The treatment sum of squares can be split into components for differences between two groups of treatments, and for differences between these groups.</p>
<h3>5.1.6 &nbsp; Comparisons in block designs</h3>
<p>The above methods for splitting the treatment sum of squares can also be used in more complex experimental designs such as randomised blocks.</p>
<h3>5.1.7 &nbsp; Estimating contrasts</h3>
<p>An alternative approach to analysing the structure of the levels of a factor estimates linear functions of the model parameter called contrasts.</p>
<h2>5.2 &nbsp; Numerical factors</h2>
<h3>5.2.1 &nbsp; One factor with numerical levels</h3>
<p>If the levels of the controlled factor are numerical values -- quantities of some numerical variable -- then we usually expect a 'smooth' relationship between the factor and response.</p>
<h3>5.2.2 &nbsp; Linear and quadratic models</h3>
<p>The simplest model for a smooth relationship between a numerical factor and response contains a linear term in the factor value. Adding a further quadratic term can model some curvature in the relationship.</p>
<h3>5.2.3 &nbsp; Terms and constraints</h3>
<p>Linear and quadratic models correspond to terms added to a model in which the factor does not affect the response. They can be equivalently defined through constraints on the smoothness of the parameters of the most general factor model.</p>
<h3>5.2.4 &nbsp; Explained sums of squares</h3>
<p>The differences between the residual sum of squares of different models for a numerical factor called explained sums of squares and are have degrees of freedom equal to the difference between the unknown parameters of the models.</p>
<h3>5.2.5 &nbsp; Anova table and tests</h3>
<p>The explained and residual sums of squares can be presented in an analysis of variance table. The p-values can be used to test whether quadratic or linear relationships are consistent with the data.</p>
<h3>5.2.6 &nbsp; Equivalence of models</h3>
<p>If there are only two levels of the factor, a linear model is equivalent to a model with parameters for each level mean. Similarly, a quadratic model is equivalent to a this general factor model if there are three factor levels.</p>
<h3>5.2.7 &nbsp; Numerical factors in other designs</h3>
<p>In any model with a numerical factor, its explained sum of squares can be split into linear and nonlinear components. The nonlinear sum of squares can be used to test whether the relationship is linear.</p>
<h3>5.2.8 &nbsp; Response surface models</h3>
<p>In some applications, we want to find the combination of values of two factors that optimises some response. A model with quadratic terms in the factors and an interaction term is often used.</p>
<h2>5.3 &nbsp; Multiple comparisons</h2>
<h3>5.3.1 &nbsp; Problems with multiple tests</h3>
<p>If several hypothesis tests are conducted with the same significance level, the probability of at least one being significant can be much higher. A modification allows the overall significance level to be fixed.</p>
<h3>5.3.2 &nbsp; Which means are different?</h3>
<p>If it is concluded that the reponse mean depends on the factor level, it is natural to ask which levels differ. If the experiment involves several factor levels, t-tests to compare pairs of means should be avoided -- with many such tests, there is an increased probability of a low p-value for at least one pair of means.</p>
<h3>5.3.3 &nbsp; Multiple comparisons</h3>
<p>A wider difference than that suggested by a pairwise comparison should be used when there are several group means to compare.</p>
<h3>5.3.4 &nbsp; Comparisons in randomised block designs</h3>
<p>A similar adjustment can be made for comparison of treatments in a randomised block design.</p>
<h3>5.3.5 &nbsp; Warning: Are multiple comparisons necessary?</h3>
<p>If there is any structure to the factor levels, it is often best to use contrasts to make more meaningful comparisons between the levels.</p>
<h2>5.4 &nbsp; Unequal replicates</h2>
<h3>5.4.1 &nbsp; Missing values with a single factor</h3>
<p>If some response measurements are missing, the ordinary analysis of variance table can still be used to test whether the level means are equal, provided the probability of a missing value is unrelated to the response that would have been obtained.</p>
<h3>5.4.2 &nbsp; Unequal replicates by design</h3>
<p>If a factor has a control level, it is sometimes replicated more than the other factor levels.</p>
<h3>5.4.3 &nbsp; Orthogonal designs for two factors</h3>
<p>If all treatments have the same replicates, the sums of squares for the two factors do not depend on the order of adding them to the model. The same holds provided the replicates for each level of X are in the same proportion for each level of Z.</p>
<h3>5.4.4 &nbsp; Anova for non-orthogonal factors</h3>
<p>If two factors are not orthogonal, there are different analysis of variance tables for the two orders of adding the factors. Both must be examined to fully understand the data.</p>
<h3>5.4.5 &nbsp; Orthogonal factor and blocks</h3>
<p>Provided the replicates of a factor in each block are in proportion, the blocks and factor are orthogonal. However this is less important since blocks are always added to the model before the factor so only a single anova table need be considered.</p>
<h3>5.4.6 &nbsp; Missing treatments and confounding</h3>
<p>This section has only considered data with at least one replicate for every treatment. Missing treatments mean that some effects cannot be estimated. In extreme cases, it is impossible to distinguish the effects of different factors -- they are confounded.</p>
<h2>5.5 &nbsp; Covariates</h2>
<h3>5.5.1 &nbsp; Variability in experimental units</h3>
<p>The more variability in the experimental units that is explained by the model used, the greater the accuracy of estimating the effects of the factors of interest.</p>
<h3>5.5.2 &nbsp; Effect of unmodelled covariate</h3>
<p>Randomisation ensures that the effects of factors are always estimated without bias. However variation between the units getting different treatments can result in very variable estimates if the variation in the experimental units is not modelled.</p>
<h3>5.5.3 &nbsp; Model for numerical covariate</h3>
<p>Numerical measurements describing variability in the experimental units can be modelled with linear terms in the model.</p>
<h3>5.5.4 &nbsp; Inference with covariates</h3>
<p>There is usually no interest in testing the significance of covariates. Inference about the model terms relating to the factors of interest can be done with confidence intervals and analysis of variance as usual.</p>
<h3>5.5.5 &nbsp; Blocks and covariates</h3>
<p>Experimental units can be grouped into blocks and also have covariates. Blocks and covariates should be included in the model before testing the effects of the factors of interest.</p>
<h3>5.5.6 &nbsp; Categorical covariates (cofactors)</h3>
<p>Categorical covariates are modelled in exactly the same way as blocks. Models can contain several terms explaining different aspects of the variability in the experimental units.</p>
<h3>5.5.7 &nbsp; Recovery from bad design</h3>
<p>'Bad' allocation of treatments to experimental units can sometimes be saved if covariates are recorded and used to reduce unexplained variation.</p>
</html>
