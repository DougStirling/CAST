<html>
<head>
<title>8. Anova theory (advanced)</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 8 &nbsp; Anova theory (advanced)</h1>
<h2>8.1 &nbsp; Distribution of variance</h2>
<h3>8.1.1 &nbsp; Distribution of Z-squared</h3>
<p>The square of a standard normal variable has a chi-squared distribution with 1 degree of freedom.</p>
<h3>8.1.2 &nbsp; Sums of squares</h3>
<p>The sum of n squared standard normal variables has a chi-squared distribution with n d.f.</p>
<h3>8.1.3 &nbsp; Sum of squares about sample mean</h3>
<p>Differences between values and the population mean can be written as the sum of two components and their sums of squares satisfy a similar relationship. This shows that the sum of squares about the sample mean is less than or equal to that about the population mean.</p>
<h3>8.1.4 &nbsp; Sums of squares tables</h3>
<p>The sum of squares about the population mean can be split in different ways into component sums of squares with chi-squared distributions. The sum of squares about the sample mean has (n-1) degrees of freedom and its mean sum of squares is the sample variance.</p>
<h3>8.1.5 &nbsp; Ratio of variances and F distribution</h3>
<p>The ratio of two independent sample variances (or mean sums of squares) has an F distribution whose degrees of freedom are those of the two variances.</p>
<h3>8.1.6 &nbsp; Overview of analysis of variance</h3>
<p>The sum of squares about the sample mean can often be further split into component sums of squares. Comparison of the corresponding mean sums of squares can be used to test whether the model underlying the data has certain characteristics. </p>
<h3>8.1.7 &nbsp; Summary of anova distributions</h3>
<p>This page summarises the most important results from the section. </p>
<h2>8.2 &nbsp; Inference for variances (optional)</h2>
<h3>8.2.1 &nbsp; Confidence interval for the variance</h3>
<p>A 95% CI for the population variance can be found from the chi-squared distn with (n-1) degrees of freedom.</p>
<h3>8.2.2 &nbsp; Properties of the confidence interval</h3>
<p>As with other 95% CIs, there is 95% probability that a confidence interval for the variance will include the underlying population variance.</p>
<h3>8.2.3 &nbsp; Warning about CI for variance</h3>
<p>The confidence level for the 95% CI is only accurate if the sample comes from a normal population. The CI should therefore be avoided unless you are sure about the shape of the population distribution.</p>
<h3>8.2.4 &nbsp; Independence of mean and variance</h3>
<p>For random samples from a normal distribution, the sample mean and variance are independent.</p>
<h3>8.2.5 &nbsp; Model and hypotheses</h3>
<p>For data that that arise as samples from normal distributions in both groups, we tested earlier whether the group means were the same. Equality of the group variances can also be examined.</p>
<h3>8.2.6 &nbsp; Test statistic</h3>
<p>The ratio of the two sample variances has an F distribution whose shape depends on the sample sizes in the two groups.</p>
<h3>8.2.7 &nbsp; F test</h3>
<p>To test equality of two variances, the F ratio is compared to an F distribution. The test is 2-tailed and the p-value is twice the smaller tail area.</p>
</html>
