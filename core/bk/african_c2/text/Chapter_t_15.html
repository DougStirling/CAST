<html>
<head>
<title>15. Independence</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 15 &nbsp; Independence</h1>
<h2>15.1 &nbsp; Marginal & conditional probability</h2>
<h3>15.1.1 &nbsp; Joint probabilities</h3>
<p>Bivariate categorical data are modelled as a sample from a population that consists of pairs of categorical values. The joint probability for any pair of categories is their population proportion.</p>
<h3>15.1.2 &nbsp; Marginal probabilities</h3>
<p>The marginal probabilities for a variable are the population proportions for its possible values. They can be found by summing joint probabilities.</p>
<h3>15.1.3 &nbsp; Conditional probabilities</h3>
<p>Conditional probabilites for a variable are proportions in a sub-population containing a specific value for the other variable. They are found by scaling the joint probabilities in that sub-population.</p>
<h3>15.1.4 &nbsp; Graphical display of probabilities</h3>
<p>Joint, marginal and conditional probabilities can be displayed graphically.</p>
<h3>15.1.5 &nbsp; Calculations with probabilities</h3>
<p>The model can be equivalently described by (a) joint probabilities, (b) marginal probabilites for X and conditional probabilities for Y, or (c) marginal probabilites for Y and conditional probabilities for X. Any of these sets of probabilities can be found any other set.</p>
<h2>15.2 &nbsp; Independence</h2>
<h3>15.2.1 &nbsp; Association</h3>
<p>Two categorical variables, X and Y, are associated (related) when the conditional distribution of Y given X=x is different for different values of x. Knowing the value of X therefore tells you something about Y.</p>
<h3>15.2.2 &nbsp; Independence</h3>
<p>When the conditional distribution of Y is the same for all values of X, the variables are called independent. This special case is of practical importance.</p>
<h2>15.3 &nbsp; Testing for independence</h2>
<h3>15.3.1 &nbsp; Independence from samples</h3>
<p>Independence is a population property. To assess independence from a sample contingency table, the observed cell counts are compared to those estimated from a model with independence.</p>
<h3>15.3.2 &nbsp; Testing for independence</h3>
<p>The raw sum of squared differences between observed and estimated cell counts is not a good test statistic.</p>
<h3>15.3.3 &nbsp; Chi-squared test statistic</h3>
<p>The 'chi-squared' statistic is a modified sum of squared differences that has a standard distribution (a chi-squared distribution) when there is independence.</p>
<h3>15.3.4 &nbsp; P-value for chi-squared test</h3>
<p>The chi-squared statistic can be used to find a p-value for testing independence. The p-value has similar interpretation and properties to p-values for all other hypothesis tests.</p>
<h3>15.3.5 &nbsp; Examples</h3>
<p>The chi-squared test is applied to a few real data sets. When the variables are found to be associated, the nature of the relationship is described from a comparison of observed and estimated cell counts.</p>
<h3>15.3.6 &nbsp; Comparing groups</h3>
<p>The chi-squared test assesses independence of two categorical variables. It is also used to test whether a single categorical variable has the same distribution in several groups.</p>
</body>
</html>
