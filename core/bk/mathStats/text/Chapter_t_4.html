<html>
<head>
<title>4. Estimating a Parameter</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 4 &nbsp; Estimating a Parameter</h1>
<h2>4.1 &nbsp; Estimation</h2>
<h3>4.1.1 &nbsp; Unknown parameters</h3>
<p>Thinking about how data arose often suggests a possible model for the values, but this usually involves one or more unknown values called parameters.</p>
<h3>4.1.2 &nbsp; Estimators of parameters</h3>
<p>The unknown parameters in a family of distributions may be estimated from a random sample, but there may be more than one way to do this from the data.</p>
<h3>4.1.3 &nbsp; Bias</h3>
<p>An estimator is a function of a random sample and therefore has a distribution itself. Its bias is the difference between the distribution's mean and the actual parameter. An unbiased estimator is good.</p>
<h3>4.1.4 &nbsp; Standard error</h3>
<p>A good estimator's distribution should also have small spread. Its standard deviation is called the standard error of the estimator.</p>
<h3>4.1.5 &nbsp; Mean squared error</h3>
<p>The mean squared error of an estimator combines its bias and standard error into a single value that can be used to compare competing estimators.</p>
<h3>4.1.6 &nbsp; Consistency</h3>
<p>Most estimators that are based on random samples have bias and standard error that tend to zero as the sample size increases. Such estimators are called consistent.</p>
<h3>4.1.7 &nbsp; Sample mean or median?</h3>
<p>The sample mean and median are compared as estimators of parameters from different families of distributions.</p>
<h2>4.2 &nbsp; Method of moments</h2>
<h3>4.2.1 &nbsp; The method of moments</h3>
<p>For distributions with a single unknown parameter, a general estimation method chooses its value to make the distribution's mean equal to the mean of a random sample.</p>
<h3>4.2.2 &nbsp; Examples</h3>
<p>The method of moments is illustrated using samples from binomial and geometric distributions.</p>
<h2>4.3 &nbsp; Maximum likelihood</h2>
<h3>4.3.1 &nbsp; Likelihood function</h3>
<p>The likelihood function for a parameter expresses the probability of getting the observed data for different values of the parameter. It is a function of the parameter and treats the observed data as constants.</p>
<h3>4.3.2 &nbsp; Maximising the likelihood</h3>
<p>The parameter value that maximises the likelihood is called its maximum likelihood estimate. It is usually where the derivative of the likelihood function is zero. The value is usually easiest to find by solving the equation that sets the derivative of the log-likelihood to zero.</p>
<h3>4.3.3 &nbsp; Examples</h3>
<p>Maximum likelihood is illustrated using samples from binomial and geometric distributions.</p>
<h2>4.4 &nbsp; Asymptotic properties of MLEs</h2>
<h3>4.4.1 &nbsp; Bias, variance and normality</h3>
<p>Maximum likelihood estimators cannot be beaten in large samples. They are asymptotically unbiased and normally distributed, and there is an approximate formula for their variance.</p>
<h3>4.4.2 &nbsp; Standard error</h3>
<p>A numerical approximation to the standard error of a maximum likelihood estimator can be found from the second derivative of the log-likelihood.</p>
<h3>4.4.3 &nbsp; Examples</h3>
<p>The standard error is found for the probability of success, firstly when estimated from a single binomial proportion, and then when estimated from a random sample from a geometric distribution.</p>
<h2>4.5 &nbsp; Numerical methods for MLEs</h2>
<h3>4.5.1 &nbsp; Newton-Raphson algorithm</h3>
<p>Maximum likelihood requires the solution to an equation involving the derivative of the log-likelihood. The Newton-Raphson algorithm is an iterative algorithm to solve an equation such as this.</p>
<h3>4.5.2 &nbsp; Log-series distribution</h3>
<p>A new type of discrete distribution called the log-series distribution is described. The Newton-Raphson algorithm is used to find the maximum likelihood estimate of the distribution's unknown parameter.</p>
<h3>4.5.3 &nbsp; Standard error</h3>
<p>The standard error of the maximum likelihood estimator can be easily found from the iterations of the Newton-Raphson algorithm.</p>
<h2>4.6 &nbsp; Confidence intervals</h2>
<h3>4.6.1 &nbsp; Interval estimates</h3>
<p>A point estimate (single value) may be far from the actual parameter. An interval estimate is a range of values that gives an indication of accuracy.</p>
<h3>4.6.2 &nbsp; Distribution of estimation error</h3>
<p>Many estimators are approximately normally distributed, unbiased, and their standard error can be found. This allows us to sketch the approximate distribution of the estimation error.</p>
<h3>4.6.3 &nbsp; Confidence intervals</h3>
<p>An approximate 95% confidence interval can be found as the estimate Â± 1.96 standard errors. Replacing 1.96 with other constants gives intervals with other confidence levels.</p>
<h3>4.6.4 &nbsp; Other examples</h3>
<p>Examples are given in which parameters in geometric and log-series distributions are estimated with confidence intervals.</p>
<h3>4.6.5 &nbsp; Properties of confidence intervals</h3>
<p>A 95% confidence interval has probability 0.95 of including the true parameter value. However from a single random sample, we cannot tell whether the CI will actually be one of these lucky ones.</p>
</body>
</html>
