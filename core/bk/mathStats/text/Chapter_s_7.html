<!DOCTYPE HTML>
<html>
<head>
  <title>7. Some Flexible Models</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 7 &nbsp; Some Flexible Models</h1>
<h1 class="sectionName">7.1 &nbsp; Over-dispersion of counts</h1>
<h2 class="pageName">7.1.1 &nbsp; Locations of items in space</h2>
<p>Poisson processes were introduced to model events that happen at random over time. It is also applicable to &quot;events&quot; that arise on other 1-dimensional continua such as flaws in a length of fabric.</p>
<p>Poisson processes can also be generalised to a 2-dimensional surface or 3-dimensional volume. For example, a 2-dimensional Poisson process might be used to model the location of items (such as animals or plants) in a study area. A Poisson process assumes that</p>
<ul>
	<li>events in any part of the 1-, 2- or 3-dimensional space arise independently from those elsewhere, and</li>
	<li>multiple events don't occur simultaneously.</li>
</ul>
<p>A <strong>homogeneous</strong> Poisson process also assumes that</p>
<ul>
	<li>the probability of an event in any small part of the space is the same as that in any other part of the same size.</li>
</ul>
<p class="heading">Poisson distribution</p>
<p>In any homogeneous Poisson process with rate \(\lambda\) events per unit size, the number of events in a period of time (or length or area) of size \(t\) has a \(\PoissonDistn(\lambda t)\) distribution.</p>
<div class="example">
	<p class="exampleHeading">Location of houses</p>
	<p>West of Tokyo lies a large alluvial plain, dotted by a network of farming villages.  A researcher analysed the positioning of the 911 houses making up one of those villages.  The area studied was a rectangle, 3km by 4km.  A grid was superimposed over a map of the village, dividing its 12 square kilometres into 1200 plots, each 100 metres on a side. The number of houses located on each of those plots is displayed in the 30 &times; 40 matrix shown below.</p>

<div class="centred">
	<table border="0" cellpadding="5" cellspacing="0" class="centred"><tr><td style="border:1px solid #999999; letter-spacing:5px; background-color:#FFFFFF">2221010012000012010122011201111211201202<br>
0201201112201100010102201221210010102012<br>
1011001011101011012020013012102112001022<br>
0111020120002200010012000100010900011111<br>
1200000000102022012101110301201111001031<br>
1310101000002202001001000012111210213111<br>
0100010101201311413101100000002220120301<br>
0010100100130010010010220200121220011001<br>
0110110113113010201000133200001010100010<br>
0000011200152000020021010020001001000120<br>
0200111011102142101220112100001220000000<br>
0001101000012220001013120000021200020111<br>
0100120000000110111121113010110141120102<br>
0001111011000012011113021000020003020112<br>
0110001120010010020001100011100002002100<br>
3411031000200010121001410022000101110440<br>
0010011111100102032022310011013001101110<br>
1101010021002200215200000000100122002101<br>
0301000200020000010200001100200000013001<br>
0110201000001100101000112110000110100212<br>
1000110011100210000130221401001030011010<br>
0211011000110031100101025211012001101200<br>
0000020111200112101003214502111120200101<br>
0011200010011000002001221003311010000010<br>
1011001122110000010021100000110011002002<br>
0011111000221200021000001103001207102002<br>
0111122200203101010010001113101021210001<br>
0210002120000010301100010010002211010110<br>
0000100200000001100110100110111201021011<br>
2001200000100112132000000000100011110210</td></tr></table>
</div>

	<p>If houses are located in this village as a homogeneous Poisson process, then these 1,200 counts will be a random sample from a  \(\PoissonDistn(\lambda)\) distribution in which \(\lambda\) is the rate of houses per \(10,000\text{ m}^2\).</p>
</div>


<h2 class="pageName">7.1.2 &nbsp; Overdispersion in Poisson distribution</h2>
<p>The assumptions underlying a homogeneous Poisson process are sometimes violated.</p>
<dl>
	<dt>Constant \(\lambda\)</dt>
	<dd>The rate of events, \(\lambda\) may vary over time or space.</dd>
	<dt>Independence</dt>
	<dd>The occurrence of events may be affected by the occurrence of events in neighbouring times or places.</dd>
</dl>
<p>These two problems often result in a <strong>more variable  counts</strong> than would be expected from a Poisson distribution — called <strong>overdispersion</strong>.</p>
<div class="example">
	<p class="exampleHeading">Location of houses</p>
	<p>For a homogeneous Poisson process with rate \(\lambda\) houses per unit area, the MLE for \(\lambda\) is</p>
\[
\hat{\lambda} \;\;=\;\; \overline{X} \;\;=\;\; \frac {911}{1200} \;\;=\;\; 0.7592
\]
	<p>The table below shows the sample proportions for each of the counts and the best-fitting Poisson probabilities using \(\hat{\lambda}\) above.</p>

<div class="centred">
	<table border="0" cellpadding="5" cellspacing="0" class="centred">
		<tr>
			<th>    No of houses,     <br>
			\(x\)</th>
			<th>Sample<br>proportion</th>
			<th>Poisson probability,<br>\(p(x)\)</th>
		</tr><tr><td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">0</td><td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">0.4867</td><td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">0.4681</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">1</td><td align="center" bgcolor="#FFFFFF">0.3317</td><td align="center" bgcolor="#FFFFFF">0.3553</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">2</td><td align="center" bgcolor="#FFFFFF">0.1400</td><td align="center" bgcolor="#FFFFFF">0.1349</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">3</td><td align="center" bgcolor="#FFFFFF">0.0292</td><td align="center" bgcolor="#FFFFFF">0.0341</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">4</td><td align="center" bgcolor="#FFFFFF">0.0075</td><td align="center" bgcolor="#FFFFFF">0.0065</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">5</td><td align="center" bgcolor="#FFFFFF">0.0033</td><td align="center" bgcolor="#FFFFFF">0.0010</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">6</td><td align="center" bgcolor="#FFFFFF">0.0000</td><td align="center" bgcolor="#FFFFFF">0.0001</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">7</td><td align="center" bgcolor="#FFFFFF">0.0008</td><td align="center" bgcolor="#FFFFFF">0.0000</td></tr>
<tr><td align="center" bgcolor="#FFFFFF">8</td><td align="center" bgcolor="#FFFFFF">0.0000</td><td align="center" bgcolor="#FFFFFF">0.0000</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">9</td><td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">0.0008</td><td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">0.0000</td></tr>

		<tr>
			<th>Total</th>
			<th>1.0000</th>
			<th>1.0000</th>
		</tr>
	</table>
</div>

	<p>Zeros and large counts arise more often than expected from a Poisson distribution.</p>
	<p>The sample variance is \(S^2 = 0.8902\) which is greater than the sample mean, \(\overline{X} = 0.7592\). Since the mean and variance of a Poisson distribution are equal, this also suggests some overdispersion in the distribution.</p>
</div>


<h2 class="pageName">7.1.3 &nbsp; Generalised negative binomial distribution</h2>

<p class="heading">Model for overdispersion</p>

<p>The Poisson distribution's variance always equals its mean. Another distribution with <strong>two</strong> parameters is needed to allow the variance to be <strong>greater</strong> than the mean.</p>

<div class="definition">
<p class='definitionTitle'>Definition</p>
<p>A random variable, \(X\), is said to have a <strong>generalised negative binomial distribution</strong></p>
\[
X \;\;\sim\;\; \NegBinDistn(\kappa, \pi)
\]
<p>if its probability function is</p>
\[
p(x) \;\;=\;\; \begin{cases} \displaystyle \frac{\Gamma(\kappa + x)}{x! \; \Gamma(\kappa)} \pi^{\kappa} (1-\pi)^x  &amp;\text{for }x = 0, 1, 2, \dots \\[0.2em]
0 &amp; \text{otherwise}
\end{cases} \]
<p>where \(\kappa \gt 1\) and \(0 \le \pi \lt 1\). The two parameters \(\kappa\) and \(\pi\) are usually two unknown.</p>
</div>

<p>This is a generalisation of the negative binomial distribution for the number failures before the \(\kappa\)'th success in a sequence of independent success/failure trials with probability \(\pi\) of success, but we now allow non-integer values of \(\kappa\).</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Mean and variance</p>
		<p>The mean and variance of the generalised negative binomial distribution are</p>
\[
E[X] = \frac {\kappa(1-\pi)} \pi \spaced{and} \Var(X) =  \frac {\kappa(1-\pi)} {\pi^2}
\]
<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>From these formulae,</p>
\[
\Var(X) \;=\;  E[X] \times \frac 1 {\pi}
\]
<p>Since \(\pi \lt 1\), \(\Var(X) \gt E[X]\), allowing it to model data with overdispersion.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Asymptotic distribution</p>
		<p>If \(\kappa \to \infty\) and \(\pi \to 1\) simultaneously with \(\displaystyle \frac{\kappa(1-\pi)}{\pi} = \lambda\), the negative binomial distribution approaches a \(\PoissonDistn(\lambda)\) distribution.</p>
		<p class="theoremNote">(Not proved)</p>
	</div>
</div>
<p>This shows that the negative binomial distribution can be made arbitrarily close to a Poisson distribution with appropriate choice of \(\kappa\) and \(\pi\).</p>
<p class="heading">Justification for the negative binomial distribution</p>
<p>The negative binomial distribution is often simply used as an empirical model with the flexibility to model overdispersed count data.. However it can also be theoretically derived in two different ways:</p>
<dl>
	<dt>Varying \(\lambda\)</dt>
	<dd>If the rate of events in a Poisson process, \(\lambda\), varies according to a particular distribution called a Gamma distribution, the number of events has a negative binomial distribution.</dd>
	<dt>Clusters of events</dt>
	<dd>Events sometimes arise in clusters. If the clusters arise as a Poisson process and the number of events per cluster has a log-series distribution, then the number of events has a negative binomial distribution. </dd>
</dl>

<p class="heading">Shape of distribution</p>
<p>We now illustrate the additional flexibility provided by the negative binomial.</p>

<p class="eqn"><img src="../../../en/overdispersion/images/s_negBinomShape.png" width="522" height="435" alt=""/></p>
<p>The top diagram shows the distribution of the number of events in a homogeneous Poisson process in which \(E[X] = \Var(X) = 3\).</p>
<p>The two negative binomial distributions also have \(E[X] = 3\), but \(\Var(X) \gt 3\), so they might be used to fit over-dispersed counts with more zeros and high values than would be expected from a homogeneous Poisson process.</p>



<h2 class="pageName">7.1.4 &nbsp; Overdispersion in binomial distribution</h2>
<p>To use a binomial distribution to model the number of successes in a sequence of success/failure trials, we must assume that</p>
<ul>
	<li>All trials are independent.</li>
	<li>The probability of success, \(\pi\), is the same for each trial.</li>
</ul>
<p>If \(\pi\) varies or the results of successive trials are positively related, there is more chance of a very low or very high count  than a binomial distribution would give — <strong>overdispersion</strong>.</p>
<div class="example">
	<p class="exampleHeading">Sex of babies</p>
	<p>The number of male children among the first 12 children in 6,115 families of size 13, were recorded from hospital records in 19th century Saxony. If the sexes of different children were independent and  each child had the same probability of being male, \(\pi\), this would be a random sample from a \(\BinomDistn(n=12, \; \pi)\) distribution.</p>
	<p>Using maximum likelihood, \(
\hat{\pi} \;=\;  
0.5192\) and the table below shows the resulting binomial probabilities alongside the sample proportions.</p>

<div class="centred">
	<table border="0" cellpadding="3" cellspacing="0" class="centred">
		<tr>
			<th>&nbsp;&nbsp;Number of&nbsp;&nbsp;<br>males, \(x\)</th>
			<th>Sample<br>&nbsp;&nbsp;proportion&nbsp;&nbsp;</th>
			<th>Binomial<br>&nbsp;&nbsp;probability, \(p(x)\)&nbsp;&nbsp;</th>
		</tr><tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999; border-top:1px solid #999999;">0</td>
<td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">0.0005</td>
<td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999; border-top:1px solid #999999;">0.0002</td></tr>

<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">1</td><td align="center" bgcolor="#FFFFFF">0.0039</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0020</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">2</td><td align="center" bgcolor="#FFFFFF">0.0170</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0117</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">3</td><td align="center" bgcolor="#FFFFFF">0.0468</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0423</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">4</td><td align="center" bgcolor="#FFFFFF">0.1096</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.1027</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">5</td><td align="center" bgcolor="#FFFFFF">0.1689</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.1775</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">6</td><td align="center" bgcolor="#FFFFFF">0.2196</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.2236</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">7</td><td align="center" bgcolor="#FFFFFF">0.1818</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.2070</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">8</td><td align="center" bgcolor="#FFFFFF">0.1356</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.1397</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">9</td><td align="center" bgcolor="#FFFFFF">0.0782</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0671</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">10</td><td align="center" bgcolor="#FFFFFF">0.0296</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0217</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">11</td><td align="center" bgcolor="#FFFFFF">0.0074</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">0.0043</td></tr>
<tr><td align="center" bgcolor="#FFFFFF" style="border-left:1px solid #999999; border-bottom:1px solid #999999;">12</td><td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">0.0011</td><td align="center" bgcolor="#FFFFFF" style="border-right:1px solid #999999; border-bottom:1px solid #999999;">0.0004</td></tr>

	</table>
</div>

	<p>There were more families with 3 or fewer males and with 9 or more males than the binomial model would predict, indicating overdispersion.</p>
	<p>The  variance of the best binomial model is</p>
\[
\Var(X) \;=\; 12 \times \hat{\pi} (1 -  \hat{\pi}) \;=\; 
2.996\]
	<p>whereas the actual sample variance was 3.490, again indicating overdispersion.</p>
	<hr width="75%">
	<p>This gives strong evidence that the assumptions underlying the binomial model do not hold. The most likely reason is that the probability of a  child being male is not constant, but varies from family to family.</p>
</div>


<h2 class="pageName">7.1.5 &nbsp; Beta-binomial distribution</h2>

<p class="heading">Model for overdispersion in success/failure data</p>

<p>A model that generalises the binomial distribution to allow for overdispersion is the <strong>beta-binomial</strong> distribution.</p>

<div class="definition">
<p class='definitionTitle'>Definition</p>
<p>A random variable, \(X\), has a <strong>beta-binomial</strong> distribution if its probability function is</p>
\[
p(x) \;\;=\;\; \begin{cases} \displaystyle {n \choose x} \frac {B(x + \alpha, n - x + \beta)}{B(\alpha, \beta)}  &amp;\text{for }x = 0, 1, 2, \dots, n \\[0.4em]
0 &amp; \text{otherwise}
\end{cases} \]
<p>where \(\alpha \gt 0\), \(\beta \gt 0\) and</p>
\[
B(a, b) \;\;=\;\; \frac {\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\]
</div>

<p>The following are given without proof:</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Mean and variance</p>
		<p>The mean and variance of the beta-binomial distribution are</p>
		<p>\[
			E[X] = \frac {n\alpha}{\alpha + \beta} \spaced{and} \Var(X) =   \frac {n\alpha\beta}{(\alpha + \beta)^2}\times \frac {\alpha + \beta + n} {\alpha + \beta + 1}
			\]</p>
	</div>
</div>
<p>If we write</p>
<p>\[
	\pi = \frac {\alpha}{\alpha + \beta} \spaced{so} (1-\pi) =    \frac {\beta}{\alpha + \beta}
	\]</p>
<p>then</p>
<p>\[
	E[X] = n\pi \spaced{and} \Var(X) =   n\pi (1 - \pi) \times \frac {\alpha + \beta + n} {\alpha + \beta + 1}
	\]</p>
<p>The distribution's variance  is  \(\frac {\alpha + \beta + n} {\alpha + \beta + 1}\) times the variance of the binomial distribution with the same mean. Since this factor is greater than 1, the beta-binomial distribution can be used as a model when there is overdispersion.</p>
<p class="heading">Probabilities in Excel</p>
<p>The following Excel functions help evaluate beta-binomial probabilities:</p>

<div class="centred">
<table border="0" cellpadding="5" cellspacing="0" class="centred">
	<tr>
		<th>Maths function</th>
		<th>In Excel</th>
	</tr>
	<tr>
		<td>\(\displaystyle {n \choose x}\)</td>
		<td>=COMBIN(n, x)</td>
	</tr>
	<tr>
		<td>\(\Gamma(k)\)</td>
		<td>=EXP(GAMMALN(k))</td>
	</tr>
</table>
</div>

<p class="heading">Relationship to binomial distribution</p>
<p>The beta-binomial distribution can be made arbitrarily close to a binomial distribution with suitable choice of \(\alpha\) and \(\beta\).</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Asymptotic distribution</p>
		<p>If \(\alpha \to \infty\) and \(\beta \to \infty\) simultaneously with \(\dfrac {\alpha}{\alpha + \beta} = \pi\), the beta-binomial distribution approaches a \(\BinomDistn(n, \pi)\) distribution.</p>
	</div>
</div>
<p class="heading">Shape of  distribution</p>
<p>The following diagram shows a few  distributions that could be used for the number of successes in \(n = 10\) success/failure trials. The top distribution is the binomial distribution.</p>
<p class="eqn"><img src="../../../en/overdispersion/images/s_betaBinomShape.png" width="521" height="610" alt=""/></p>
<p>The three beta-binomial distributions all have the <strong>same mean</strong> as the binomial distribution, but their variances are greater — they have more chance of 0 or 10 successes.</p>


<h1 class="sectionName breakBefore">7.2 &nbsp; Varying hazard rate</h1>
<h2 class="pageName">7.2.1 &nbsp; Weibull distribution</h2>
<p class="heading">Lifetime distributions</p>
<p>The \(\ExponDistn(\lambda)\) distribution is an appropriate model for the lifetime of an item if its hazard function is constant, \(h(x) = \lambda\). This is unrealistic in most applications — usually items become more likely to fail as they age and wear down.</p>
<p>The Weibull distribution is a more general model that allows the hazard rate to increase or decrease over time.</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
<p>A random variable \(X\) is said to have a <strong>Weibull distribution</strong> with parameters \(\alpha \gt 0\) and  \(\lambda \gt 0\),</p>
\[
X \;\;\sim\;\; \WeibullDistn(\alpha,\; \lambda)
\]
<p>if its probability density function is</p>
\[
f(x) \;\;=\;\; \begin{cases} \alpha \lambda^{\alpha} x^{\alpha - 1} e^{-(\lambda x)^{\alpha}} &amp; x \gt 0 \\[0.4em]
0 &amp; \text{otherwise}
\end{cases} \]
</div>

<p>The Weibull distribution's hazard function has a particularly simple form.</p>

<div class="theoremProof">
<div class="theorem">
<p class="theoremTitle">Weibull hazard function</p>
<p>If a random variable \(X\) has a \(\WeibullDistn(\alpha, \lambda)\) distribution, its hazard function is</p>
\[
h(x) \;\;=\;\; \alpha \lambda^{\alpha} x^{\alpha - 1} \]
<p class="theoremNote">(Proved in full version)</p>
</div>
</div>

<p>Since \(h(x) \;\;\propto\;\; x^{\alpha - 1}\), the Weibull distribution can be used as a model for items that either deteriorate or improve over time.</p>
<dl>
	<dt>\(\alpha \gt 1\)</dt>
	<dd>The hazard function \(h(x)\) is an increasing function of \(x\) so the item becomes <strong>less reliable</strong> as it gets older.</dd>
	<dt>\(\alpha \lt 1\)</dt>
	<dd>The hazard function \(h(x)\) is a decreasing function of \(x\) so the item becomes <strong>more reliable</strong> as it gets older.</dd>
	<dd></dd>
	<dt>\(\alpha = 1\)</dt>
	<dd>The hazard function \(h(x)\) is constant and the lifetime distribution is an exponential distribution.</dd>
</dl>


<h2 class="pageName">7.2.2 &nbsp; Mean, variance and shape</h2>
<div class="theoremProof">
	<div class="theorem">
<p class="theoremTitle">Mean and variance of Weibull distribution</p>
<p>If a random variable \(X\) has a Weibull distribution with probability density function</p>
\[
f(x) \;\;=\;\; \begin{cases} \alpha \lambda^{\alpha} x^{\alpha - 1} e^{-(\lambda x)^{\alpha}} &amp; x \gt 0 \\[0.4em]
0 &amp; \text{otherwise}
\end{cases} \]
<p>then its mean and variance are</p>
\[
E[X] \;=\; \frac 1 {\lambda} \Gamma\left(1 + \frac 1 {\alpha}\right)
\spaced{and} \Var(X) \;=\; \frac 1 {\lambda^2} \left( \Gamma\left(1 + \frac 2 {\alpha}\right) - \Gamma\left(1 + \frac 1 {\alpha}\right)^2\right)
\]
<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>

<p>We now show how the shape of the Weibull distribution is affected by its two parameters. The two distributions below both have mean \(E[X] = 2\).</p>
<p class="eqn"><img src="../../../en/weibull/images/s_weibull1.png" width="517" height="510" alt=""/></p>
<p>When \(\alpha = 0.5\),</p>
\[
h(x) \;\;\propto\;\; x^{\alpha - 1} \;\;=\;\; \frac 1{\sqrt{x}}\]
<p>When \(x \approx 0\), the hazard rate is extremely high, making the item  very likely to fail near the start of its life. However the hazard rate drops as the item gets older (as \(x\) increases) so as the item survives longer, it becomes less likely to fail — some items  survive <strong>very</strong> long times, well beyond the upper end of the axis in the diagram.</p>
<p class="eqn"><img src="../../../en/weibull/images/s_weibull2.png" width="511" height="190" alt=""/></p>
<p>In this Weibull distribution, the hazard rate starts low then increases over time.</p>


<h2 class="pageName">7.2.3 &nbsp; Calculating Weibull probabilities</h2>
<p>Probabilities for the Weibull distribution are usually found from the cumulative distribution function.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Cumulative distribution function</p>
		<p>If \(X \sim \WeibullDistn(\alpha, \lambda)\) its cumulative distribution function is</p>
		\[
		F(x) \;\;=\;\; P(X \le x) \;\;=\;\; 1 - e^{-(\lambda x)^{\alpha}} \] 
		<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>Given values of \(x\), \(\alpha\) and \(\lambda\), these probabilities can be evaluated on a scientific calculator. Excel also has a function to evaluate cumulative Weibull probabilities, but its third parameter is the inverse of \(\lambda\), rather than \(\lambda\) itself. The cumulative probability could be found by typing into a spreadsheet cell</p>
<p class="eqn">=WEIBULL.DIST( \(x\),  \(\alpha\),   \(1/\lambda\),  true )</p>
<p>Although the parameter \(\alpha\) has a meaningful interpretation since \(h(x) \propto x^{\alpha - 1}\), the value of the parameter \(\lambda\) is <strong>not</strong> easily interpreted. The <strong>mean</strong> lifetime of the items is an easier value to interpret than  \(\lambda\) itself,</p>
\[
E[X] \;=\; \frac 1 {\lambda} \Gamma\left(1 + \frac 1 {\alpha}\right)
\]
<div class="questionSoln">
	<div class="question">
		<p class="questionTitle">Question</p>
		<p>If an item's hazard rate is proportional to the square root of its age, and its mean lifetime is 3 years, what is the probability that it will survive for longer than 10 years?</p>
		<p class="questionNote">(Solved in full version)</p>
	</div>
</div>
<p>We now give an example in which the hazard rate <strong>decreases</strong> over time.</p>
<div class="questionSoln">
	<div class="question">
		<p class="questionTitle">Question</p>
		<p>If the item's hazard rate was <strong>inversely </strong>proportional to the square root of its age, and its mean lifetime is 3 years, what would be the corresponding probability of surviving for longer than 10 years? 40 years?</p>
		<p class="questionNote">(Solved in full version)</p>
	</div>
</div>


<h1 class="sectionName breakBefore">7.3 &nbsp; Gamma distribution</h1>
<h2 class="pageName">7.3.1 &nbsp; Distribution for positive variables</h2>
<p>We now describe a family of distributions that can be used to model &quot;quantity&quot; variables — ones that can only take positive values. The Gamma distribution is a generalisation of the \(\ErlangDistn(k,\; \lambda)\) distribution that allows non-integer values for the parameter \(k\). By convention, Erlang parameters \(k\) and \(\lambda\) are denoted by the symbols \(\alpha\) and \(\beta\) in Gamma distributions.</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
	<p>A random variable \(X\) is said to have a <strong>Gamma distribution</strong> with parameters \(\alpha \gt 0\) and  \(\beta \gt 0\),</p>
	\[
	X \;\;\sim\;\; \GammaDistn(\alpha,\; \beta)
	\]
	<p>if its probability density function is</p>
	\[
	f(x) \;\;=\;\; \begin{cases} \dfrac {\beta^\alpha }{\Gamma(\alpha)} x^{\alpha - 1} e^{-x\beta}&amp; \quad\text{if }x \gt 0 \\
	0 &amp; \quad\text{otherwise}
	\end{cases} \] </div>
<p>The exponential distribution is a special case of the gamma distribution when \(\alpha = 1\). The distribution becomes increasingly skew as \(\alpha\) decreases from this value. The two Gamma distributions below both have mean \(E[X] = 2\).</p>
<p class="eqn"><img src="../../../en/gamma/images/s_gamma1.png" width="518" height="470"  alt=""/></p>
<p>When \(\alpha\) increases, the mode of the distribution (where its density is highest) increases from zero and the distribution's shape becomes more symmetric. The two Gamma distributions below again both have \(E[X] = 2\).</p>
<p class="eqn"><img src="../../../en/gamma/images/s_gamma2.png" width="518" height="296"  alt=""/></p>
<p class="heading">Comparison of Gamma and Weibull distributions</p>
<p>The Gamma and Weibull distributions  are both generalisations of the exponential distribution — exponential distributions are special cases when \(\alpha = 1\) and both can be used as models for lifetime data. The main differences between them arise in the <strong>tails</strong> of the distributions, especially when \(\alpha\) is positive.</p>
<table border="0" cellpadding="5" cellspacing="0" class="centred">
	<tr>
		<td>\(\WeibullDistn(\alpha,\; \lambda)\):        </td>
		<td>\(f(x) \propto x^{\alpha - 1} e^{-(\lambda x)^{\alpha}}\)</td>
	</tr>
	<tr>
		<td>\(\GammaDistn(\alpha,\; \beta)\):        </td>
		<td>\(f(x) \propto x^{\alpha - 1} e^{-\beta x}\)</td>
	</tr>
</table>
<p>When \(\alpha \gt 1\), the Weibull distribution's upper tail decreases much faster than the Gamma distribution's upper tail, so the Gamma distribution has a longer upper tail (and is more skew).</p>
<p>In many applications, the Gamma distribution's longer tail matches what is seen (or expected) in sample data.</p>


<h2 class="pageName">7.3.2 &nbsp; Gamma probabilities and quantiles</h2>

<p class="heading">Cumulative distribution function</p>
<p>The cumulative distribution function of the Gamma distribution is</p>
\[
		F(x) \;\;=\;\; P(X \le x) \;\;=\;\; \int_0^x {\frac {\beta^\alpha }{\Gamma(\alpha)} u^{\alpha - 1} e^{-u\beta}} \;du \]
		<p>This integral cannot be simplified and can only  be evaluated numerically. In Excel, the following function can be used.</p>
		<p class="eqn">= GAMMA.DIST( \(x\), \(\alpha\), \(\beta\), true)</p>
<div class="example">
	<p class="exampleHeading">Question</p>
			<p>If a random variable, \(X\), has a Gamma distribution</p>
\[
	X \;\;\sim\;\; \GammaDistn(\alpha = 7,\; \beta = 12)
	\]
	<p>what is the probability of getting a value between 0.5 and 1.0?</p>
	<p class="exampleNote">(Solved in full version)</p>
</div>
<p class="heading">Quantiles from Gamma distributions</p>
		<p>In a similar way, there is no algebraic formula for the quantiles of a Gamma distribution, but computer algorithms are available to find them numerically.	To	find the value \(x\) such that \(F(x) = q\), the following Excel function can be used.</p>
		<p class="eqn">= GAMMA.INV( \(q\), \(\alpha\), 1/\(\beta\))</p>
<div class="example">
	<p class="exampleHeading">Question</p>
			<p>If a random variable, \(X \sim \GammaDistn(\alpha = 7,\; \beta = 12)\), what is the lower quartile of its distribution?		</p>
			<p class="exampleNote">(Solved in full version)</p>
</div>


<h2 class="pageName">7.3.3 &nbsp; Some Gamma distribution properties</h2>
<p>We now give formulae for the mean and variance of the Gamma distribution.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Mean and variance</p>
		<p>If a random variable \(X\) has a Gamma distribution with probability density function</p>
\[
	f(x) \;\;=\;\; \begin{cases} \dfrac {\beta^\alpha }{\Gamma(\alpha)} x^{\alpha - 1} e^{-x\beta}&amp; \text{if }x \gt 0 \\
	0 &amp; \text{otherwise}
	\end{cases} \]
	<p>then its mean and variance are</p>
		\[
		E[X] \;=\; \frac{\alpha}{\beta}
		\spaced{and} \Var(X) \;=\; \frac{\alpha}{\beta^2}
		\] 
		<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>The sum of  independent \(\ErlangDistn(k_1,\; \lambda)\) and \(\ErlangDistn(k_2,\; \lambda)\) random variables has an \(\ErlangDistn(k_1 + k_2,\; \lambda)\) distribution, and the same holds for the sum of Gamma random variables, provided their second parameters are equal.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Additive property of Gamma distributions</p>
		<p>If \(X_1 \sim \GammaDistn(\alpha_1,\; \beta)\) and \(X_2 \sim \GammaDistn(\alpha_2,\; \beta)\) are independent, then</p>
		\[
		X_1 + X_2 \;\;\sim\;\;  \GammaDistn(\alpha_1 + \alpha_2,\; \beta)
		\] 
		<p class="theoremNote">(Not proved)</p>
	</div>
</div>
<p>The Central Limit Theorem can be used to give a normal approximation to the Gamma distribution when \(\alpha\) is large.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Asymptotic normal distribution</p>
		<p>The shape of the \(\GammaDistn(\alpha,\; \beta)\) distribution approaches that of a normal distribution as \(\alpha \to \infty\)</p>
		<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>


<h1 class="sectionName breakBefore">7.4 &nbsp; Beta distribution</h1>
<h2 class="pageName">7.4.1 &nbsp; Values between zero and one</h2>
<p>Occasionally variables can only take values within a restricted range. The family of beta distributions is flexible enough to model many  variables that must take values between zero and one.</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
	<p>A random variable \(X\) is said to have a <strong>Beta distribution</strong> with parameters \(\alpha \gt 0\) and  \(\beta \gt 0\),</p>
	\[
	X \;\;\sim\;\; \BetaDistn(\alpha,\; \beta)
	\]
	<p>if its probability density function is</p>
	\[
	f(x) \;\;=\;\; \begin{cases} \dfrac {\Gamma(\alpha +\beta) }{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}&amp; \text{if }0 \lt x \le 1 \\[0.4em]
	0 &amp; \text{otherwise}
	\end{cases} \] </div>
<p>A special case of the beta distribution arises when \(\alpha = \beta = 1\):</p>

\[
\BetaDistn(\alpha = 1,\; \beta = 1) \;\;\equiv\;\; \RectDistn(0, 1)
\]

<p>Larger values of the parameters decrease the spread of the distribution. The following Beta distributions all have mean \(E[X] = 0.4\).</p>
<p class="eqn"><img src="../../../en/beta/images/s_betaShape1.png" width="517" height="450"  alt=""/></p>
<p>On the other hand, smaller values &quot;push the distribution towards zero and one&quot;.</p>
<p class="eqn"><img src="../../../en/beta/images/s_betaShape2.png" width="517" height="600"  alt=""/></p>


<h2 class="pageName">7.4.2 &nbsp; Mean and variance</h2>
<p>Deriving the mean and variance of the Beta distribution requires the following result.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">A useful integral</p>
		<p>For any constants \(a \gt 0\) and \(b \gt 0\),</p>
\[
\int_0^1{x^{a - 1} (1 - x)^{b - 1}} dx \;\;=\;\; \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)}
\]
	</div>
</div>
<p>(This result  can be used to prove that the beta distribution's pdf integrates to 1.)</p>

<div class="theoremProof">
<div class="theorem">
<p class="theoremTitle">Mean and variance of beta distribution</p>
<p>If a random variable, \(X\), has a beta distribution with pdf</p>
\[
	f(x) \;\;=\;\; \begin{cases} \dfrac {\Gamma(\alpha + \beta) }{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}&amp; \text{if }0 \lt x \le 1 \\
	0 &amp; \text{otherwise}
	\end{cases} \]
<p>its mean and variance are</p>
\[
E[X] \;=\; \frac{\alpha}{\alpha + \beta} \spaced{and} \Var(X) \;=\; \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\]
<p class="theoremNote">(Proved in full version)</p>
</div>
</div>


<h1 class="sectionName breakBefore">7.5 &nbsp; Normal distribution</h1>
<h2 class="pageName">7.5.1 &nbsp; Standard normal distribution</h2>
<p>The family of normal distributions is  flexible enough to be used as a model for many practical variables.</p>

<div class="definition">
<p class='definitionTitle'>Definition</p>
<p>A random variable, \(X\), is said to have a <strong>normal</strong> distribution,</p>
\[
X \;\; \sim \; \; \NormalDistn(\mu,\; \sigma^2)
\]
<p>if its probability density function is</p>
\[
f(x) \;\;=\;\; \frac 1{\sqrt{2\pi}\;\sigma} e^{- \frac{\large (x-\mu)^2}{\large 2 \sigma^2}} \qquad \text{for } -\infty \lt x \lt \infty
\]</div>

<p>Normal distributions are symmetric and the two parameters only affect the centre and spread of the distribution.</p>
<p class="eqn"><img class="svgImage" src="../../../en/normalDistn/images/normalDensity.gif" width="498" height="145"> </p>
<p class="heading">Standard normal distribution</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
	<p>A <strong>standard normal</strong> distribution is one whose parameters are \(\mu = 0\) and \(\sigma = 1\),</p>
\[
	Z \;\; \sim \; \; \NormalDistn(0,\; 1)
	\]
<p>A random variable, \(Z\) with a standard normal distribution is often called a <strong>z-score</strong>.</p>
</div>
<p>If \(Z\) has a  standard normal distribution, its pdf has a particularly simple form:</p>
\[
f(z) \;\;=\;\; \frac 1{\sqrt{2\pi}} e^{- \frac{\large z^2}{\large 2}} \qquad \text{for } -\infty \lt x \lt \infty
\] 


<h2 class="pageName">7.5.2 &nbsp; Mean and variance</h2>
<p>The mean and variance of a general normal distribution, can be found from those of the standard normal distribution.</p>
<div class="theoremProof">
	<div class="theorem">
<p class="theoremTitle">Mean and variance of standard normal distribution</p>
<p>If \(Z \sim \NormalDistn(0,\; 1)\), its mean and variance are</p>
\[
	E[Z] \;=\; 0 \spaced{and} \Var(Z) \;=\; 1
	\]
<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>A change of variable, \(z = \frac {x-\mu}{\sigma}\), can be used to find the mean and variance of a general normal distribution  from this result.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Mean and variance of a general normal distribution</p>
		<p>If \(X \sim \NormalDistn(\mu,\; \sigma^2)\), its mean and variance are</p>
		\[
		E[X] \;=\; \mu \spaced{and} \Var(X) \;=\; \sigma^2
		\]
		<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>This explain why the symbols &quot;\(\mu\)&quot; and &quot;\(\sigma^2\)&quot; are used for the normal distribution's two parameters.</p>


<h2 class="pageName">7.5.3 &nbsp; Z-scores</h2>

<p>The following diagram  describes the probability density function of <strong>any</strong> normal distribution.</p>
<p class=eqn><img src="../../../en/normalDistn/images/normalDensity.svg" width="498" height="145"></p>

<p>It can be used to add a scale appropriate to any values of \(\mu\) and \(\sigma\). For example, the pdf of a \(\NormalDistn(\mu=180, \sigma=10)\) distribution is</p>
<p class=eqn><img src="../../../en/normal/images/s_normalApples.png" width="526" height="212"></p>
<p class="heading">Z-scores</p>
<p>The number of standard deviations from the mean is called a <strong>z-score</strong>.</p>
\[
Z = \frac {X-\mu} {\sigma}
\]
<p>Z-scores  have a standard normal distribution,</p>
\[
	Z \;\; \sim \; \; \NormalDistn(0,\; 1)
	\]
	


<h2 class="pageName">7.5.4 &nbsp; Probabilities for normal distributions</h2>

<p class="heading">Cumulative distribution function</p>

<p>The cumulative distribution function for a \(\NormalDistn(\mu,\; \sigma^2)\) distribution is</p>
\[
F(x) \;\;=\;\; \int_{-\infty}^x {\frac 1{\sqrt{2\pi}\;\sigma} e^{- \frac{\large (u-\mu)^2}{\large 2 \sigma^2}}} du \]
<p>This integration cannot be performed algebraically, but numerical algorithms will find cumulative probabilities for you. For example, in Excel you can use the function</p>
<p class="eqn">= NORM.DIST( \(x\), \(\mu\), \(\sigma\), true)</p>
<p class="heading">Normal probabilities from z-scores</p>
<p>Although  probabilities for any normal distribution can be found as described above,  an alternative method uses z-scores. This lets us find probabilities about a normal random variable using the <strong>standard</strong> normal distribution.</p>
<p class="eqn"><img src="../../../en/normalDistn/images/s_xProb.gif" width="509" height="436"></p>
<p>In Excel, this would be evaluated as</p>
<p class="eqn">=NORM.S.DIST(z, true)</p>
<p>Although this offers few practical advantages when a computer is used,</p>
<ul>
	<li>If a computer is unavailable, tables of cumulative probabilities for the standard normal distribution exist and can be used to find normal probabilities.</li>
	<li>The z-score itself is an informative value — for example, we know that z-scores below -3 or above +3 are very unlikely.</li>
</ul>


<h2 class="pageName">7.5.5 &nbsp; Normal quantiles</h2>
<p>We are sometimes given the value of the probability, \(P(X \le x)\) and need to find the value \(x\). If we are provided with a probability, \(p\), then the value \(x\) such that</p>
\[
	P(X \le x) = p
	\]
<p>is  the \(p\)'th <strong>quantile</strong> of the distribution of \(X\). We now give an example to illustrate the use of quantiles for a normally distributed random variable.</p>
<div class="example">
	<p class="exampleHeading">Example</p>
	<p>If the weight of a Fuji apple has the following normal distribution</p>
	\[
	X \;\; \sim \; \; \NormalDistn(\mu=180, \sigma=10)
	\]
	<p>what is the apple weight that will be exceeded with 95% probability? In other words, we want to find the apple weight \(x\) such that</p>
	\[
	P(X \lt x)  \;\;= \;\; 0.05
	\]
	<p>In terms of z-scores,</p>
\[
	P(X \lt x)  \;= \; P\left(Z \lt \frac {x-180} {10}\right)  \;= \; 
	0.05
\]
	<p>Using the function &quot;<span class="eqn">=NORM.S.INV(0.05)</span>&quot; in Excel, we can find that</p>
\[
	P(Z \lt -1.645) \;\;=\;\; 
	0.05
\]
	<p>Translating back to the original units,</p>
\[
	x \;=\; 180 - 1.645 \times 10 \;=\; 163.55 \text{ grams}
\]</div>


<h2 class="pageName">7.5.6 &nbsp; Linear combinations, sums and means</h2>
<p class="heading">Two independent normal variables </p>
<p><span class="theorem">For any two independent  random variables, \(X\) and \(Y\), with means \(\mu_X\) and \(\mu_Y\) and  variances  \(\sigma_X^2\) and \(\sigma_Y^2\),</span></p>
\[ \begin {align}
		E[aX + bY] &amp; = a\mu_X + b\mu_Y \\[0.5em]
		\Var(aX + bY) &amp; = a^2\sigma_X^2 + b^2\sigma_Y^2
		\end {align} \]
		<p>When \(X\) and \(Y\)have normal distributions, we can be more precise about the distribution's shape.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Linear function of independent normal variables</p>
		<p>If \(X\) and \(Y\) are independent random variables,</p>
\[ \begin {align}
X \;&amp;\sim\; \NormalDistn(\mu_X,\; \sigma_X^2) \\
Y \;&amp;\sim\; \NormalDistn(\mu_Y,\; \sigma_Y^2)
\end {align} \]
		<p>then</p>
\[
aX + bY \;\sim\; \NormalDistn(a\mu_X + b\mu_Y,\; a^2\sigma_X^2 + b^2\sigma_Y^2)
\]		</div>
</div>
<p class="heading">Random sample </p>
<p>This can be extended to the sum of values in a normal random sample.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Sum of a random sample</p>
		<p>If \(\{X_1, X_2, ..., X_n\}\) is a random sample of <em>n</em> values from a  \(\NormalDistn(\mu,\; \sigma^2)\) distribution then,</p>
\[
\sum_{i=1}^n {X_i} \;\sim\; \NormalDistn(n\mu,\; n\sigma^2)
\]	
<p class="theoremNote">(Proved in full version)</p>
	</div>
</div>
<p>A similar result holds for the <strong>mean</strong> of a random sample from a normal distribution.</p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Mean of a random sample</p>
		<p>If \(\{X_1, X_2, ..., X_n\}\) is a random sample of <em>n</em> values from a  \(\NormalDistn(\mu,\; \sigma^2)\) distribution then,</p>
		\[
		\overline{X} \;\sim\; \NormalDistn\left(\mu,\; \frac {\sigma^2}{n}\right)
		\] </div>
</div>


<h2 class="pageName">7.5.7 &nbsp; Independence of sample mean and variance</h2>
<p><span class="theorem">We end this section  with another important result that is stated here without proof.</span></p>
<div class="theoremProof">
	<div class="theorem">
		<p class="theoremTitle">Independence of sample mean and variance</p>
		<p>If \(\{X_1, X_2, \dots, X_n\}\) is a random sample from a \(\NormalDistn(\mu, \sigma^2)\) distribution, the sample variance,</p>
			
			\[
			S^2 \;=\; \frac {\sum_{i=1}^n {(X_i - \overline{X})^2}} {n-1}
			\]
			
			<p>is independent of the sample mean, \(\overline{X}\).</p>
	</div>
</div>
<p>Although we cannot <strong>prove</strong> independence  with the statistical theory that we have covered so far, it can be demonstrated with a simulation. In the scatterplot below, each cross gives the mean and standard deviation from a random sample of 20 values from a \(\NormalDistn(\mu=12,\; \sigma^2 = 2^2)\) distribution.</p>
<p class="eqn"><img src="../../../en/normal/images/s_meanSdIndep.png" width="294" height="285"  alt=""/></p>
<p>The scatterplot is a fairly circular cloud of crosses, so there is no tendency for large sample standard deviations to be associated with either large or small sample means. This supports the independence of the sample mean and standard deviation.</p>


</body>
</html>
