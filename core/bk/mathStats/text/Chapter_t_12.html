<html>
<head>
<title>12. Multivariate distributions</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 12 &nbsp; Multivariate distributions</h1>
<h2>12.1 &nbsp; Discrete bivariate distributions</h2>
<h3>12.1.1 &nbsp; Joint probability function</h3>
<p>The joint probability function of X and Y gives the probabilities for all possible combinations of the variables' values. Probabilities for events can be found by summing the joint probability function.</p>
<h3>12.1.2 &nbsp; Three-dimensional bar chart</h3>
<p>The joint probability function of X and Y can be displayed graphically in a 3-dimensional bar chart.</p>
<h3>12.1.3 &nbsp; Marginal distributions</h3>
<p>The marginal probability function for one variable gives probabilities for its possible values. It can be found by summing the joint probability function over the values of the other variable.</p>
<h3>12.1.4 &nbsp; Conditional distributions</h3>
<p>The conditional probabilities for one variable, given the value of the other, form a univariate distribution. They are the ratio of the joint probability function to the second variable's marginal one.</p>
<h3>12.1.5 &nbsp; Independence</h3>
<p>Two random variables are independent if their joint probability function equals the product of the two marginal ones.</p>
<h3>12.1.6 &nbsp; Random samples</h3>
<p>Independence can be generalised to n variables. If they all have the same distribution, they are a random sample.</p>
<h2>12.2 &nbsp; Continuous bivariate distributions</h2>
<h3>12.2.1 &nbsp; Joint probability density function</h3>
<p>Joint probability density functions describe the distributions of pairs of continuous random variables. Probabilities are defined as volumes under it.</p>
<h3>12.2.2 &nbsp; Probabilities as integrals</h3>
<p>Probabilities can be found as double integrals of the joint probability density function.</p>
<h3>12.2.3 &nbsp; Marginal distributions</h3>
<p>The marginal distribution of one of the two variables is a univariate distribution whose pdf can be found by integrating the joint pdf over the other variable.</p>
<h3>12.2.4 &nbsp; Conditional distributions</h3>
<p>The conditional probability density function of one variable given the value of the other is the joint pdf divided by the marginal pdf of the other variable.</p>
<h3>12.2.5 &nbsp; Independence and random samples</h3>
<p>As with discrete variables, continuous variables are independent if their joint pdf is the product of the two marginal pdfs. A random sample is a collection of n independent variables with the same distribution.</p>
<h2>12.3 &nbsp; Expected values</h2>
<h3>12.3.1 &nbsp; Discrete expected values</h3>
<p>The expected value of a function, g(X,Y), of two discrete random variables, X and Y, is the sum of the possible values of the function with each weighted by its probability — the sum of g(x,y)p(x,y) over all pairs (x,y).</p>
<h3>12.3.2 &nbsp; Continuous expected values</h3>
<p>When X and Y are continuous, the expected value of g(X,Y) is similarly defined but with a double integral of g(x,y)f(x,y) replacing the double summation.</p>
<h3>12.3.3 &nbsp; Properties of expected values</h3>
<p>Some properties of expected values are described. Conditional expected values are also defined here, and the page explains how to obtain unconditional expected values from conditional ones.</p>
<h3>12.3.4 &nbsp; Means and variances</h3>
<p>The means and variances of individual variables can be found as expected values from their marginal distributions.</p>
<h2>12.4 &nbsp; Covariance and correlation</h2>
<h3>12.4.1 &nbsp; Covariance</h3>
<p>This page defines the covariance of two variables and gives some properties.</p>
<h3>12.4.2 &nbsp; Variance of a sum</h3>
<p>The variance of the sum of two independent variables is the sum of their variances. This formula must be modified if the variables are correlated.</p>
<h3>12.4.3 &nbsp; Correlation coefficient</h3>
<p>The correlation coefficient of X and Y is defined. It is unaffected by linear transformations of the two variables.</p>
<h3>12.4.4 &nbsp; Linear relationships</h3>
<p>The correlation coefficient must lie between –1 and +1. The values ±1 can arise if and only if the two variables are exactly linearly related.</p>
<h3>12.4.5 &nbsp; Independence</h3>
<p>If two variables are independent, their covariance and correlation are both zero.</p>
<h2>12.5 &nbsp; Multinomial distribution</h2>
<h3>12.5.1 &nbsp; Joint probability function</h3>
<p>The multinomial distribution is a generalisation of the binomial distribution. It describes situations where each independent trial may have more than two possible outcomes.</p>
<h3>12.5.2 &nbsp; Marginal distributions</h3>
<p>The marginal distributions of single variables are all binomial if their joint distribution distribution is multinomial.</p>
<h3>12.5.3 &nbsp; Conditional distributions</h3>
<p>The conditional distribution of any subset of variables that have a multinomial distribution is also multinomial.</p>
<h3>12.5.4 &nbsp; Means, variances and correlations</h3>
<p>The means and variances of individual variables that have a joint multinomial distribution can be found from their marginal binomial distributions. Formulae for the covariance and correlation of two of the variables are derived.</p>
<h3>12.5.5 &nbsp; Parameter estimation</h3>
<p>The maximum likelihood estimates of the multinomial probabilities are the corresponding sample proportions.</p>
<h2>12.6 &nbsp; Bivariate normal distribution</h2>
<h3>12.6.1 &nbsp; Standard bivariate normal distribution</h3>
<p>The standard bivariate normal distribution has a single parameter, that affects the strength of the relationship between the variables. The joint pdf is given and the distribution's shape is described.</p>
<h3>12.6.2 &nbsp; General bivariate normal distribution</h3>
<p>The general bivariate normal distribution is a generalisation with five parameters. Linear transformations of standard bivariate normal variables have this distribution.</p>
<h3>12.6.3 &nbsp; Marginal distributions</h3>
<p>The marginal distributions are univariate normal distributions and their means and variances are four of the distribution's five parameters.</p>
<h3>12.6.4 &nbsp; Conditional distributions</h3>
<p>The conditional distribution of Y, given that X=x, is also a univariate normal distribution. Its shape is that of a slice through the joint pdf at x.</p>
<h3>12.6.5 &nbsp; Covariance and correlation</h3>
<p>A formula is derived for the covariance of the two variables and their correlation is shown to be the fifth parameter of the bivariate normal distribution.</p>
</body>
</html>
