<!DOCTYPE HTML>
<html>
<head>
  <title>9. More About Estimation</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 9 &nbsp; More About Estimation</h1>
<h1 class="sectionName">9.1 &nbsp; Estimating two parameters</h1>
<h2 class="pageName">9.1.1 &nbsp; Method of moments</h2>

<p class="heading">Models with two unknown parameters</p>
<p>If a family of distributions has a <strong>single</strong> unknown parameter, \(\theta\), the method of moments estimate makes the distribution's mean equal to the mean of a random sample — by solving</p>
\[
\mu(\theta) \;\;=\;\; \overline{x}
\]
<p>However many families of standard distributions involve <strong>two</strong> unknown parameters. The method of moments can be extended to models with two parameters.</p>

<div class="definition">
	<p class='definitionTitle'>Definition</p>
<p>If a distribution has two unknown parameters, \(\theta\) and \(\phi\), the <strong>method of moments </strong>estimates of the parameters are found by solving</p>
\[
\mu(\theta, \phi) \;=\; \overline{x} \spaced{and} \sigma^2(\theta, \phi) \;=\; s^2
\]
<p>where \(\mu(\theta, \phi)\) and \(\sigma^2(\theta, \phi)\) are the mean and variance of the distribution and \(\overline{x}\) and \(s^2\) are the mean and variance of a random sample from it.</p>
</div>

<p>Our first example is almost trivially simple.</p>
<div class="example">
	<p class="exampleHeading">Normal distribution</p>
	<p>If \(X\) has a normal distribution,</p>
	\[
	X \;\;\sim\;\; \NormalDistn(\mu, \sigma^2)
	\]
	<p>then the method of moments estimates of its parameters are</p>
	\[
	\hat{\mu} = \overline{x} \spaced{and} \hat{\sigma}^2 = s^2
	\]</div>
<p>The next example is a little harder.</p>
<div class="questionSoln">
	<div class="question">
		<p class="questionTitle">Question: Negative binomial distribution</p>
		<p>If \(X\) has a generalised negative binomial distribution,</p>
\[
X \;\;\sim\;\; \NegBinDistn(\kappa, \pi)
\]
<p>what are the method of moments estimates of \(\kappa\) and \(\pi\)  from a random sample?</p>
<p class="questionNote">(Solved in full version)</p>
	</div>
</div>
<p class="heading">Three or more  parameters</p>
<p>Unfortunately the method of moments  does not extend easily to models with three or more parameters.</p>


<h2 class="pageName">9.1.2 &nbsp; Maximum likelihood</h2>
<p>Unlike the method of moments,  maximum likelihood estimation can be used for models with <strong>any</strong> number of unknown parameters. We will describe the method for a model with two unknown parameter, \(\theta\) and \(\phi\), but it should be clear how to extend it to three or more parameters.</p>
<p>If \(\{x_1, x_2, \dots, x_n\}\) is a random sample from a discrete distribution with probability function \(p(x\;|\; \theta, \phi)\), the likelihood function is again the probability of getting the observed data for any values of the parameters.</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; p(x_1, x_2, \dots, x_n \;| \; \theta, \phi) \;\;=\;\; \prod_{i=1}^n {p(x_i\;|\; \theta, \phi)}
\]
<p>For a continuous distribution, the corresponding definition is</p>
\[
L(\theta, \phi \; | \; x_1, x_2, \dots, x_n) \;\;=\;\; \prod_{i=1}^n {f(x_i\;|\; \theta, \phi)}
\]
<p class="heading">Maximising the likelihood</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
<p>If a random variable \(X\) has a distribution that involves two unknown parameters, \(\theta\) and \(\phi\), the <strong>maximum likelihood</strong> estimates of the parameters are the values that maximise the likelihood function.</p>
</div>
<p>This is usually at a <strong>turning point</strong> of the likelihood function — where the partial derivatives of \(L(\theta, \phi)\) with respect to \(\theta\) and \(\phi\) are zero,</p>


\[
\frac{\partial L(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial L(\theta, \phi)}{\partial \phi} = 0
\]
<p>Solving these equations give MLEs  for \(\theta\) and \(\phi\). Equivalently, writing \(\ell(\theta, \phi) = \log L(\theta, \phi)\), we can solve the equations</p>
\[
\frac{\partial \ell(\theta, \phi)}{\partial \theta} = 0 \spaced{and} \frac{\partial \ell(\theta, \phi)}{\partial \phi} = 0
\]
<p>This is usually easier and gives identical parameter estimates.</p>


<h2 class="pageName">9.1.3 &nbsp; Example</h2>
<p>We now give an example.</p>
<div class="example">
	<p class="exampleHeading">Example: Normal distribution</p>
	<p>We now consider a random sample, \(\{X_1, \dots, X_n\}\) from a \(\NormalDistn(\mu, \sigma^2)\) distribution. The distribution's probability density function is</p>
	\[
	f(x) \;\;=\;\; \frac 1{\sqrt{2\pi}\;\sigma} e^{- \frac{\Large (x-\mu)^2}{\Large 2 \sigma^2}}
	\]
	<p>and its logarithm is</p>
	\[
	\log f(x) \;\;=\;\; -\frac 1 2 \log(\sigma^2) - \frac{(x-\mu)^2}{2 \sigma^2} - \frac 1 2 \log(2\pi)
	\]
	<p>The log-likelihood function is therefore</p>
\[
	\ell(\mu, \sigma^2) \;\;=\;\; \sum_{i=1}^n {\log f(x_i)} \;\;=\;\; -\frac n 2 \log(\sigma^2) - \frac{\sum_{i=1}^n {(x_i-\mu)^2}}{2 \sigma^2} - \frac n 2 \log(2\pi)
	\]
	<p>To get the maximum likelihood estimates, we therefore solve</p>
\[
\frac{\partial \ell(\mu, \sigma^2)}{\partial \mu} \;\;=\;\; \frac{\sum{(x_i - \mu)}}{\sigma^2} \;\;=\;\; 0
\]
	<p>and</p>
\[
\frac{\partial \ell(\mu, \sigma^2)}{\partial \sigma^2} \;\;=\;\; -\frac n {2 \sigma^2} + \frac{\sum_{i=1}^n {(x_i-\mu)^2}}{2 \sigma^4} \;\;=\;\; 0
\]
	<p>Solving gives</p>
\[ \begin{align}
  \hat{\mu} \;&=\; \overline{x} \\[0.2em]
  \hat{\sigma}^2 \;&=\; \frac {\sum{(x_i-\overline{x})^2}}	n
\end{align} \]
<p>Note that the MLE of \(\sigma^2\) is <strong>biased</strong>. The sample variance, \(S^2\), divides by \((n-1)\) instead of \(n\) — it is unbiased and is usually prefered.</p>
</div>


<h2 class="pageName">9.1.4 &nbsp; Grid search for MLEs</h2>
<p>For some families of two-parameter distributions, it is difficult to find maximum likelihood estimates algebraically.</p>
<ul>
	<li>It may be difficult to differentiate the log-likelihood function</li>
	<li>The equations found by differentiating the log-likelihood may be impossible to solve algebraically.</li>
</ul>
<p>A numerical method must then be used to evaluate the maximum likelihood estimates.</p>
<p>A simple algorithm is a <strong>grid search</strong>; it simply evaluates the log-likelihood over a grid of values of the two parameters, letting us identify  <strong>approximately</strong> where the maximum lies. The grid  can then be refined to focus on a narrower range of possible parameter values.</p>
<div class="example">
	
	<p class="exampleHeading">Beta distribution</p>

<p>The following data set contains proportions between zero and one:</p>
<div class="centred">
<table border="0" cellpadding="5" cellspacing="0" class="centred">
	<tr>
		<td>0.078</td>
		<td>0.713</td>
		<td>0.668</td>
		<td>0.621</td>
		<td>0.069</td>
		<td>0.378</td>
		<td>0.735</td>
		<td>0.255</td>
		<td>0.220</td>
		<td>0.220</td>
	</tr>
	<tr>
		<td>0.136</td>
		<td>0.413</td>
		<td>0.516</td>
		<td>0.183</td>
		<td>0.724</td>
		<td>0.377</td>
		<td>0.409</td>
		<td>0.403</td>
		<td>0.042</td>
		<td>0.692</td>
	</tr>
	<tr>
		<td>0.486</td>
		<td>0.421</td>
		<td>0.358</td>
		<td>0.236</td>
		<td>0.654</td>
		<td>0.717</td>
		<td>0.520</td>
		<td>0.266</td>
		<td>0.520</td>
		<td>0.641</td>
	</tr>
</table>
</div>

<p>A reasonable distribution that could be used to model the data would  be a beta distribution with probability density function</p>
\[
	f(x) \;\;=\;\; \begin{cases} \dfrac {\Gamma(\alpha +\beta) }{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}&amp; \text{if }0 \lt x \le 1 \\
	0 &amp; \text{otherwise}
	\end{cases} \]
<p>The log-likelihood is</p>
\[ \begin{align}
\ell(\alpha, \beta) \;=\; n \log \Gamma(\alpha + \beta) &amp;- n \log \Gamma(\alpha) - n \log \Gamma(\beta) \\
&amp;+ (\alpha - 1) \sum(\log(x_i) + (\beta - 1)\sum \log(1 - x_i)
\end{align} \]
<p>so we will maximise</p>
\[ \ell(\alpha, \beta) \;=\; 30 \log \Gamma(\alpha + \beta) - 30 \log \Gamma(\alpha) - 30 \log \Gamma(\beta) -31.89 (\alpha - 1) - 18.75 (\beta - 1)
\]
<p>with respect to \(\alpha\) and \(\beta\). This cannot be done algebraically.</p>
<p>The following Excel spreadsheet shows  the log-likelihood for values of \(\alpha\) between 1 and 2.4, and values of \(\beta\) between 2 and 3.4.</p>
<p class="eqn"><img src="../../../en/estimateTwoParam/images/excelGrid.png" width="706" height="251"></p>
				<!--  this has a bitmap so the SVG version cannot be used because it does not display in Safari -->
<p>From these log-likelihoods, the maximum is at \(\alpha \approx 1.8\) and \(\beta \approx 2.6\).</p>
<p>Refining the grid to values of \(\alpha\) and \(\beta\) near 1.8 and 2.6, we can find that the MLEs are approximately</p>
\[ \hat{\alpha} = 1.81 \spaced{and} \hat{\beta} = 2.56
\] </div>


<h1 class="sectionName breakBefore">9.2 &nbsp; Confidence intervals from pivots</h1>
<h2 class="pageName">9.2.1 &nbsp; Wald-type confidence intervals</h2>

<p class="heading">Normal approximation</p>

<p>Earlier confidence intervals for parameters  were  based on point estimates that are approximately normally distributed.</p>
<p>Given an estimate of the standard error of the estimator, an approximate confidence interval can be obtained from the quantiles of the normal distribution. For example, an approximate 95% CI for a parameter \(\theta\) is</p>
\[
\hat{\theta} - 1.96\; \se(\hat{\theta}) \;\;\lt\;\; \theta \;\;\lt\;\; \hat{\theta} + 1.96\; \se(\hat{\theta})
\]
<div class="example">
	<p class="exampleHeading">Poisson distribution example</p>
	<p>The following table describes the number of heart attacks in a city in 10 weeks.</p>
	
	<div class="centred">
	<table border="0" cellpadding="5" cellspacing="0" class="centred">
		<tr>
			<th>Week</th>
			<td width="24" align="center">1</td>
			<td width="24" align="center">2</td>
			<td width="24" align="center">3</td>
			<td width="24" align="center">4</td>
			<td width="24" align="center">5</td>
			<td width="24" align="center">6</td>
			<td width="24" align="center">7</td>
			<td width="24" align="center">8</td>
			<td width="24" align="center">9</td>
			<td width="24" align="center">10</td>
		</tr>
		<tr>
			<th>    Count    </th>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border:1px solid #999999; border-right:0px;">6</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">11</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">13</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">10</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">21</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">8</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">16</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">6</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;">9</td>
			<td width="24" align="center" bgcolor="#FFFFFF" style="border:1px solid #999999; border-left:0px;">19</td>
		</tr>
	</table>
	</div>
	
	<p>Assuming a constant rate of heart attacks per week, \(\lambda\), this can be modelled as a random sample from a \(\PoissonDistn(\lambda)\) distribution. The MLE of \(\lambda\) is</p>
\[
\hat{\lambda} \;\;=\;\; \overline{x} \;\;=\;\; 11.9
\]
	<p>Since the variance of the Poisson distribution is \(\lambda\), we can use the Central Limit Theorem to show that \(\hat \lambda\) is approximately normally distributed in large samples and has standard error</p>

\[
\se(\hat{\lambda}) \;\;=\;\; \sqrt{\frac{\hat{\lambda}}{n}} \;\;=\;\; 1.091
\]
	<p>This justifies using a normal approximation to find an approximate 95% confidence interval,</p>
\[
\hat{\lambda} \pm 1.96\; \se(\hat{\lambda}) \;\;=\;\; 11.9 \pm 1.96 \times 1.091 \;\;=\;\; 9.76 \text{ to } 14.04
\]</div>
<p class="heading">Maximum likelihood estimators</p>
<p>This can be used for <strong>all</strong> maximum likelihood estimators. In large samples, a parameter's MLE is approximately normally distributed with   standard error,</p>
\[
\se(\hat {\theta}) \;\;\approx\;\; \sqrt {- \frac 1 {\ell''(\hat {\theta})}}
\]
<p>This leads to 95% confidence intervals of the form</p>
\[
\hat{\theta} \pm 1.96 \sqrt {- \frac 1 {\ell''(\hat {\theta})}}
\]
<p>The constant 1.96 can be replaced by other quantiles from the normal distribution to give other confidence levels. Confidence intervals that are found in this way are called <strong>Wald-type confidence intervals</strong>.</p>


<h2 class="pageName">9.2.2 &nbsp; Pivots</h2>

<p class="heading">Assumption of normality</p>

<p>Wald-type confidence intervals  need  \(\hat{\theta}\) to be approximately <strong>normally</strong> distributed. Although this usually holds  for large enough sample sizes, it is often violated when the sample size is small.</p>
<p>A better type of confidence interval  avoids the need to assume normality. It is based on a random quantity called a <strong>pivot</strong>.</p>
<div class="definition">
	<p class='definitionTitle'>Definition</p>
<p>If a random sample, \(\{X_1, X_2, \dots, X_n\}\) is selected from a distribution with unknown parameter \(\theta\), a <strong>pivot</strong> is a function of the data and \(\theta\) whose distribution is fully known (and therefore does not involve unknown parameters),</p>
\[
g(\theta, X_1, \dots, X_n) \;\;\sim\;\; \mathcal{Distn}
\]</div>
<p>Since the distribution \(\mathcal{Distn}\) has no unknown parameters,  its quantiles are constants that we can evaluate. For a  \((1 - \alpha)\) confidence interval, we need the quantiles giving  probability \(\dfrac{\alpha}2\) in each tail of the distribution.</p>
<p class="eqn"><img class="svgImage" src="../../../en/pivotal/images/quantiles.png" width="412" height="246"></p>
<p>For example, if the pivot had a \(\NormalDistn(0, 1)\) distribution, for a 95% confidence interval, we would find the quantiles  \(D_{2{\frac 1 2}\%} = -1.96\) and \(D_{97{\frac 1 2}\%} = +1.96\).</p>
<p class="heading">Confidence interval</p>
<p>From how we defined these quantiles,</p>
\[
P\left(D_{\alpha / 2} \;\lt\; g(\theta, X_1, \dots, X_n) \;\lt\; D_{1 - \alpha / 2} \right) \;\;=\;\; 1 - \alpha
\]
<p>We can therefore define a \((1 - \alpha)\) confidence interval to be the values of \(\theta\) such that</p>
\[
D_{\alpha / 2} \;\;\lt\;\; g(\theta, x_1, \dots, x_n) \;\;\lt\;\; D_{1 - \alpha / 2}
\]
<p>Since there is a probability \((1 - \alpha)\) of this holding for a random sample from the distribution, the resulting confidence interval has confidence level \((1 - \alpha)\).</p>


<h2 class="pageName">9.2.3 &nbsp; Confidence interval for exponential rate</h2>
<p>We will now find a confidence interval for the rate parameter, \(\lambda\), of a homogeneous Poisson process, based on a random sample of inter-event times.</p>
<div class="example">
	<p class="exampleHeading">Example</p>
	<p>The  values below are times between failures of an item.</p>
	
	<div class="centred">
	<table border="0" cellpadding="5" cellspacing="0" class="centred" style="border:1px solid #999999">
		<tr>
			<td width="30" align="center" bgcolor="#FFFFFF">487</td>
			<td width="30" align="center" bgcolor="#FFFFFF">18</td>
			<td width="30" align="center" bgcolor="#FFFFFF">100</td>
			<td width="30" align="center" bgcolor="#FFFFFF">7</td>
			<td width="30" align="center" bgcolor="#FFFFFF">98</td>
			<td width="30" align="center" bgcolor="#FFFFFF">5</td>
			<td width="30" align="center" bgcolor="#FFFFFF">85</td>
			<td width="30" align="center" bgcolor="#FFFFFF">91</td>
			<td width="30" align="center" bgcolor="#FFFFFF">43</td>
			<td width="30" align="center" bgcolor="#FFFFFF">230</td>
			<td width="30" align="center" bgcolor="#FFFFFF">3</td>
			<td width="30" align="center" bgcolor="#FFFFFF">130</td>
		</tr>
	</table>
	</div>
	
	<p>If failures arise at random over time with a constant rate, \(\lambda\), the values are a random sample from an \(\ExponDistn(\lambda)\)	distribution, and the maximum likelihood estimator of \(\lambda\)  is the inverse of the sample mean,</p>
\[
\hat{\lambda} \;\;=\;\; \frac n {\sum{X_i}}
\]
<p>The sum of \(n\) independent exponential variables has an Erlang distribution,</p>
\[
\sum_{i=1}^n{X_i} \;\;\sim\;\; \ErlangDistn(n, \lambda) \;=\; \GammaDistn(n, \lambda)
\]
<p>This distribution depends on \(\lambda\) but the following quantity <strong>is</strong> a pivot:</p>
\[
\lambda \sum_{i=1}^n{X_i} \;\;\sim\;\; \GammaDistn(n, 1)
\]

<p>Since \(n = 12\), we can use Excel to find the 2½% and 97½% points of the \(\GammaDistn(12, 1)\) distribution — 6.201 and 19.682. A 95% confidence interval is therefore the values of \(\lambda\) for which</p>
\[
6.201 \;\;\lt\;\; \lambda \sum_{i=1}^n{x_i} \;\;\lt\;\; 19.682
\]
\[
6.201 \;\;\lt\;\; 1297\lambda \;\;\lt\;\; 19.682
\]
<p>Rearranging, a 95% CI for the failure rate, \(\lambda\), is</p>
\[
0.0048 \;\;\lt\;\; \lambda \;\;\lt\;\; 0.0152
\]</div>


<h2 class="pageName">9.2.4 &nbsp; Confidence interval for binomial probability</h2>
<p>Consider a single value, \(X\), from a \(\BinomDistn(n, \pi)\) distribution. The MLE of \(\pi\) is</p>
\[
\hat{\pi} \;\;=\;\; \frac X n
\]
<p>The binomial variable \(X\) has mean and variance</p>
\[
E[X] = n\pi \spaced{and} \Var(X) = n\pi(1 - \pi)
\]
<p>Standardising \(X\) (subtracting its mean and dividing by its standard deviation) gives a distribution that is approximately normal as the sample size, \(n\), increases. Therefore</p>
\[
\frac{X - n\pi}{\sqrt{n \pi(1 - \pi)}} \;\;\underset {\text{approx}}{\sim} \;\; \NormalDistn(0, 1)
\]
<p>This can be used as a pivot. An approximate 95% confidence interval is therefore the solution to</p>
\[
-1.96 \;\;\lt\;\; \frac{x - n\pi}{\sqrt{n \pi(1 - \pi)}} \;\;\lt\;\; 1.96
\]
<div class="questionSoln">
	<div class="question">
<p class="questionTitle">Question</p>
<p>A retail clothing outlet has collected the following data from random sampling of invoices for T-shirts over the past month.</p>

<div class="centred">
<table border="0" cellpadding="5" cellspacing="0" class="centred">
	<tr>
		<th>&nbsp;</th>
		<th width="50">Small</th>
		<th width="50">Medium</th>
		<th width="50">Large</th>
		<th width="50">XL</th>
		<th width="50">Total</th>
	</tr>
	<tr>
		<th>North Island</th>
		<td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-left:1px solid #999999;">2</td>
		<td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">15</td>
		<td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999;">24</td>
		<td align="center" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-right:1px solid #999999;">9</td>
		<td align="center">50</td>
	</tr>
	<tr>
		<th>South Island</th>
		<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-left:1px solid #999999;">4</td>
		<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">17</td>
		<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;">23</td>
		<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-right:1px solid #999999;">6</td>
		<td align="center">50</td>
	</tr>
</table>
</div>

<p>Find a 95% confidence interval for the probability that a T-shirt purchased from one of the store's North Island shops is Small.</p>
<p class="questionNote">(Solved in full version)</p>
	</div>
</div>

<p>Using a pivot, the above 95% confidence interval is</p>
\[
0.011 \;\;\lt\;\; \pi \;\;\lt\;\; 0.135
\]
<p>whereas the conventional Wald-type 95% confidence interval would be</p>
\[
-0.014 \;\;\lt\;\; \pi \;\;\lt\;\; 0.094
\]
<p>This includes impossible negative values for \(\pi\), so the confidence interval found from a pivot is better.</p>


</body>
</html>
