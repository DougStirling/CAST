<html>
<head>
<title>5. Continuous Distributions</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 5 &nbsp; Continuous Distributions</h1>
<h2>5.1 &nbsp; Finding probabilities</h2>
<h3>5.1.1 &nbsp; Probabilities by integration</h3>
<p>The probability of a value within any range is the area under the pdf above it. This can be found by integrating the pdf.</p>
<h3>5.1.2 &nbsp; Rectangular distribution</h3>
<p>The simplest continuous distribution is a rectangular one in which each value between two constants is equally likely. Probabilities can be found by geometry or integration.</p>
<h3>5.1.3 &nbsp; Other examples</h3>
<p>Finding probabilities by integration is illustrated with another two examples.</p>
<h3>5.1.4 &nbsp; Cumulative distribution function</h3>
<p>The cumulative distribution function, F(x), is the probability of value less than or equal to x.</p>
<h3>5.1.5 &nbsp; Quantiles</h3>
<p>The p'th quantile of a distribution is the value x such that the probability of a value less than x is p.</p>
<h2>5.2 &nbsp; Mean and variance</h2>
<h3>5.2.1 &nbsp; Expected values</h3>
<p>The expected value of any function of a continuous random variable is defined by an integral.</p>
<h3>5.2.2 &nbsp; Mean and variance</h3>
<p>The expected value of X is called its mean. The variable's variance is the expected value of the squared difference between X and its mean.</p>
<h3>5.2.3 &nbsp; Example</h3>
<p>The mean and variance of a rectangular distribution are derived.</p>
<h2>5.3 &nbsp; Random samples</h2>
<h3>5.3.1 &nbsp; Independence and random samples</h3>
<p>Two continuous random variables are independent if knowing the value of one provides no information about the value of the other. A collection of n independent random variables with the same distribution is a random sample.</p>
<h3>5.3.2 &nbsp; Distribution of sample sum and mean</h3>
<p>Formulae are given for the mean and variance of the sum and mean of a random sample. The sum and mean are both approximately normal in large samples.</p>
<h2>5.4 &nbsp; Estimating parameters</h2>
<h3>5.4.1 &nbsp; Bias and standard error</h3>
<p>Bias, standard error, mean squared error and consistency are defined in the same way for estimators of parameters in discrete and continuous distributions.</p>
<h3>5.4.2 &nbsp; Method of moments</h3>
<p>If a distribution has a single unknown parameter, its method of moments estimator is the value that makes the distribution's mean equal to the mean of a random sample. The method of moments estimator for the maximum of a rectangular distribution is found.</p>
<h3>5.4.3 &nbsp; Maximum likelihood</h3>
<p>The likelihood function of a continuous distribution's parameter is the product of the probability density functions of a random sample. The maximum likelihood estimate is usually found by setting the derivative of the log-likelihood to zero.</p>
<h3>5.4.4 &nbsp; Properties of maximum likelihood estimators</h3>
<p>Maximum likelihood estimators are asymptotically unbiased with normal distributions. A formula is given for the approximate standard error.</p>
<h3>5.4.5 &nbsp; Confidence intervals</h3>
<p>An approximate 95% confidence interval is the maximum likelihood estimate Â± 1.96 standard errors. Replacing "1.96" with other constants gives other confidence levels.</p>
<h3>5.4.6 &nbsp; Example: normal distribution mean</h3>
<p>The maximum likelihood estimate of a normal distribution's mean is found. The asymptotic formula for the standard error of the estimator gives the exact standard error.</p>
<h3>5.4.7 &nbsp; Example: Rectangular maximum</h3>
<p>The maximum likelihood estimate of the maximum of a rectangular distribution is at a discontinuity of the likelihood function and cannot be found by differentiating the log-likelihood.</p>
</body>
</html>
