<html>
<head>
  <title>11. Comparing Groups</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 11 &nbsp; Comparing Groups</h1>
<h1 class="sectionName">11.1 &nbsp; Models for two groups</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Interest in underlying population</li>
<li>Model for two groups</li>
<li>Parameters of the normal model</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='4'>
<li>Parameter estimates</li>
<li>Difference between means</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">11.1.1 &nbsp; Interest in underlying population</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Data from two groups</p>
<p>When data are  collected from two groups, we are usually interested in  differences
between the groups
<strong>in general</strong>. The <strong>specific</strong> individuals  are of
less interest. Questions are therefore about the characteristics of the populations
or processes that we assume <strong>underlie</strong> the
data.</p>
<p class="heading">Example</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/failedCo.gif" width="523" height="299"> </p>
<p>The diagram below illustrates a possible model for the data above.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_pdfs.gif" width="321" height="251"></p>
<p>Without an understanding of the distribution of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
it is impossible to properly interpret what the sample difference, 9.5 pieces,
tells you about the difference between the underlying population means.</p>




<h1 class="sectionName breakBefore">11.2 &nbsp; Distn of sums and differences</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Means and sums of samples</li>
<li>Sum and difference</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='3'>
<li>Sum and difference (cont)</li>
<li>Probabilities for sums and differences</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">11.2.1 &nbsp; Means and sums of samples</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Sample mean and sum</p>
<p>The mean of a random
sample, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">,
has a distribution that is approximately normal if the sample size, <em>n</em>, is
large and alway has a
mean and standard deviation that depend on the population mean, µ, and standard deviation,
σ,</p>
<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span></p>
<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>
<p>Occasionally the sum of values in a random sample values is more useful than the
mean,</p>
<p class=eqn><img class="gif" width="220" height="18"></p>
<p>Its distribution is
a scaled version of the distribution of the mean &mdash; the same shape but different
mean and standard deviation.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumMean.gif" width="75" height="14"></p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumSD.gif" width="89" height="21"></p>
<p class="heading">Mean vs Sum</p>
<p>As the sample size increases,</p>
<ul>
<li>the standard deviation of the mean decreases, but</li>
<li>the standard deviation of the sum <strong>increases</strong>.</li>
</ul>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sdInequality.gif" width="130" height="16"></p>




<h2 class="pageName">11.2.2 &nbsp; Sum and difference</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Sum and difference of two  variables</p>
<p>Applying the result about the sum of a random sample to a sample of size <em>n</em> = 2, <em>X</em><sub>1</sub>
and <em>X</em><sub>2</sub>,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn.gif" width="119" height="50"></p>
<p>If we generalise by allowing <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> to
have different means, µ<sub>1</sub> and µ<sub>2</sub>, but the same σ,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn2.gif" width="146" height="47"> </p>
<p>A similar result holds for the difference between <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>:</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/diff2Distn2.gif" width="146" height="47"></p>
<p>If <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> are independent and have
normal distributions, their sum and difference are also normally distributed.</p>




<h2 class="pageName">11.2.3 &nbsp; Sum and difference (cont)</h2><!DOCTYPE HTML>


<p class="heading notPrinted">General result</p>
<p>The results generalise further to independent variables that may have different
means <strong>and</strong> standard deviations.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumDiffSD.gif" width="250" height="161"></p>
<p>The formulae for the standard deviations are more easily remembered in terms of
the <strong>variances</strong> of
the  quantities. For example,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Variance.gif" width="151" height="25"></p>




<h2 class="pageName">11.2.4 &nbsp; Probabilities for sums and differences</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Finding probabilities</p>
<p>To find the probability that a sum or difference satisfies an inequality, the
inequality should be translated into ones about a z-score, using the mean and standard
deviation of the quantity,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/standardiseEqn.gif" width="95" height="31"></p>
<p>The standard normal distribution can then be used to find the  probabilities. The
examples below illustrate the method. </p>
<p class="heading">Example (total of several variables)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/totalExample.gif" width="496" height="516"> </p>
<p class="heading">Example (sum of two variables with different sd)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumExample.gif" width="496" height="516"> </p>




<h1 class="sectionName breakBefore">11.3 &nbsp; Comparing means in two groups</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Distn of difference between means</li>
<li>SE of difference between means</li>
<li>CI for difference between means</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='4'>
<li>Testing a hypothesis</li>
<li>One-tailed tests for differences</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">11.3.1 &nbsp; Distn of difference between means</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Difference between  means</p>
<p>The difference between <strong>any</strong> two independent
quantities <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> has a distribution
with</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/meanSDDiff2.gif" width="113" height="51"> </p>
<p>Applying this to  the difference between the
means of two random samples,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffMeanSD.gif" width="340" height="163"> </p>
<dl>
<dt>If the distributions are normal in each group,&nbsp;...</dt>
<dd>... the  sample means are normal, so their difference
also has a normal distribution.</dd>
<dt>Otherwise,&nbsp;...</dt>
<dd>... the two sample means are approximately normal if the sample sizes are
large,  so their difference is also close to normal.</dd>
</dl>

<div class="centred"><div class="boxed">
<p>Irrespective of the distributions within the
two groups, <br>
<img class="gif" src="../../../en/twoGroupInf/images/normalDistn.gif" width="331" height="44"></p>
</div></div>





<h2 class="pageName">11.3.2 &nbsp; SE of difference between means</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Estimation error</p>
<p>The difference between the sample means, <span class="eqn"><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span></span>,
is a point estimate of the difference between the means of the underlying populations, <span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span>.
In order to properly interpret it, we must understand the distribution of
the estimation error.</p>
<p class=eqn><img class="gif" width="372" height="42"> </p>
<p>Replacing σ<sub>1</sub><sup>2</sup> and σ<sub>2</sub><sup>2</sup> by <em>s</em><sub>1</sub><sup>2</sup> and
<em>s</em><sub>2</sub><sup>2</sup> gives an approximate error distribution,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/estErrorDistn2.gif" width="172" height="42"> </p>
<p>The standard deviation of these errors is the <strong>standard error</strong> of
the estimator.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/failedCoError.gif" width="514" height="417"></p>
<p>where the degrees of freedom for the t-value are </p>
<p class=eqn><span class="black">&nu; &nbsp; = &nbsp; min (<em>n</em><sub>1</sub>&minus;1, &nbsp;<em>n</em><sub>2</sub>&minus;1)</span> </p>
<p class="gray">(A more complex formula is available that gives a higher
value for &nu;. It is slightly better but the difference is usually
negligible.)</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/failedCoCI.gif" width="514" height="401"> </p>
<p>As with all other hypothesis tests, a p-value near zero gives evidence that
the null hypothesis does not hold &mdash; evidence of a difference between the group
means. </p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/colaTest.gif" width="514" height="468"> </p>
<p class="heading">Test statistic, p-value and conclusion</p>
<p>Consider a test for the hypotheses,</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&mu;<strong><sub>2</sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;&gt;&nbsp; </strong>&mu;<strong><sub>2</sub></strong></span> </p>
<p>The alternative hypothesis is only supported by very small values of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
This also corresponds to small values of the test statistic <span class="em black">t</span> ,
so the p-value is the <strong>lower</strong> tail probability of the t distribution. </p>
<p class=eqn><img class="gif" width="453" height="265"> </p>
<p>A small  p-value is interpreted as giving evidence that H<sub>0</sub> is false, in a
similar way to all other kinds of hypothesis test.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/moderatorTest.gif" width="514" height="468"> </p>
<p>Since our  model  involves only two parameters, π<sub>1</sub> and π<sub>2</sub>,
 the two groups are the same only if π<sub>2</sub> - π<sub>1</sub> = 0. The value
of π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub> is usually unknown
but  can be estimated by <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>.
However   <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> is
a random quantity so its variability must
be taken into account when interpreting its value.</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/ethics.gif" width="514" height="336"></p>
<p>Applying the  general results about the difference between two independent
random quantities:</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/meanSD.gif" width="554" height="120"> </p>
<p>Since the individual proportions are approximately normal (in large samples),
their difference is also approximately normal:</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffPDistn.gif" width="417" height="70"></p>




<h2 class="pageName">11.4.3 &nbsp; CI for difference in proportions</h2><!DOCTYPE HTML>


<p class="heading">Standard error of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub></p>
<p>The  standard deviation of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> is
also its standard error when it<sub></sub> is
used to estimate π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub>,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/sdDiffEqn.gif" width="228" height="45"></p>
<p>In practice, π<sub>1</sub> and π<sub>2</sub> must be replaced by their sample
equivalents to estimate the standard error. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/sdDiffEqn2.gif" width="228" height="45"></p>
<p class="heading">Confidence interval for difference</p>
<p>Most 95% confidence intervals are of the form</p>
<p class="eqn"><em>estimate</em>   ±   1.96 &times; se(<em>estimate</em>)</p>
<p>perhaps with a refinement of using a slightly higher value than 1.96 (e.g.
a t-value) if the standard error is estimated. Applying this to our estimate
of π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub>and using 2 instead of 1.96 gives the
approximate 95% confidence interval</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffCIEqn2.gif" width="278" height="48"></p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/ethicsCI.gif" width="514" height="373"> </p>
<p>The p-value is interpreted in the same way as for all previous tests. A p-value
close to zero is unlikely when <b>H<sub>0</sub></b> is true, but is more likely
when <b>H<sub>A</sub></b> holds. Small p-values therefore provide evidence of
a difference between the population probabilities. </p>
<p class="heading">One-tailed test</p>
<p>In a 1-tailed test, the alternative hypothesis is</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&gt;&nbsp; 0</strong></span> &nbsp;&nbsp; <span class="red"><strong>or</strong></span> &nbsp;&nbsp; <span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&lt;&nbsp; 0</strong></span></p>
<p>The test statistic is identical to that for a 2-tailed test and the p-value
is obtained in a similar way, but it is found from only a <strong>single</strong> tail
of the standard normal distribution. </p>
<p class="heading"><span class="black">Alternative test statistic</span></p>
<p>Since π<sub>1</sub> and
π<sub>2</sub> are equal if <b>H<sub>0</sub></b> is true, the overall proportion
of successes, <em>p</em>, can be used in the formula for the standard error
of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffSDEstEqn.gif" width="476" height="95"> </p>
<p>This refinement makes little difference in practice,
so the examples below use the 'simpler' formula that we gave earlier.</p>
<p class="heading">Two-tailed example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/bangladeshTest.gif" width="541" height="476"></p>
<p>This is compared to a t distribution with <em>n</em>&nbsp;-&nbsp;1
degrees of freedom to find the p-value.</p>
<p class="heading">Example</p>
<p>The diagram below illustrates a 2-tailed test for equal means, based on <em>n</em> = 15
paired observations.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_example.gif" width="475" height="384"> </p>
<p class="heading">Estimating σ<sup>2</sup></p>
<p>The sample standard deviation in any single group, <em>s<sub>i</sub></em>,
is a valid estimate of σ, but we need to combine these <em>g</em> separate estimates
in some way.</p>
<p>It is easier to describe estimation of σ<sup>2</sup> rather than σ. If the sample
sizes are the same in all groups, a <strong>pooled</strong> estimate
of σ<sup>2</sup> is the average of the group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst3.gif" width="105" height="38"> </p>
<p>If the sample sizes are <strong>not</strong> equal in all groups, this is
generalised by adding the numerators and denominators of the formulae for the <em>g</em> separate
group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst.gif" width="444" height="67"> </p>
<p>More mathematically,  <em>y<sub>ij</sub></em> denotes the <span class="em black">j</span> 'th
of the <span class="em black">n<sub>i</sub></span>  values in group <span class="em black">i</span> ,
for <span class="em black">i</span> &nbsp;=&nbsp;1 to <span class="em black">g</span> .
The pooled estimate of σ<sup>2</sup> can then be written as</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst2.gif" width="210" height="80"> </p>
<p>The pooled variance is influenced most by the sample variances in the groups
with biggest sample sizes.</p>




<h2 class="pageName">11.6.3 &nbsp; Revisiting two groups</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Revisiting the difference between <span class="red">two</span> group
means</p>
<p>In an earlier section, we described  confidence intervals and tests about the
difference between two group means, µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub>.
They can be improved if we can assume that</p>
<p class=eqn><span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span> </p>
<p>Inference is still based on <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
but the equation for its standard deviation can be simplified</p>
<p class=eqn><img class="gif" width="261" height="45"> </p>
<p class="heading">Confidence interval</p>
<p>A 95% confidence interval for µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub> has
the same general form as before,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/generalCI.gif" width="160" height="22"> </p>
<p>but the standard deviation and the degrees of freedom
for the t-value, &nu;, are different.</p>
<div class="centred"><table border="0" class="centred" cellpadding="10" cellspacing="0">
<tr>
<td>&nbsp;</td>
<td align="center"><img class="gif" src="../../../en/multiGroup/images/sdEst.gif" width="42" height="22"></td>
<td align="center"><span class="black">degrees of freedom</span></td>
</tr>
<tr>
<td>Allowing <span class="black">&sigma;<sub>1</sub> &ne; &sigma;<sub>2</sub></span></td>
<td align="center" bgcolor="#FFFFFF"><img class="gif" src="../../../en/multiGroup/images/sdEst1.gif" width="85" height="45"></td>
<td align="center" bgcolor="#FFFFFF" class="black">min( <em>n</em><sub>1</sub> -
1, <em>n</em><sub>2</sub> - 1)</td>
</tr>
<tr>
<td>Assuming <span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span></td>
<td align="center" bgcolor="#FFFFFF"><img class="gif" src="../../../en/multiGroup/images/diffMeanSDEst.gif" width="109" height="39"></td>
<td align="center" bgcolor="#FFFFFF" class="black"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> - 2</td>
</tr>
</table></div>
<p>If it can be assumed that &sigma;<sub>1</sub> = &sigma;<sub>2</sub>,  the
confidence interval is usually narrower.</p>
<p class="heading">Example</p>
<p>The diagram below shows 95% confidence intervals obtained by the two methods.</p>
<p align="center"><img src="../../../en/multiGroup/images/hypnosisPooled.gif" width="550" height="340"> </p>
<p>The p-value for this test is found from the tail area of the t distribution
with (<span class="black"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> -
2</span>) degrees of freedom.</p>




<h2 class="pageName">11.6.4 &nbsp; Variation between and within groups</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Comparing several groups</p>
<p>A new approach is needed to compare the means of three or more groups &mdash; the
 methods for two groups cannot be extended. We again assume a normal model with equal
standard deviations,</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p>Testing whether there are differences between the groups involves the hypotheses,</p>

<p style="margin-left:200px; color:#000000"><font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>0</sub></strong></font> : &nbsp; µ<sub><em>i</em></sub> &nbsp;=&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
all i and j</em><br>
<font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>A</sub></strong></font>: &nbsp; µ<sub><em>i</em></sub> &nbsp;≠&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
at least some i, j</em></p>

<p class="heading">Variation between and within groups</p>
<p>Testing whether the model means, {µ<sub><em>i</em></sub>}, are equal is done
by assessing the <strong>variation between the group means</strong> in the data.
However, because of randomness in sample data, the means are unlikely be the same,
even if <strong>H<sub>0</sub></strong> is true.</p>
<p>In the example on the left below, the group means vary so much that the 
{µ<sub><em>i</em></sub>} are almost certainly not equal. However the group means
on the right are relatively similar and their differences may simply be randomness.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_variationBetween.gif" width="518" height="295"></td>
<td class="green">The <strong>total sum of squares</strong> reflects
the total variability of the response.</td>
</tr>
</table></div>
<p>The overall variance of all values (ignoring 
groups) is the total sum of squares divided by (<em>n</em>&nbsp;-&nbsp;1).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_totalSsq.gif" width="413" height="277"></td>
<td class="red">The <strong> sum of squares between groups</strong> measures
the variability of the group means.</td>
</tr>
</table></div>
<p>Variation between groups is summarised by the differences between the group
means and the overall mean. Note that the summation  is <span class="black bold">over
all observations in the data set</span>.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_betweenSsq.gif" width="413" height="277"></td>
<td class="blue">The <strong> sum of squares within groups</strong> quantifies
the spread of values within each group.</td>
</tr>
</table></div>
<p>This is also called the <strong>residual</strong> sum of squares since it
describes variability that is unexplained by differences between the groups.
Note that the pooled estimate of the common variance, σ<sup>2</sup>, is the sum
of squares within groups divided by (<em>n</em>&nbsp;-&nbsp;<em>g</em>).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_withinSsq.gif" width="413" height="277"> </p>




<h2 class="pageName">11.6.6 &nbsp; Coefficient of determination</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Sums of squares</p>
<div class="centred"><table border="0" class="centred" cellpadding="4" cellspacing="0">
<tr>
<th align="left">Sum of squares</th>
<th>Interpretation</th>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/totalSsq2.gif" width="141" height="25">
<td>Overall variability of <em>Y</em>, taking no account of the groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residSsq2.gif" width="154" height="25">
<td>Variability
that <strong>cannot be explained</strong> by the model.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/regnSsq2.gif" width="149" height="24">
<td>Variability that is <strong>explained</strong> by
the model.</td>
</tr>
</table></div>
<p class="heading">Coefficient of determination</p>
<p>The <strong>proportion</strong> of
the total sum of squares that is explained by the model is called
the <strong>coefficient of determination</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/rSquaredDefn.gif" width="97" height="38"> </p>
<ul>
<li>0  ≤  R<sup>2</sup>  ≤  1</li>
<li>The proportion of <strong>unexplained</strong> variation is (1&nbsp;-&nbsp;R<sup>2</sup>)</li>
<li>When R<sup>2</sup> &asymp; 0, the group means are similar
to each other.</li>
<li>If R<sup>2</sup> &asymp; 1, the individual values must be  close
to their group means.</li>
</ul>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/multiGroup/images/volatilityR2.gif" width="555" height="344"></td>
<td class="green">The <strong>mean total sum of squares</strong> is the
sample variance of the response (ignoring groups).</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residMss.gif" width="143" height="43"></td>
<td class="blue">The <strong>mean within-group sum of squares</strong> is
the pooled estimate of the variance within groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/regnMss.gif" width="162" height="43"></td>
<td class="red">The <strong>mean between-group sum of squares</strong> is
harder to directly interpret.</td>
</tr>
</table></div>
<p>The numerators in these ratios add up:</p>
<p class="eqn"><span class="green">SS<sub>Total</sub></span>  =  <span class="red">SS<sub>Between</sub></span>  +  <span class="blue">SS<sub>Within</sub></span></p>
<p>and the same relationship holds for their denominators (degrees of freedom):</p>
<p class="eqn"><span class="green">df<sub>Total</sub></span>  =  <span class="red">df<sub>Between</sub></span>  +  <span class="blue">df<sub>Within</sub></span></p>
<p class="heading">F ratio and p-value</p>
<p>The test statistic is  an <strong>F-ratio</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/fRatio.gif" width="114" height="37"> </p>
<p>This test statistic compares between- and within-group variation. The further
apart the group means, the larger <span class="red">SS<sub>Between</sub></span> and the larger the F-ratio.<br>
</p>

<div class="centred"><div class="boxed">
<p>Large values of F suggest that H<sub>0</sub> does
not hold &mdash; that the group means are not the same.</p>
</div></div>

<p>The p-value for the test is the probability of such a high F ratio if <strong>H<sub>0</sub></strong> is
true (all group means are the same). It is based on a standard distribution called
an <strong>F distribution</strong> and is interpreted in the same way as other
p-values. </p>

<div class="centred"><div class="boxed">
<p>The closer the p-value to zero, the stronger the evidence
that H<sub>0</sub> does not hold.</p>
</div></div>

<p class="heading">Analysis of variance table</p>
<p>An <strong>analysis
of variance table</strong> (<strong>anova table</strong>) describes some of the calculations
above:</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/anovaTable2.gif" width="550" height="177"></p>




<h2 class="pageName">11.6.8 &nbsp; Examples</h2><!DOCTYPE HTML>


<p align="center"><img src="../../../en/multiGroup/images/volatilityF.gif" width="550" height="434" class="summaryPict"></p>
<p align="center"><img src="../../../en/multiGroup/images/laminectomyF.gif" width="550" height="434" class="summaryPict"></p>




</html>
