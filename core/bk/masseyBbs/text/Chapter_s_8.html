<!DOCTYPE HTML>
<html>
<head>
  <title>8. Comparisons</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 8 &nbsp; Comparisons</h1>
<h1 class="sectionName">8.1 &nbsp; Models for two groups</h1>
<h2 class="pageName">8.1.1 &nbsp; Interest in underlying population</h2>

<p class="heading notPrinted">Data from two groups</p>
<p>When data are  collected from two groups, we are usually interested in  differences
between the groups
<strong>in general</strong>. The <strong>specific</strong> individuals  are of
less interest. Questions are therefore about the characteristics of the populations
or processes that we assume <strong>underlie</strong> the
data.</p>
<p class="heading">Example</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/failedCo.gif" width="523" height="299" class="summaryPict"></p>
<p>The  questions do not refer to the 16 specific subjects &mdash; they ask about
whether anticipation of hypnosis affects the ventilation rate <strong>in general</strong>.
We
would like to use the answers to predict what will happen to other people.</p>




<h2 class="pageName">8.1.2 &nbsp; Model for two groups</h2>

<p class="heading notPrinted">Data and model</p>
<p>Data from two groups can be displayed with two histograms:</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_histos.gif" width="325" height="281"> </p>
<p>The diagram below illustrates a possible model for the data above.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_pdfs.gif" width="321" height="251"></p>




<h2 class="pageName">8.1.3 &nbsp; Parameters of the normal model</h2>

<p class="heading notPrinted">Parameters</p>
<p>A normal model for two groups has four unknown parameters (the mean and standard
deviation for each normal distribution). These parameters give considerable flexibility
and allow the model to be used for a variety of different data sets.</p>
<p>(The number of parameters can be reduced to three if it is assumed that the two
standard deviations are the same, but we will not consider this type of model here.)</p>




<h2 class="pageName">8.1.4 &nbsp; Parameter estimates</h2>

<p class="heading notPrinted">Parameter estimates</p>
<p>A normal model for 2-group data involves 4 unknown parameters, µ<sub>1</sub>,
µ<sub>2</sub>, σ<sub>1</sub> and σ<sub>2</sub>. The means and standard deviations
in the two samples provide objective estimates of the four parameters.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_bestFit.gif" width="330" height="289"></p>




<h2 class="pageName">8.1.5 &nbsp; Difference between means</h2>

<p class="heading notPrinted">Comparing the populations</p>
<p>Although  standard deviations in the two populations may also differ, we are usually
most interested in the difference between the population means. Differences between
the means can be expressed in terms of the model parameters with the following questions.</p>
<ul>
<li>Is µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> = 0?</li>
<li>What is the value of µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub>?</li>
</ul>
<p class="heading">Randomness of sample difference</p>
<p>These questions  are about µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> and the
best estimate of it is <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
However, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span> cannot
give  definitive answers <sub></sub> since
it is  random   &mdash; it varies from sample to sample.</p>
<p class="eqn"><img class="gif" src="../../../en/twoGroupModel/images/piecework2.gif" width="523" height="296"></p>
<p>Without an understanding of the distribution of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
it is impossible to properly interpret what the sample difference, 9.5 pieces,
tells you about the difference between the underlying population means.</p>




<h2 class="pageName">8.1.6 &nbsp; Distribution of a difference</h2>

<p class="heading notPrinted"><span class="heading">Difference between two variables</span></p>
<p>If <em>X</em><sub>1</sub> has mean µ<sub>1</sub> and standard deviation σ<sub>1</sub>,
and <em>X</em><sub>2</sub> has mean µ<sub>2</sub> and standard deviation σ<sub>2</sub>,
then the mean and standard deviation of the difference, <em>X</em><sub>1</sub>&nbsp;-&nbsp;<em>X</em><sub>2</sub>,
are</p>

<p class=eqn><img src="../images/diffMeanSd.gif" width="157" height="51"></p>

<p>This is not a typo. The variance of <em>X</em><sub>1</sub>&nbsp;-&nbsp;<em>X</em><sub>2</sub> is
the <strong>sum</strong> of the variances of <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>.</p>
<p class="heading">Requirement of independence</p>
The above formula for the standard deviation is only true if <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> are
independent of one another.
<div class="centred"><div class="boxed">
<p>It is important to know that the two variables are independent before using
the above formula for the standard deviation of their difference.	
</div></div>


<h1 class="sectionName breakBefore">8.2 &nbsp; Are the means equal?</h1>
<h2 class="pageName">8.2.1 &nbsp; Distn of difference between means</h2>

<p class="heading notPrinted">Difference between  means</p>
<p>The difference between <strong>any</strong> two independent
quantities <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> has a distribution
with</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/meanSDDiff2.gif" width="113" height="51"> </p>
<p>Applying this to  the difference between the
means of two random samples,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffMeanSD.gif" width="340" height="163"> </p>
<dl>
<dt>If the distributions are normal in each group,&nbsp;...</dt>
<dd>... the  sample means are normal, so their difference
also has a normal distribution.</dd>
<dt>Otherwise,&nbsp;...</dt>
<dd>... the two sample means are approximately normal if the sample sizes are
large,  so their difference is also close to normal.</dd>
</dl>

<div class="centred"><div class="boxed">
<p>Irrespective of the distributions within the
two groups, <br>
<img class="gif" src="../../../en/twoGroupInf/images/normalDistn.gif" width="331" height="44"></p>
</div></div>





<h2 class="pageName">8.2.2 &nbsp; SE of difference between means</h2>

<p class="heading notPrinted">Estimation error</p>
<p>The difference between the sample means, <span class="eqn"><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span></span>,
is a point estimate of the difference between the means of the underlying populations, <span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span>.
In order to properly interpret it, we must understand the distribution of
the estimation error.</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></td>
			<td valign="middle">&nbsp; is a point estimate of&nbsp;<span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span></td>
		</tr>
	</table>
</div>
<p>Replacing σ<sub>1</sub><sup>2</sup> and σ<sub>2</sub><sup>2</sup> by <em>s</em><sub>1</sub><sup>2</sup> and
<em>s</em><sub>2</sub><sup>2</sup> gives an approximate error distribution,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/estErrorDistn2.gif" width="172" height="42"> </p>
<p>The standard deviation of these errors is the <strong>standard error</strong> of
the estimator.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/failedCoError.gif" width="514" height="417" class="summaryPict"></p>
<p>Our best estimate is that healthy companies have a mean assets-to-liabilities
ratio that is 0.902 higher than that of failed companies. From the error distribution,
the error in this estimate is unlikely to be more than about 0.3.</p>




<h2 class="pageName">8.2.3 &nbsp; Are the means equal?</h2>

<p class="heading">Testing for a difference between  two  means</p>
<p>The difference between two groups that is of most practical
importance is a difference between their <strong>means</strong>. </p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;&ne;&nbsp; 0</strong></span></p>
<p>The summary statistic that throws most light on these hypotheses is the difference
between the sample means, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
Testing therefore involves assessment of whether this difference is unusually
far from zero. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/pValue.gif" width="454" height="266"> </p>
<p>As with all other hypothesis tests, a p-value near zero gives evidence that
the null hypothesis does not hold &mdash; evidence of a difference between the group
means. </p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/colaTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">General properties of p-values</p>
<p>A statistical hypothesis test cannot provide
a definitive answer about whether two groups have different means. The randomness
of sample data means that p-values are also random quantities.</p>
<p>It is possible to get a small p-value (supporting H<sub>A</sub>) when H<sub>0</sub> is
true, and it is possible to get a large p-value (consistent with H<sub>0</sub>)
when H<sub>A</sub> is true.</p>
<div class="centred"><div class="boxed"><p>There is some chance of being misled by an 'unlucky sample.</p></div></div>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>All p-values between 0 and 1 are equally likely. For example, there is a
5% probability of getting a p-value less than 0.05.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The p-value is more likely to be near zero, though there is still some chance
of a larger p-value.</dd>
</dl>
<p class="heading">Effect of increasing the sample size</p>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>The p-values remain equally likely between 0 and 1.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The distribution of p-values becomes more concentrated near zero, so you
are more likely to conclude that the population means are really different.</dd>
</dl>




<h2 class="pageName">8.2.4 &nbsp; One-tailed tests for differences</h2>

<p class="heading notPrinted">One- and two-tailed tests for differences</p>
<p>In a <strong>two-tailed test</strong>, the alternative hypothesis is that the two population
means are different. A <strong>one-tailed test</strong> arises when we want to test whether one
mean is <strong>higher</strong> than the other (or <strong>lower</strong> than the other).</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/hypotheses.gif" width="441" height="84"> </p>
<p class="heading">Test statistic, p-value and conclusion</p>
<p>Consider a test for the hypotheses,</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&mu;<strong><sub>2</sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;&gt;&nbsp; </strong>&mu;<strong><sub>2</sub></strong></span> </p>
<p>The alternative hypothesis is only supported by very small values of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
This also corresponds to small values of the test statistic <span class="em black">t</span> ,
so the p-value is the <strong>lower</strong> tail probability of the t distribution. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/oneTailedP.gif" width="453" height="265"> </p>
<p>A small  p-value is interpreted as giving evidence that H<sub>0</sub> is false, in a
similar way to all other kinds of hypothesis test.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/moderatorTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">Properties of p-values</p>
<p>We again stress that a statistical hypothesis test cannot provide a definitive
answer. The randomness of sample data means that p-values are also random quantities,
so there is some chance of us being misled by an 'unlucky' sample:</p>
<ul>
<li>If µ<sub>1</sub> = µ<sub>2</sub>, it is still possible to get a small p-value
(e.g. a 5% probability of getting a p-value less than 0.05).</li>
<li>If µ<sub>1</sub> and µ<sub>2</sub> are different, large p-values are still
possible (though less likely than small p-values).</li>
</ul>




<h1 class="sectionName breakBefore">8.3 &nbsp; Paired data</h1>
<h2 class="pageName">8.3.1 &nbsp; Paired data</h2>

<p class="heading notPrinted">Paired data</p>
<p>When two types measurements, <em>X</em> and <em>Y</em>, are made from each
individual (or other unit), the data are called <strong>bivariate</strong>. Sometimes
the two measurements are of  closely related quantities and
may even describe the same quantity at different times. </p>
<div class="centred"><div class="boxed"><p>When the sum or difference of <em>X</em> and <em>Y</em> is a meaningful quantity,
the data are called <strong>paired data</strong>.</p></div></div>
<p class="heading">Hypotheses of interest</p>
<p>For paired data, We often want to test whether the means of the two variables
are equal,</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>Sometimes a one-tailed test is required, such as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &gt; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p class="heading">Examples</p>
<dl>
<dt>Pre-test, post-test data</dt>
<dd>This arises when a measurement is made from each individual, then a second
measurement of the same type is made after some kind of intervention (e.g. training
or medication). Has the intervention &quot;improved&quot; the measurement?</dd>
<dt>Twin studies</dt>
<dd>Some experiments or other studies are conducted with
identical twins, either human or animal. The  members
of each pair experience different environments &mdash; either two different experimental
treatments or two other differences. Are there differences between the two treatments?</dd>
<dt>Other types of pairing</dt>
<dd>For example, damaged cars may each be taken to two garages for estimates
of the cost of repair. The two estimates for each car are paired data. Does one
garage overcharge?</dd>
</dl>





<h2 class="pageName">8.3.2 &nbsp; Analysis of differences</h2>

<p class="heading notPrinted">Differences</p>
<p>Information about the difference between the means of <em>X</em> and <em>Y</em> is
contained in the values <em>D</em> = (<em>Y</em> - <em>X</em>)  for
each individual. The hypotheses</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>can then be expressed as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> = 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> &ne; 0</strong></span></p>
<p>This reduces the paired data set to a <strong>univariate</strong> data set of
differences,
<em>D</em>, and reduces questions about (µ<sub><em>Y</em></sub> - µ<sub><em>X</em></sub>)
to questions about the mean of <em>D</em>.</p>
<p class="heading">Analysis of paired data</p>
<p>By taking differences between <em>Y</em> and <em>X</em>, much of the variability
between the individuals is eliminated, making it easier to see whether their means
are different. The example below shows paired data on the left with blue lines
joining the x- and y-values in each pair. The differences on the right make it clearer
that the y-values are usually higher than the corresponding x-values.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_pairing.gif" width="404" height="322"></p>





<h2 class="pageName">8.3.3 &nbsp; Paired t-test</h2>

<p class="heading notPrinted">Approach (paired t-test)</p>
<p>Testing whether two paired measurements, <em>X</em> and <em>Y</em>,
have equal means is done in terms of the differences</p>
<p class="eqn"><em><strong>D</strong></em><strong> = <em>Y</em> - <em>X</em></strong></p>
<p>The test is then expressed as</p>
<p class="eqn"><strong>H<sub>0</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> =
0</p>
<p class="eqn"><strong>H<sub>A</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> ≠
0</p>
<p>or a one-tailed variant. The hypotheses are therefore assessed with a standard
univariate t-test using test statistic</p>
<p class="eqn"><img class="gif" src="../../../en/testPaired/images/testStat.gif" width="86" height="58"></p>
<p>This is compared to a t distribution with <em>n</em>&nbsp;-&nbsp;1
degrees of freedom to find the p-value.</p>
<p class="heading">Example</p>
<p>The diagram below illustrates a 2-tailed test for equal means, based on <em>n</em> = 15
paired observations.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_example.gif" width="475" height="384"></p>
<p>From the p-value, we conclude that there is very strong evidence that the
means for <em>Y</em> and <em>X</em> are different.</p>





<h2 class="pageName">8.3.4 &nbsp; Pairing and experimental design</h2>

<p class="heading notPrinted">Choice between paired data or two independent samples</p>
<p>It is sometimes possible to answer questions about the difference
between two means by collecting two alternative types of data.</p>
<dl>
<dt>Two independent samples</dt>
<dd>Measurements are made from two samples of individuals from the groups whose
means are to be compared. A 2-sample t-test can be used to compare the means.</dd>
<dt>One paired sample</dt>
<dd>The 'individuals' can be re-defined as pairs of related values from the two
groups and a single sample of these pairs can be collected. A paired t-test
can be performed on the differences to compare the means.</dd>
</dl>
<div class="centred"><div class="boxed"><p>If the individuals in the 2 groups can be paired so that the pairs
are relatively similar, a paired design gives more accurate results.</p></div></div>
<p class="heading">Matched pairs in experiments</p>
<p>In experiments to compare two treatments, it may be possible to group the
experimental units into pairs that are similar in some way. These are called <strong>matched
pairs</strong>.
If the two experimental units in each pair are randomly assigned to the two
treatments, the data can be analysed as described in this section.</p>
<p>The difference between the treatments is estimated more accurately than in
a completely randomised experiment.</p>




<h1 class="sectionName breakBefore">8.4 &nbsp; Comparing two proportions</h1>
<h2 class="pageName">8.4.1 &nbsp; Modelling two proportions</h2>

<p class="heading notPrinted">Two groups of successes and failures</p>
<p>We now consider data that are obtained as random samples from two populations,
with the sampled individuals being categorised into <em>successes</em> and <em>failures</em>.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/propnModel.gif" width="368" height="196"> </p>
<p>Since our  model  involves only two parameters, π<sub>1</sub> and π<sub>2</sub>,
 the two groups are the same only if π<sub>2</sub> - π<sub>1</sub> = 0. The value
of π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub> is usually unknown
but  can be estimated by <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>.
However   <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> is
a random quantity so its variability must
be taken into account when interpreting its value.</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/ethics.gif" width="514" height="336" class="summaryPict"></p>
<p>Note that the  questions do not refer to the specific 100  managers in the
study. They ask about differences between male and female managers 'in general'.</p>

<div class="centred"><div class="boxed"><p>We are interested in  π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub> rather
than  <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>, so we need
to understand the accuracy of our point estimate.</p></div></div>





<h2 class="pageName">8.4.2 &nbsp; Distribution of difference in proportions</h2>

<p class="heading">Equal population proportions (probabilities)?</p>
<p>If π<sub>1</sub> and π<sub>2</sub> are proportions in two populations and
the corresponding sample proportions are <em>p</em><sub>2</sub>&nbsp;and&nbsp;<em>p</em><sub>1</sub>,
we usually first ask whether π<sub>1</sub> and
π<sub>2</sub> are
equal and this is based on how far (<em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>)
is from zero. The values of (<em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>)
that would be consistent with equal population means depends on its distribution.</p>
<p class="heading">Distribution of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> (when
π<sub>1</sub> = π<sub>2</sub>)</p>
<p>In a test for whether π<sub>1</sub> and π<sub>2</sub> are equal, the null
hypothesis is</p>
<p class="eqn"><strong>H<sub>0</sub></strong> :  <span class="heading">π<sub>1</sub> = π<sub>2</sub></span></p>
<p>If <span class="eqn"><strong>H<sub>0</sub></strong> </span>is true,   we can
write <span class="heading">π<sub>1</sub> = π<sub>2</sub></span> = π, the <span class="heading">distribution
of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub></span> has
the following properties:</p>
<dl>
<dt>Mean</dt>
<dd>Its
mean is zero.</dd>
<dt>Standard deviation</dt>
<dd>Its  standard deviation is</dd>
</dl>
<p class=eqn><img src="../images/diffPropnSd.gif" width="213" height="55" alt="mu(p) and sd(p)"></p>
<dl><dt>Shape</dt>
<dd>If the sample sizes, <em>n</em><sub>1</sub> and <em>n</em><sub>2</sub>, are
reasonably large and  π  is neither very close
to zero or one, the <span class="heading">distribution of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub></span> is
close to a normal distribution.</dd>
</dl>
<p class="heading">Standard deviation in practice</p>
<p>To obtain a numerical value for the standard deviation above, we must replace
π by an estimate &mdash; the overall proportion in the combined samples, <em>p</em>.</p>
<p class=eqn><img src="../images/diffPropnSdEst.gif" width="393" height="54" alt="mu(p) and sd(p)"></p>


<h2 class="pageName">8.4.3 &nbsp; Testing for difference in probabilities</h2>

<p class="heading">Two-tailed test</p>

<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&pi;<strong><sub>2</sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&ne;&nbsp; </strong>&pi;<strong><sub>2</sub></strong></span></p>

<p>For this test, the steps involved in obtaining a p-value are: </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/pValCalc.gif" width="436" height="261"> </p>
<p>The p-value is interpreted in the same way as for all previous tests. A p-value
close to zero is unlikely when <b>H<sub>0</sub></b> is true, but is more likely
when <b>H<sub>A</sub></b> holds. Small p-values therefore provide evidence of
a difference between the population probabilities. </p>
<p class="heading">One-tailed test</p>
<p>In a 1-tailed test, the alternative hypothesis is</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&gt;&nbsp; 0</strong></span> &nbsp;&nbsp; <span class="red"><strong>or</strong></span> &nbsp;&nbsp; <span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&lt;&nbsp; 0</strong></span></p>
<p>The test statistic is identical to that for a 2-tailed test and the p-value
is obtained in a similar way, but it is found from only a <strong>single</strong> tail
of the standard normal distribution. </p>
<p class="heading">Two-tailed example</p>
<p align="center"><img src="../images/bangladeshTest.gif" width="541" height="476" class="summaryPict"></p>
<p class="heading">One-tailed example</p>
<p align="center"><img src="../images/pizzaTest.gif" width="541" height="476" class="summaryPict"></p>




<h1 class="sectionName breakBefore">8.5 &nbsp; Contingency table tests</h1>
<h2 class="pageName">8.5.1 &nbsp; Independence from samples</h2>

<p class="heading">Assessing independence from a sample</p>
<p>Independence is an important concept, but it is defined in terms of the joint <strong>population </strong> probabilites
and in most practical situations these are unknown. We must assess independence
from a <strong>sample</strong> of individuals  &mdash; a contingency
table.</p>
<p class="heading">Example</p>
<p>The contingency table below categorises a sample of 214 individuals by gender
and some other characteristic (possibly weight group or grade in a test). </p>
<div class="centred"><table class="centred" border="0" cellpadding="5" cellspacing="0">
<caption align="TOP">
<strong>Sample Data</strong>
</caption>
<tr>
<th align="CENTER">&nbsp;</th>
<th align="CENTER"><span class="blue">  Male  </span></th>
<th align="CENTER"><span class="blue">Female</span></th>
<th align="CENTER"><span class="red">Total</span></th>
</tr>
<tr>
<th align="CENTER"><span class="blue">A</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-left:1px solid #999999;">20</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-right:1px solid #999999;">60</td>
<td align="CENTER"><span class="red">80</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">B</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">9</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">84</td>
<td align="CENTER"><span class="red">93</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">C</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-left:1px solid #999999;">2</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-right:1px solid #999999;">39</td>
<td align="CENTER"><span class="red">41</span></td>
</tr>
<tr>
<th align="CENTER"><span class="red">Total</span></th>
<td align="CENTER"><span class="red">31</span></td>
<td align="CENTER"><span class="red">183</span></td>
<td align="CENTER"><span class="blue">214</span></td>
</tr>
</table></div>
<p>Is this consistent with a model of independence of the characteristic and gender?
(Are the probabilities of A, B and C grades the same for males and females?)</p>
<p class=heading>Estimated cell counts under independence</p>
<p>To assess independence, we first find the pattern of cell counts that is <strong>most
consistent with independence</strong> in a contingency table with the observed marginal
totals.</p>
<div class="centred"><table class="centred" border="0" cellpadding="5" cellspacing="0">
<tr>
<th align="CENTER">&nbsp;</th>
<th align="CENTER"><span class="blue">  Male  </span></th>
<th align="CENTER"><span class="blue">Female</span></th>
<th align="CENTER"><span class="red">Total</span></th>
</tr>
<tr>
<th align="CENTER"><span class="blue">A</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-left:1px solid #999999;">?</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-right:1px solid #999999;">?</td>
<td align="CENTER"><span class="red">80</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">B</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-left:1px solid #999999;">?</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-right:1px solid #999999;">?</td>
<td align="CENTER"><span class="red">93</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">C</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-left:1px solid #999999;">?</td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-right:1px solid #999999;">?</td>
<td align="CENTER"><span class="red">41</span></td>
</tr>
<tr>
<th align="CENTER"><span class="red">Total</span></th>
<td align="CENTER"><span class="red">31</span></td>
<td align="CENTER"><span class="red">183</span></td>
<td align="CENTER"><span class="blue">214</span></td>
</tr>
</table></div>
<p>The pattern that is most consistent with independence has the following <strong>estimated
cell counts</strong>:</p>
<p class=eqn><img class="gif" src="../../../en/indep/images/expCountEqn.gif" width="315" height="37"></p>
<p>where <span class="em blue">n</span>  denotes the total for the whole
table and <span class="em red">n<sub>x</sub></span>  and <span class="em red">n<sub>y</sub></span>  denote
the marginal totals for row <em>x</em> and column <em>y</em>.</p>
<p>Applying this to our example gives the following table:</p>
<div class="centred"><table border="0" class="centred" cellpadding="5" cellspacing="0">
<tr>
<th align="CENTER">&nbsp;</th>
<th width="143" align="CENTER"><span class="blue">Male</span></th>
<th width="143" align="CENTER"><span class="blue">Female</span></th>
<th align="CENTER"><span class="red">Total</span></th>
</tr>
<tr>
<th height="48" align="CENTER"><span class="blue">A</span></th>
<td height="48" colspan="2" rowspan="3" align="CENTER" bgcolor="#FFFFFF" style="border:1px solid #999999;"><img src="../../../en/indep/images/s_expectedTable.gif" width="279" height="134"></td>
<td height="48" align="CENTER"><span class="red">80</span></td>
</tr>
<tr>
<th height="48" align="CENTER"><span class="blue">B</span></th>
<td height="48" align="CENTER"><span class="red">93</span></td>
</tr>
<tr>
<th height="48" align="CENTER"><span class="blue">C</span></th>
<td height="48" align="CENTER"><span class="red">41</span></td>
</tr>
<tr>
<th align="CENTER"><span class="red">Total</span></th>
<td align="CENTER"><span class="red">31</span></td>
<td align="CENTER"><span class="red">183</span></td>
<td align="CENTER"><span class="blue">214</span></td>
</tr>
</table></div>




<h2 class="pageName">8.5.2 &nbsp; Testing for independence</h2>

<p class=heading>Comparison of observed and estimated cell counts</p>
<p>We test for independence with the hypotheses:</p>
<p class=eqn><span class="black"><strong><font size="+1">H</font><sub>0</sub> :</strong>&nbsp; <em>X and Y are independent</em><br><strong><font size="+1">H</font><sub>A</sub> :</strong>&nbsp; <em>X and Y are dependent</em>&nbsp;&nbsp;</span></p>
<p>The test asks whether the observed and
estimated cell counts are 'sufficiently close' &mdash; are the observed counts consistent
with the counts estimated under independence?</p>
<div class="centred"><table class="centred" border="0" cellpadding="5" cellspacing="0">
<caption align="TOP">
<strong><span class="blue">Observed</span> and <span class="green">estimated</span> cell
counts</strong>
</caption>
<tr>
<th align="CENTER">&nbsp;</th>
<th align="CENTER"><span class="blue">   Male   </span></th>
<th align="CENTER"><span class="blue"> Female </span></th>
<th align="CENTER"><span class="red">Total</span></th>
</tr>
<tr>
<th align="CENTER"><span class="blue">A</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-left:1px solid #999999;"><span class="blue">20</span><br>
<span class="green">(11.59)</span></td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-right:1px solid #999999;"><span class="blue">60</span><br>
<span class="green">(68.41)</span></td>
<td align="CENTER"><span class="red">80</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">B</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-left:1px solid #999999;"><span class="blue">9</span><br>
<span class="green">(13.47)</span></td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-right:1px solid #999999;"><span class="blue">84</span><br>
<span class="green">(79.53)</span></td>
<td align="CENTER"><span class="red">93</span></td>
</tr>
<tr>
<th align="CENTER"><span class="blue">C</span></th>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-left:1px solid #999999;"><span class="blue">2</span><br>
<span class="green">(5.94)</span></td>
<td align="CENTER" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999; border-right:1px solid #999999;"><span class="blue">39</span><br>
<span class="green">(35.06)</span></td>
<td align="CENTER"><span class="red">41</span></td>
</tr>
<tr>
<th align="CENTER"><span class="red">Total</span></th>
<td align="CENTER"><span class="red">31</span></td>
<td align="CENTER"><span class="red">183</span></td>
<td align="CENTER"><span class="blue">214</span></td>
</tr>
</table></div>
<p class="heading">Possible test statistic?</p>
<p>A simple summary of how close the observed counts, <span class="black em">n<sub>xy</sub></span>,
are to the estimated cell counts, <span class="black em">e<sub>xy</sub></span>, is
the sum of  the squared differences,</p>
<p class=eqn><img class="gif" src="../../../en/indep/images/ssqErrorForm.gif" width="100" height="26"> </p>
<p>Unfortunately this would be a bad test statistic &mdash; its distribution depends
not only on the numbers of rows and columns in the table, but also on the number
of individuals classified &mdash; the overall total for the table.</p>


<p class="heading">A better test statistic</p>
<p>The following χ<sup>2</sup> (pronounced <i>chi-squared</i>) statistic has
much better properties than the raw sum of squares on the previous page</p>
<p class=eqn><img class="gif" src="../../../en/indep/images/chiSquaredEqn.gif" width="145" height="47"> </p>
<p>Its distribution only depends on the number of rows and columns in the contingency
table.</p>


<h2 class="pageName">8.5.3 &nbsp; Chi-squared distribution</h2>

<p class="heading notPrinted">Distribution of chi-squared statistic</p>
<p>When there is independence, the χ<sup>2</sup> statistic for  a contingency table
with <i>r</i> rows and <i>c</i> columns has approximately
a standard distribution called a <strong>chi-squared</strong> distribution  with
(<i>r</i>&nbsp;-&nbsp;1)(<i>c</i>&nbsp;-&nbsp;1) <strong>degrees of freedom</strong>.</p>
<p>The mean of a chi-squared distribution equals its degrees of freedom and it is
skew. Some examples are given below for contingency tables of different sizes:</p>
<p class="eqn"><img src="../../../en/indep/images/s_chi2.gif" width="525" height="386"></p>




<h2 class="pageName">8.5.4 &nbsp; P-value for chi-squared test</h2>

<p class="heading">Testing for independence</p>
<p class=eqn><span class="black"><strong><font size="+1">H</font><sub>0</sub> :</strong>&nbsp; <em>X and Y are independent</em><br><strong><font size="+1">H</font><sub>A</sub> :</strong>&nbsp; <em>X and Y are dependent</em>&nbsp;&nbsp;</span> </p>
<p>The following test statistic is used:</p>
<p class=eqn><img class="gif" src="../../../en/indep/images/chiSquaredEqn.gif" width="145" height="47"> </p>
<dl>
<dt>If <em>X</em> and <em>Y</em> are independent</dt>
<dd>χ<sup>2</sup> has (approximately) a chi-squared distribution with no unknown
parameters</dd>
<dt>If <em>X</em> and <em>Y</em> are associated</dt>
<dd>The pattern of observed counts, <span class="black em">n<sub>xy</sub></span>, is
expected to be different from that of the <span class="black em">e<sub>xy</sub></span>,
so χ<sup>2</sup> is expected to be larger.</dd>
</dl>
<p class="heading">P-value</p>
<p class=eqn><img class="gif" src="../../../en/indep/images/chi2PValue.gif" width="456" height="262"> </p>
<p>The p-values is interpreted in the same way as for other hypothesis tests &mdash; it
describes the strength of evidence against the null hypothesis:</p>
<div class="centred"><table class="centred" border="0" cellpadding="5" cellspacing="0">
<tr>
<th align="left">p-value</th>
<th align="left">Interpretation</th>
</tr>
<tr bgcolor="white">
<td style="border-top:1px solid #999999;"><strong>over 0.1</strong></td>
<td style="border-top:1px solid #999999;">no evidence against the null hypothesis (independence)</td>
</tr>
<tr bgcolor="white">
<td><strong>between 0.05 and 0.1   </strong></td>
<td>very weak evidence of dependence between the row and column variables</td>
</tr>
<tr bgcolor="white">
<td><strong>between 0.01 and 0.05   </strong></td>
<td>moderately strong evidence of dependence between the row and column variables</td>
</tr>
<tr bgcolor="white">
<td style="border-bottom:1px solid #999999;"><strong>under 0.01</strong></td>
<td style="border-bottom:1px solid #999999;">strong evidence of dependence between the row and column variables</td>
</tr>
</table></div>
<p class="heading">Warning about low estimated cell counts</p>
<p>The χ<sup>2</sup> test statistic
has only <strong>approximately</strong> a
chi-squared distribution. The p-value found from it can be relied on if:</p>
<ul>
<li>All estimated cell counts are 1.0 or more</li>
<li>At least 80% of the estimated cell counts are 5.0 or more</li>
</ul>
<p>If the cell counts are small enought that these conditions do not hold, the p-value
is less reliable. (But advanced statistical methods are required to do better!)</p>




<h2 class="pageName">8.5.5 &nbsp; Examples</h2>

<p class=heading>Examples</p>
<p align="center"><img src="../../../en/indep/images/travelLitTest.gif" width="541" height="459" class="summaryPict"></p>
<p align="center"><img src="../../../en/indep/images/jobSatisTest.gif" width="541" height="459" class="summaryPict"></p>




<h2 class="pageName">8.5.6 &nbsp; Comparing groups</h2>

<p class=heading>Contingency tables and groups</p>
<p>Contingency tables can either arise from bivariate categorical data or from univariate
categorical data that is recorded separately from several groups.</p>
<p>The chi-squared test assesses independence in bivariate data. The <strong>same
test</strong> can also be used to compare the different groups if there is grouped
data.</p>
<dl>
<dt>Null hypothesis (corresponding to independence)</dt>
<dd>The category probabilities are the same within each group.</dd>
<dt>Alternative hypothesis (corresponding to association)</dt>
<dd>The different groups have different probabilities.</dd>
</dl>
<p class=heading>Example</p>
<p align="center"><img src="../../../en/indep/images/recruitTest.gif" width="541" height="459" class="summaryPict"></p>



</body>
</html>
