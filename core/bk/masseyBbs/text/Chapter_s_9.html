<!DOCTYPE HTML>
<html>
<head>
  <title>9. Regression</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 9 &nbsp; Regression</h1>
<h1 class="sectionName">9.1 &nbsp; Simple linear regression</h1>
<h2 class="pageName">9.1.1 &nbsp; Linear models</h2>

<p class="heading">Response and explanatory variables</p>
	<p>With bivariate data, it is often useful to predict the value of one variable
(the response variable,
<em>Y</em>)  from the other (the explanatory variable, <em>X</em>). </p>
<p>A curve or straight line  that is drawn close to the crosses on a scatterplot
 can be used to predict the y-value corresponding
to any <em>x</em>.</p>
<p class="eqn"><img src="../../../en/leastSqrs/images/s_predict.gif" width="357" height="272"></p>
<p>Note that the response variable should always be drawn on the <strong>vertical</strong> axis.</p>
<p class="heading">Linear model</p>
<p>A <strong>linear</strong> model is an adequate description of many bivariate
data sets:</p>
<p class=eqn><span class="black"><em>y</em> &nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span> </p>
	<p>The constant <em>b</em><sub>0</sub> is  the <strong>intercept</strong> of
the line and describes the <em>y</em>-value when <em>x</em> is zero. The constant <em>b</em><sub>1</sub> is
the line's <strong>slope</strong>; it 
		describes the change in <i>y</i> when <i>x</i> increases by one. </p>
<p class="eqn"><img src="../../../en/leastSqrs/images/s_slopeIntercept.gif" width="238" height="197"></p>


<h2 class="pageName">9.1.2 &nbsp; Predictions and residuals</h2>

<p class="heading">Fitted values</p>
	<p>To assess how well a particular linear model fits any one of our data points, (<em>x<sub>i</sub></em>,&nbsp;<em>y<sub>i</sub></em>), 
	we might consider how well the model would predict the y-value of the point, </p>
	<div class="centred">
		<table border="0" cellpadding="0" cellspacing="0" class="centred">
			<tr>
				<td valign="middle"><img src="../../../en/../images/symbol.yiHat.png" width="11" height="18" align="baseline"></td>
				<td valign="middle">&nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x<sub>i</sub></em></td>
			</tr>
		</table>
	</div>
	<p>These predictions  are called <strong>fitted 
		values</strong>. </p>
	<p class="heading">Residuals</p>
	<p>The difference between the <em>i</em>'th fitted values  and its actual 
		y-value is called its <strong>residual</strong>. </p>
	<div class="centred">
		<table border="0" cellpadding="0" cellspacing="0" class="centred">
			<tr>
				<td valign="middle"><em>e<sub>i</sub></em>&nbsp;&nbsp;=&nbsp;&nbsp;<em>y<sub>i</sub></em> &minus; </td>
				<td valign="middle"><img src="../../../en/../images/symbol.yiHat.png" width="11" height="18" align="baseline"></td>
			</tr>
		</table>
	</div>
	<p>The residuals describe the 'errors' that would have resulted from using the
 model to predict <em>y</em> from the <em>x</em>-values of our data points.</p>
<p class="eqn"><img src="../../../en/leastSqrs/images/s_resid.gif" width="351" height="207"></p>
	
	<p>Note that the residuals are the <strong>vertical distances of the crosses to the line</strong>.</p>
	
	



<h2 class="pageName">9.1.3 &nbsp; Least squares</h2>

<p class="heading">Aim of small residuals</p>
	<p>The residuals from a linear model (vertical distances from the crosses 
		to the line) indicate how closely the model's predictions  match the actual responses in the data. </p>
	<p class="eqn"><img src="../../../en/leastSqrs/images/s_allResids.gif" width="354" height="310"> </p>
	<p>'Good' values for <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub> can be
objectively chosen to be the values that minimise the residual sum of squares.
This is the <strong>method
of least squares</strong> and the values of <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub> are
called  <strong>least squares 
		estimates</strong>.</p>
<p>The diagram below respresents the squared residuals as blue squares. The least squares estimates minimise the total blue area.</p>
	<p class="eqn"><img src="../../../en/leastSqrs/images/s_sqrResids.gif" width="359" height="316"> </p>




<h2 class="pageName">9.1.4 &nbsp; Interest in generalising from data</h2>

<p class="heading notPrinted">Bivariate data: population or sample?</p>
<p>In most bivariate data sets, we have no interest in the specific individuals from
which the data are collected. The individuals are 'representative' of a larger population
or process, and our main interest is in this underlying population. </p>
<p class="heading">Example</p>
<p>A  newspaper compiled data from each of New Jersey's 21 counties about the
number of people per bank branch in each county and its percentage of minority
groups.</p>
<p class="eqn"><img src="../../../en/regnModel/images/s_sampleScatter_c.gif" width="403" height="316"></p>
<p>Local residents might be interested in the specific counties, but most outsiders
would want to <strong>generalise</strong> from the data to describe the relationship
in a way that might describe other <strong>similar</strong> areas in the Eastern
USA. How strong is the evidence that banks tend to have fewer branches in areas
with large minority groups?</p>




<h2 class="pageName">9.1.5 &nbsp; Normal linear model</h2>

<p class="heading notPrinted">Response distribution at each X</p>
<p>In an experiment, several response measurements are often made at each  distinct
value of <i>X</i>. The diagram below shows one such data set using a histogram for
the distribution of <em>Y</em> at
each x-value.</p>
<p class="eqn"><img src="../../../en/regnModel/images/s_histos.gif" width="380" height="331"></p>
<p class="heading">Model for data</p>
<p>The response measurements at any x-value can be modelled as a random sample from
a normal distribution. The collection of distributions of <em>Y</em> at different
values of <em>X</em> is called a <strong>regression model</strong>.</p>
<p class="eqn"><img src="../../../en/regnModel/images/s_pdfs.gif" width="349" height="373"></p>


<p class="heading">Normal linear model for the response</p>
<p>The most commonly used regression model is a <strong>normal linear model</strong>.
It involves: </p>
<dl>
<dt><strong>Normality</strong></dt>
<dd>At each value of <i>X</i>, <i>Y</i> has a normal distribution.</dd>
<dt><strong>Constant variance</strong></dt>
<dd>The standard deviation of <i>Y</i> is the same for all values of <i>X</i>.</dd>
<dt><strong>Linearity</strong></dt>
<dd>The mean of <i>Y</i> is linearly related to <i>X</i>.</dd>
</dl>
<p>The last two properties of the normal linear model can be expressed as </p>
<p class=eqn><span class="black">&sigma;<sub><em>y</em></sub> &nbsp;=&nbsp; &sigma;</span> </p>
<p class=eqn><span class="black">&mu;<sub><em>y</em></sub> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em></span> </p>


<h2 class="pageName">9.1.6 &nbsp; Another way to describe the model</h2>

<p class="heading notPrinted">Alternative descriptions of the model</p>
<p>The normal linear model describes the distribution of <em>Y</em> for any value
of <em>X</em>:</p>
<p class=eqn><span class="black"><em>Y</em>&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (&mu;<sub>y</sub>&nbsp;, &sigma;<sub>y</sub>)</span> </p>
<p>where</p>
<p class=eqn><span class="black">&mu;<sub><em>y</em></sub> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em></span> </p>
<p class=eqn><span class="black">&sigma;<sub><em>y</em></sub> &nbsp;=&nbsp; &sigma;</span> </p>
<p>An equivalent way to write the same model is...</p>
<p class=eqn><span class="black"><em>y<sub></sub></em> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em> &nbsp;+&nbsp; &epsilon;</span> </p>
<p>where  &epsilon; is
called the model <strong>error</strong> and has a distribution</p>
<p class=eqn><span class="black">&epsilon;&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (0<sub></sub>&nbsp;, &sigma;)</span> </p>
<p>The error, ε , for a data point is the <strong>vertical distance</strong> between the cross on a scatterplot and the regression
line.</p>
<p class=eqn><img class="gif" src="../../../en/regnModel/images/errorDiagram.gif" width="355" height="274"> </p>
<p class="heading">Band containing about 95% of values</p>
<p>Applying the 70-95-100 rule of thumb to the errors, about 95% of them
will be within 2 standard deviations of zero &mdash; i.e. between ±2σ.</p>
<p>Since the errors are vertical distances of data points to the regression line,
a  band
2σ on each side of it should contain about 95% of the crosses
on a scatterplot of the data.</p>
<p class=eqn><img class="gif" src="../../../en/regnModel/images/band95.gif" width="297" height="274"></p>




<h2 class="pageName">9.1.7 &nbsp; Model parameters</h2>

<p class="heading notPrinted">Slope and intercept</p>
<p>A normal linear model, </p>
<p class=eqn><span class="black">&mu;<sub><em>y</em></sub> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em></span> </p>
<p class=eqn><span class="black">&sigma;<sub><em>y</em></sub> &nbsp;=&nbsp; &sigma;</span> </p>
<p>involves 3 parameters, β<sub>0</sub>, β<sub>1</sub> and σ. The model's slope,
β<sub>1</sub>,
and intercept, β<sub>0</sub>, can be interpreted in a similar way to the slope
and intercept of a least squares line.</p> 
<dl>
<dt>Slope</dt>
<dd>Increase in the mean response per unit increase in <i>X</i>.</dd>
<dt>Intercept</dt>
<dd>Mean response when <em>X</em> = 0.</dd>
</dl>

<p class="heading" style="margin-bottom:5px; margin-top:35px">Examples of interpretation</p>
<div class="centred">
	<table class="centred" border="0" cellpadding="8" cellspacing="0">
		<tr>
			<th align="CENTER" width="300">Context</th>
			<th width="33%" align="CENTER">Interpretation of β<sub>1</sub></th>
			<th width="33%" align="CENTER">Interpretation of β<sub>0</sub></th>
		</tr>
		<tr bgcolor="#FFFFFF">
			<td width="300" style="border:1px solid #999999;"><i>Y</i> = Sales of a band's new album ($)<br>
				<i>X</i> = Money spent on advertising ($)</td>
			<td width="33%" style="border:1px solid #999999; border-left:0px;">Increase in mean sales for each extra dollar spent on advertising</td>
			<td width="33%" style="border:1px solid #999999; border-left:0px;">Mean sales if there was no advertising</td>
		</tr>
		<tr bgcolor="#FFFFFF">
			<td style="border:1px solid #999999; border-top:0px;"><i>Y</i> = Exam mark <br>
				<i>X</i> = Hours of study by student before exam</td>
			<td style="border-bottom:1px solid #999999; border-right:1px solid #999999;">Increase in expected mark for each additional
				hour of study</td>
			<td style="border-bottom:1px solid #999999; border-right:1px solid #999999;">Expected mark if there is no study</td>
		</tr>
		<tr bgcolor="#FFFFFF">
			<td style="border:1px solid #999999; border-top:0px;"><i>Y</i> = Hospital stay (days) <br>
				<i>X</i> = Age of patient</td>
			<td style="border-bottom:1px solid #999999; border-right:1px solid #999999;">Average extra days in hospital per extra
				year of age</td>
			<td style="border-bottom:1px solid #999999; border-right:1px solid #999999;">Average days in hospital at age 0. Not particularly
				meaningful here. </td>
		</tr>
	</table>
</div>


<h1 class="sectionName breakBefore">9.2 &nbsp; Linear model assumptions</h1>
<h2 class="pageName">9.2.1 &nbsp; Assumptions in a normal linear model</h2>

<p class="heading notPrinted">Assumptions</p>
<p>The normal linear model is:</p>
<p class=eqn><span class="black"><em>y<sub></sub></em> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em> &nbsp;+&nbsp; &epsilon;</span> </p>
<p class=eqn><span class="black">&epsilon;&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (0<sub></sub>&nbsp;, &sigma;)</span> </p>
<p>The following four requirements are implicit in the model but
may be violated, as illustrated by the examples. </p>

<div class="centred"><table border="0" cellspacing="0" cellpadding="0" class="centred">
<tr>
<td width="50%" align="LEFT" valign="top"><p class=heading>Linearity</p>
<p>The response may change nonlinearly with <em>x</em>.</p>
<p class="eqn"><img class="gif" src="../../../en/regnProblem/images/nonlinear.gif" width="246" height="166"></p></td>
<td width="220" align="LEFT" valign="top"><p class=heading>Constant standard deviation</p>
<p>The response may be more variable at some <em>x</em> than others.</p>
<p class="eqn"><img class="gif" src="../../../en/regnProblem/images/hetroskedastic.gif" width="246" height="166"></p></td>
</tr>
<tr>
<td align="LEFT" valign="top"><p class=heading>Normal distribution for errors</p>
<p>The errors may have skew distributions.</p>
<p class="eqn"><img class="gif" src="../../../en/regnProblem/images/skewError.gif" width="246" height="166"></p></td>
<td align="LEFT" valign="top"><p class=heading>Independent errors</p>
<p>When the observations are ordered in time, successive errors may be correlated.</p>
<p class="eqn"><img class="gif" src="../../../en/regnProblem/images/autoCorr.gif" width="246" height="166"></p></td>
</tr>
</table></div>




<h2 class="pageName">9.2.2 &nbsp; Residual plots</h2>

<p class="heading notPrinted">Detecting problems</p>
<p>Problems may be immediately apparent in a scatterplot of the raw data, but
a residual plot often highlights them.</p>
<p class="eqn"><img src="../../../en/regnProblem/images/s_residPlot.gif" width="606" height="293"></p>




<h2 class="pageName">9.2.3 &nbsp; Probability plot of residuals</h2>

<p class="heading notPrinted">Normal probability plot of residuals</p>
<p>The normal linear model assumes that the model errors are
normally distributed,</p>
<p class=eqn><span class="black">&epsilon;&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (0<sub></sub>&nbsp;, &sigma;)</span> </p>
<p>A histogram of the residuals can be examined for normality but a better way
 is with a <strong><a href="javascript:showNamedPage('normalDistn10')">normal
probability plot</a></strong> of the residuals. If the residuals are normally
distributed, the crosses in the normal probability plot should lie close to a
straight line.</p>
<p class="eqn"><img src="../../../en/regnProblem/images/s_probPlot.gif" width="550" height="274"></p>
<p class="heading">Warning</p>
<p>If the assumptions of linearity and constant variance are violated, or if
there are outliers, the probability plot of residuals will often be curved, irrespective
of the error distribution.</p>

<div class="centred"><div class="boxed">
<p>Only draw a probability plot if you are sure that the data are linear,
have constant variance and have no outliers.</p>
</div></div>





<h2 class="pageName">9.2.4 &nbsp; Outliers</h2>

<p class="heading notPrinted">Outliers and errors</p>
<p>In a scatterplot,  a cross that is unusually far above or below
the regression line is an <strong>outlier</strong>. It would correspond to a
large error, ε.</p>
<p class=eqn> <img class="gif" src="../../../en/regnProblem/images/outlierError.gif" width="297" height="274"> </p>

<p class="heading">Residual plot</p>
<p>Outliers  are usually clearer if the <strong>residuals</strong> are plotted against <i>X</i> rather
than the original response.</p>
<p class="eqn"><img src="../../../en/leastSqrs/images/s_residPlot.gif" width="557" height="264"></p>


<h2 class="pageName">9.2.5 &nbsp; Standardised residuals (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">Standardised residuals</p>
<p>To help assess the residuals, we usually standardise them &mdash; dividing
each by an estimate of its standard deviation.</p>
<div class="centred">
	<table border="0" cellspacing="0" cellpadding="0" class="centred" style="margin-top:0; margin-bottom:0; color:#000000">
		<tr>
			<td rowspan="3"><strong>standardised residual&nbsp; </strong>= &nbsp; </td>
			<td style="text-align:center"><em><font size="+1">e</font></em></td>
		</tr>
		<tr>
			<td style="border-bottom:1px solid #000000; line-height:3px; height:3px;"><img src="../../../images/blankSquare.gif"></td>
		</tr>
		<tr>
			<td style="text-align:center"><em><font size="+1">s<sub>e</sub></font></em></td>
		</tr>
	</table>
</div>
<p>The standardised residuals are each approximately normal (0, 1) if the normal
linear model fits, so only about 5% 
will be outside the range ±2, and hardly any outside the range ±3.</p>

<div class="centred"><div class="boxed">
<p>Standadised residuals greater than 3 or less than -3 are often taken
to indicate possible outliers.</p>
</div></div>

<p>Note however that in a large data set of 1,000 values, we would <strong>expect</strong> 50
values outside ±2 and 3
values outside ±3. Values a little outside ±3 <strong>can</strong> occur by chance.</p>


<h2 class="pageName">9.2.6 &nbsp; Outliers and leverage (opt) (Optional (not examined))</h2>

<p class="heading">Problems with residuals as indicators of outliers</p>
<p>All data points pull the least squares line towards themselves &mdash; the line
is positioned to minimise the sum of squares of the residuals</p>
<p class=eqn><span class="bold black">minimise</span>  &nbsp; <img class="gif" src="../../../en/regnProblem/images/minResidSsq.gif" width="37" height="20"></p>
<p>Large residuals pull very strongly on the line since they are <strong>squared</strong> in
the least squares criterion. As a result,</p>

<div class="centred"><div class="boxed">
<p>Outliers will strongly pull the least squares line towards themselves,
making their residuals smaller than you might otherwise expect.</p>
</div></div>

<p class="heading">Leverage</p>
<p>If an outlier corresponds to an <em>x</em>-value near its mean, it usually
will have a large residual,</p>
<p class="eqn"><img src="../../../en/regnProblem/images/s_outlier.gif" width="553" height="321"></p>
<p>However if the outlier occurs at an extreme <em>x</em>-value, it has a stronger
influence on the position of the least squares line than the other data points.
Such points are called <strong>high leverage</strong> points and pull the least
squares line strongly towards them. Outliers that are high leverage points may
therefore result in residuals that do not stand out from the other residuals.</p>
<p class="eqn"><img src="../../../en/regnProblem/images/s_leverage.gif" width="557" height="319"></p>





<h1 class="sectionName breakBefore">9.3 &nbsp; Inference for regression parameters</h1>
<h2 class="pageName">9.3.1 &nbsp; Estimating the slope and intercept</h2>

<p class="heading">Least squares</p>
<p>In practical situations, we must estimate β<sub>0</sub>,
β<sub>1</sub> and σ from a data set that we believe satisfies the normal linear model.</p>

<div class="centred"><div class="boxed">
<p>The best estimates of β<sub>0</sub> and β<sub>1</sub> are the slope and
intercept of the least squares line, <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub></p>
</div></div>

<p>Since<em> b</em><sub>0 </sub>and <em>b</em><sub>1</sub> are functions of a
data set that we assume to be a random sample from the normal linear model, <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub> are
themselves random quantities and have distributions.</p>
<p class="heading">Simulated example</p>
<p>The diagram below represents a regression model with a grey band. A sample
of 20 values has been generated from this model and the least squares line (shown
in blue) has been fitted to the simulated data. The least squares line provides
estimates of the slope and intercept but they are not exactly equal to the underlying
model values.</p>
<p class="eqn"><img src="../../../en/regnEst/images/s_lsAndModel.gif" width="406" height="357"></p>
<p>A different sample would give 20 different  points and a different least squares
line, so the least squares slope and intercept are random.</p>




<h2 class="pageName">9.3.2 &nbsp; Estimating the error standard devn</h2>

<p class="heading">Errors and residuals</p>
<p>The error, ε, for any data point is its <a href="javascript:showNamedPage('regnModel4')">vertical
distance from the regression line</a>. </p>
<p class=eqn><img class="gif" src="../../../en/regnModel/images/errorDiagram.gif" width="355" height="274"> </p>
<p>In practice, the slope and intercept of the regression line are unknown, so <strong>the
errors are also unknown values</strong>, but  the least squares residuals provide
<strong>estimates</strong>.</p>
<p class=eqn><img class="gif" src="../../../en/regnEst/images/residDiagram.gif" width="355" height="274"> </p>
<p class="heading">Estimating the error standard deviation</p>
<p>The third unknown parameter of the normal linear model, σ, is the standard deviation
of the errors,</p>
<p class=eqn><span class="black">&sigma; &nbsp;=&nbsp;<strong>st devn</strong>( &epsilon; )</span> </p>
<p>σ can be estimated from the least squares residuals, {<em>e<sub>i</sub></em>},</p>
<p class=eqn><img class="gif" src="../../../en/regnEst/images/errorSdEst.gif" width="95" height="41"> </p>
<p>This is similar to the formula for the standard deviation of the residuals,
but uses the divisor (<em>n</em> &minus; 2) instead of (<em>n</em> &minus; 1).
It describes the size of a 'typical' residual.</p>
<p class="heading">Example</p>
<p class="eqn"><img src="../../../en/regnEst/images/s_residSd.gif" width="463" height="291"></p>




<h2 class="pageName">9.3.3 &nbsp; Distn of least squares estimates</h2>

<p class="heading notPrinted">Distribution of the least squares slope and intercept</p>
<p>The least squares line<sub></sub> varies from sample
to sample &mdash; it is random.</p>
<p class="eqn"><img src="../../../en/regnEst/images/s_randomLines.gif" width="550" height="290"></p>
<p>The least squares estimates <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub> of
the two linear model parameters β<sub>0</sub> and β<sub>1</sub> therefore also
vary from sample to sample and have normal distributions that are centered
on β<sub>0</sub> and β<sub>1</sub> respectively.</p>
<p class="eqn"><img src="../../../en/regnEst/images/s_slopeInterceptDistn.gif" width="541" height="321"></p>




<h2 class="pageName">9.3.4 &nbsp; Standard error of least squares slope</h2>

<p class="heading notPrinted">Standard error of slope</p>
<p>When the least squares slope, <em>b</em><sub>1</sub>, is used as an estimate
of β<sub>1</sub>,
it has <strong>standard
error</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/regnEst/images/sdSlopeEqn.gif" width="227" height="42"> </p>
<p>where </p>
<ul>
<li>σ is the standard deviation of the errors &mdash; i.e. the spread of points<i></i> around
the regression line,</li>
<li><i>n</i> is the number of data points, and</li>
<li><em>s</em><sub>x</sub> is the sample standard deviation of <i>X</i>.</li>
</ul>
<p class=heading>Implications for data collection</p>
<div class="centred"><div class="boxed">
<p style="text-align:left">The standard error of <em>b</em><sub>1</sub> is lowest
when: </p>
<ol>
<li>the response standard deviation, σ, is low</li>
<li>the sample size, <i>n</i>, is large</li>
<li>the spread of x-values is high</li>
</ol>
</div></div>
<p>To get the most accurate estimate of the slope from experimental data,</p>
<dl>
<dt>Reduce σ</dt>
<dd>σ can be reduced by ensuring that the experimental units are as similar as
possible.</dd>
<dt>Increase <em>n</em></dt>
<dd>Collect as much data as possible.</dd>
<dt>Increase <em>s<sub>x</sub></em></dt>
<dd>Choose to run the experiment with x-values that are widely spread.</dd>
</dl>
<br>
<p>However don't just collect data at the ends of the 'acceptable'
range of x-values, even though this maximises <em>s</em><sub>x</sub>. </p>
<p class=eqn><img class="gif" src="../../../en/regnEst/images/nonlinearity2.gif" width="271" height="210"></p>


<h2 class="pageName">9.3.5 &nbsp; Testing whether slope is zero</h2>

<p class="heading">Does the response depend on X?</p>
<p>In a normal linear model, the response has a distribution whose mean, µ<sub>y</sub>,
depends linearly on the explanatory variable,</p>
<p class=eqn><span class="black"><em>Y</em>&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (&mu;<sub>y</sub>&nbsp;, &sigma;<sub>y</sub>)</span> </p>
<p>If the slope parameter, β<sub>1</sub>, is zero, then the response has a normal
distribution that <strong>does not depend on <i>X</i></strong>.</p>
<p class=eqn><span class="black"><em>Y</em> &nbsp; ~ &nbsp; <font face="Arial, Helvetica, sans-serif">normal</font> (&beta;<sub>0</sub> , &nbsp;&sigma;)</span> </p>
<p>This can be tested formally with a hypothesis test for whether β<sub>1</sub> is
zero.</p>
<p class="heading">Hypothesis test</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&beta;<sub>1</sub> &nbsp;=&nbsp; 0<br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&beta;<sub>1</sub>  &nbsp;&ne;&nbsp;  0</span> </p>
<p>The test is based on the 'statistical distance' of <em>b</em><sub>1</sub> from
zero,</p>
<p class=eqn><img class="gif" src="../../../en/regnTest/images/standardisedSlope2.gif" width="69" height="48"></p>
<p>and this has a t distribution with (<em>n</em> - 1) degrees of freedom if
there really is no relationship.</p>
<p class=eqn><img class="gif" src="../../../en/regnTest/images/slopeTestP.gif" width="426" height="268"> </p>
<p class="heading">Using output from statistical software</p>
<p>Computer software will provide everything you need to perform the test in
its regression output:</p>
<dl>
<dt>Least squares estimates</dt>
</dl>
<p class="eqn"><img src="../images/test_anova_ls.gif" width="336" height="119"></p>
<dl>
<dt>Standard error of slope</dt>
</dl>
<p class="eqn"><img src="../images/test_anova_se.gif" width="323" height="119"></p>
<dl>
<dt>Test statistic</dt>
</dl>
<p class="eqn"><img src="../images/test_anova_t.gif" width="328" height="133"></p>
<dl>
<dt>p-value</dt>
</dl>
<p class="eqn"><img src="../images/test_anova_pValue.gif" width="418" height="196"></p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/regnTest/images/whistleblowers.gif" width="550" height="442" class="summaryPict"></p>
<p align="center"><img src="../../../en/regnTest/images/grandfatherClocks.gif" width="550" height="442" class="summaryPict"></p>





<h2 class="pageName">9.3.6 &nbsp; Strength of evidence and relationship</h2>

<p class="heading notPrinted">Strength of  relationship vs strength of <span class="darkred">evidence</span> for
relationship</p>
<p>It is important to distinguish between the correlation coefficient, <em>r</em>,
and the p-value for testing whether there is a relationship between <em>X</em> and <em>Y</em>. </p>
<dl>
<dt>Correlation coefficient</dt>
<dd>Describes the strength of the relationship between <em>X</em> and <em>Y</em></dd>
<dt>The p-value for testing whether <em>X</em> and <em>Y</em> are related</dt>
<dd>Describes the strength of <strong>evidence</strong> for whether <em>X</em> and <em>Y</em> are
related <strong>at all</strong></dd>
</dl>
<p>It is important not to confuse these two values when interpreting the p-value
for a test.</p>
<ul>
<li>A p-value  close to zero does <strong>not</strong> imply that there must
be a strong relationship. It
just means that we are <strong>sure</strong> that
there is <strong>some</strong> relationship, however weak.</li>
<li>A large p-value  does <strong>not</strong> imply that the relationship
must be weak. The sample size might just be too small to be sure that the relationship
exists.</li>
</ul>
<p>This is partly explained by an alternative formula for the test statistic,</p>
<p class=eqn><img class="gif" src="../../../en/regnTest/images/s_tFromR.gif" width="162" height="48"></p>
<p>The test statistic and  the p-value therefore both depend on both <em>r</em> and
the sample size, <em>n</em>. Increasing <em>n</em> and increasing <em>r</em> <strong>both</strong> result
in a lower p-value.</p>
<p class=eqn><img class="gif" src="../../../en/regnTest/images/rnEffect.gif" width="550" height="440"></p>




<h1 class="sectionName breakBefore">9.4 &nbsp; Predicting the response</h1>
<h2 class="pageName">9.4.1 &nbsp; Point estimates of the response</h2>

<p class="heading notPrinted">Predicting the response at x</p>
<p>Our point estimate (best guess) for a the response at a particular
value of x is</p>
<p class=eqn><span class="black"><span style="position:relative; top:4px"><img src="../../../images/symbol.yHat.png" width="10" height="17" align="baseline"></span>&nbsp; =&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span> </p>
<p>Note that the least squares line should only be used for prediction when the
linear model assumptions hold. In particular there should be:</p>
<ol>
<li>No outliers or points with high leverage</li>
<li>No curvature</li>
</ol>
<p>It is also dangerous to predict far outside the range of the x's we have used
to fit the model (the training data) since we have no information about whether
the relationship remains linear. This is called <strong>extrapolation</strong>.</p>


<h2 class="pageName">9.4.2 &nbsp; Estimated response distn at X (opt) (Optional (not examined))</h2>

<p>A normal linear model provides
a response distribution <strong>for all <i>X</i></strong>. With estimates for
all three model parameters, we can obtain the approximate response distribution
at <strong>any</strong> x-value, even if we have no data at that x-value. </p>
<p class="eqn"><img src="../../../en/regnPred/images/s_estimDistn.gif" width="434" height="369"></p>




<h2 class="pageName">9.4.3 &nbsp; Variability of estimate at X (opt) (Optional (not examined))</h2>

<p class="heading notPrinted">What affects the accuracy of a prediction?</p>
<p>The predicted response at <i>X</i> is</p>
<p class=eqn><span class="black"><span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yHat.png" width="10" height="17" align="baseline"></span>&nbsp; =&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span> </p>
<p>and has a normal distribution
with mean</p>
<p class=eqn><span class="black">&mu;<sub><em>y</em></sub> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em></span> </p>
<p>Its standard deviation   depends on the value <i>x</i> at which the prediction
is being made. The further <i>x</i> is from
its mean in the training data, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">,
the greater the variability in the prediction.</p>
<p class="heading">Simulation</p>
<p>The effect of the x-value on the variability of the predicted response can
be shown using least squares lines fitted to simulated data:</p>
<p class="eqn"><img src="../../../en/regnPred/images/s_lsVariability.gif" width="550" height="294"></p>
<p>The diagram below shows two theoretical distributions from the above model.
(The spread would be even greater for predicting at <em>x</em> = 10.)</p>
<p class="eqn"><img src="../../../en/regnPred/images/s_predictDistn.gif" width="348" height="281"></p>




<h2 class="pageName">9.4.4 &nbsp; Estimating the mean vs prediction (opt) (Optional (not examined))</h2>

<p class="heading">Estimating the <span class="red">mean</span> response</p>
<p>In some situations, we are interested in estimating the mean response at some
x-value,</p>
<p class=eqn><span class="black">&mu;<sub><em>y</em></sub> &nbsp;=&nbsp; &beta;<sub>0</sub> &nbsp;+&nbsp; &beta;<sub>1</sub><em>x</em></span> </p>
<p>The least squares estimate, </p>
<p class=eqn><span class="black"><span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yHat.png" width="10" height="17" align="baseline"></span>&nbsp; =&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span> </p>
<p>becomes increasingly accurate as the sample size increases (since  <em>b</em><sub>0</sub> and <em>b</em><sub>1</sub> become
more accurate estimates of β<sub>0</sub> and β<sub>1</sub>).</p>
<p class=heading>Predicting a <span class="red">single item's</span> response</p>
<p>To predict the response for a <strong>single</strong> new
individual with a known x-value, the
same prediction would be used,</p>
<p class=eqn><span class="black"><span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yHat.png" width="10" height="17" align="baseline"></span>&nbsp; =&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span> </p>
<p>However no matter how accurately we estimate the <strong>mean</strong> response
for such individuals, a single new individual's response will have a distribution
with standard deviation σ around this mean and we have no information to help
us predict how far it will be from its mean. The 
prediction error <strong>cannot</strong> have a standard deviation
that is less than σ.</p>
<div class="centred"><div class="boxed"><p>The error in predicting an individual's response is usually greater than the
error in estimating the mean response.</p></div></div>
<p class=heading>Simulation</p>
<p>The diagram below contrasts estimation of the mean response and prediction
of a new individual's response at <em>x</em> = 5.5. Least squares lines have
been fitted to several simulated data sets, one of which is shown on the left.
The two kinds of errors from the simulations are shown on the right, showing
that the prediction errors are usually greater.</p>
<p class="eqn"><img src="../../../en/regnPred/images/s_estimPredict.gif" width="453" height="525"></p>




<h2 class="pageName">9.4.5 &nbsp; Confidence & prediction intervals (opt) (Optional (not examined))</h2>

<p>The same value,</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.yHat.png" width="10" height="17" align="baseline"></td>
			<td valign="middle">&nbsp; =&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></td>
		</tr>
	</table>
</div>
<p>is used both to estimate the mean response at <em>x</em> and to predict a
new individual's response at <em>x</em>, but the errors are different in
the two situations &mdash; they tend to be larger for predicting a new value.</p>
<p class="heading">95% confidence interval for  mean response </p>
<p class=eqn><img class="gif" src="../../../en/regnPred/images/ciForMeanY.gif" width="224" height="26"> </p>
<p>A formula for the standard error on the right exists, but you should rely
on statistical software to find its value.</p>
<p class=heading>95% prediction interval for a new individual's response</p>
<p>For prediction, a similar interval is used:</p>
<p class=eqn><img class="gif" src="../../../en/regnPred/images/predInterval.gif" width="169" height="21"> </p>
<p>where  <em>k</em> is greater than the corresponding standard error
for the confidence interval. Statistical software should again be used to find
its value.</p>
<p class=heading>Example</p>
<p>The diagram below shows 95% confidence intervals for the mean response at <em>x</em> and
95% prediction intervals for a new response at <em>x</em> as bands for a small
data set with <em>n</em> = 7 values. </p>
<p class="eqn"><img src="../../../en/regnPred/images/s_bands.gif" width="402" height="284"></p>
<p class=heading>Extrapolation</p>
<p>These 95% confidence intervals and 95% prediction intervals are valid within
the range of x-values about which we have collected data, but <strong>they should
not be relied on for extrapolation</strong>.
Both intervals assume that the normal linear model describes the process, but
we have no information about linearity beyond the x-values that have been collected.</p>




<h1 class="sectionName breakBefore">9.5 &nbsp; Coefficient of determination</h1>
<h2 class="pageName">9.5.1 &nbsp; Sums of squares</h2>

<p class="heading">Total variation</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img src="../../../en/regnAnova/images/ssqTotalAnova.gif" width="74" height="26" alt="SStotal"></td>
<td class="green">The <strong>total sum of squares</strong> reflects the total variability of the response.</td>
</tr>
</table></div>
<p>The overall variance of all response values is the total sum of squares divided
by (<em>n</em>&nbsp;-&nbsp;1).</p>
<p class="eqn"><img src="../images/s_totalSsq.gif" width="413" height="277"></p>
<p class="heading">Explained variation (signal)</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img src="../../../en/regnAnova/images/ssqRegnAnova.gif" width="75" height="26" alt="SSexplained"></td>
<td class="red">The <strong> explained sum of squares </strong> is the
variation that is explained by the model.</td>
</tr>
</table></div>
<p class="eqn"><img src="../images/s_regnSsq.gif" width="413" height="277"></p>
<p class="heading">Residual variation (noise)</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img src="../../../en/regnAnova/images/ssqResidAnova.gif" width="79" height="26" alt="SSresidual"></td>
<td class="blue">The <strong> residual sum of squares </strong> is the unexplained variation.</td>
</tr>
</table></div>
<p>Note that the pooled estimate of the error variance, σ<sup>2</sup>, is the
residual sum of squares  divided by (<em>n</em>&nbsp;-&nbsp;2).</p>
<p class="eqn"><img src="../images/s_residSsq.gif" width="413" height="277"></p>
<p class="heading">Relationship between sums of squares</p>
<p>The following relationship requires some algebra to prove but is important.</p>
<p class=eqn><img src="../../../en/regnAnova/images/ssqEqnAnova.gif" id="gif_image_1" width="300" height="47"></p>




<h2 class="pageName">9.5.2 &nbsp; Coefficient of determination</h2>

<p class="heading notPrinted">Coefficient of determination</p>

<div class="centred"><div class="boxed">
<dl>
<dt>When the relationship is strong,</dt>
<dd>...the explained sum of squares is close to the total sum of squares (and
the residual sum of squares is small).</dd>
<dt>When the relationship is weak,</dt>
<dd>...the explained sum of squares is small relative to the total sum of squares.</dd>
</dl>
</div></div>

<p>A useful summary statistic is the <strong>proportion</strong> of
the total variation that is explained, <strong>the coefficient of determination</strong>, <em>R</em><sup>2</sup>, </p>
<p class=eqn><img src="../../../en/regnAnova/images/r2Defn3.gif" width="110" height="38" alt="R-squared = SSexplained / SStotal"> </p>
<p>A proportion (1&nbsp;-&nbsp;<em>R</em><sup>2</sup>)
of the total variation remains <strong>unexplained</strong> by
the  model.</p>
<p>Although it is derived with quite a different aim,</p>
<p class=eqn><img src="../../../en/regnAnova/images/rSquaredEqn.gif" width="230" height="24" alt="R-squared = corr-squared"></p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/regnAnova/images/cancerRadiation.gif" width="550" height="344" class="summaryPict"></p>


<h1 class="sectionName breakBefore">9.6 &nbsp; Multiple regression</h1>
<h2 class="pageName">9.6.1 &nbsp; More than one explanatory variable</h2>

<p class="heading">Response and explanatory variables</p>
	<p>We are often interested in how 
		a  'response' variable, <em>Y</em>, depends on other explanatory variables. If there is a <strong>single</strong> explanatory variable, <em>X</em>,  we can predict <em>Y</em> from <em>X</em> with a simple linear model of the form,</p>
	<p class=eqn><span class="black"><em>y</em> &nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span>
	<p>However if other explanatory variables have been recorded from each individual,  we should be able to use them to predict the response more accurately.</p>
	



<h2 class="pageName">9.6.2 &nbsp; Multiple regression equation</h2>

<p class="heading">Adding extra variables</p>
	<p>A simple linear model for a single explanatory variable,</p>
	<p class=eqn><span class="black"><em>y</em> &nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em></span>
	<p>can be easily extended to describe the effect of a  second explanatory variable, <em>Z</em>, with an extra linear term,</p>
	<p class="eqn"><span class="black"><em>y</em> &nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em> + <em>b</em><sub>2 </sub><em>z</em></span></p>
	<p>and so on with more explanatory variables,</p>
	<p class="eqn"><span class="black"><em>y</em> &nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x</em> + <em>b</em><sub>2 </sub><em>z</em> + <em>b</em><sub>3 </sub><em>w</em> + ...</span></p>
	<p>This type of model is called a <strong>multiple regression</strong> model.</p>
	<p class="heading">Coefficients</p>
	<p>Despite our use of the same symbols (<em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, ...) for all three models above, their 'best' values are often different for the different models. An example will be given in the next page.</p>




<h2 class="pageName">9.6.3 &nbsp; Interpreting coefficients</h2>

<p class="heading">Marginal and conditional relationships</p>
	<p>In a linear model that predicts a response from several explanatory variables, the least squares coefficient associated with any explanatory variable describes its effect on the response <strong>if all other variables are held constant</strong>. 
		This is also called the variable's <strong>conditional effect</strong> on 
		the response.</p>
	<p>This may be very different from the size and even the sign of the coefficient when a linear model is fitted with <strong>only that single explanatory variable</strong>. This simple linear model describes the <strong>marginal</strong> relationship between the response and that variable.</p>
	<p class="heading">Example</p>
	<p>In a model for predicting the percentage body fat of men, the best model (as determined by least squares) in a simple model with <em>weight</em>, is</p>
	<p class="eqn"><em>Predicted body fat</em>   =   -10.00  +  0.162 <em>Weight</em></p>
	<p>However if we add <em>Abdomen circumference</em> to the model, the best values for the coefficients are</p>
	<p class="eqn"><em>Predicted body fat</em>   =   -41.35  -  0.136 <em>Weight</em>  +  0.915 <em>Abdomen</em></p>
	<ul>
		<li>For each 1lb extra <em>Weight</em>, men have, on average, 0.162% <strong>more</strong> body fat.</li>
		<li>For each 1lb extra <em>Weight</em>, men have, on average, 0.136% <strong>less</strong> body fat <strong>than others with the same <em>Abdomen circumference</em></strong>.</li>
	</ul>




<h2 class="pageName">9.6.4 &nbsp; Standard errors</h2>

<p class="heading">General linear model</p>
<p>The general linear model is</p>
<p class="eqn"><em>y</em> &nbsp;= &nbsp;&beta;<sub>0</sub>&nbsp; +&nbsp;&beta;<sub>1</sub> <em>x</em><sub>1</sub>&nbsp; +&nbsp;&beta;<sub>2</sub><em> x</em><sub>2</sub>&nbsp; +&nbsp;&beta;<sub>3</sub><em> x</em><sub>3</sub>&nbsp; +&nbsp; ...&nbsp; +&nbsp; ε</p>
<p>where</p>
<p class=eqn>ε &nbsp; ~ &nbsp; normal (0, <span class="green">σ</span>)</p>
<p class="heading">Parameter estimates and standard errors</p>
<p>The best estimates of <span class="eqn">&beta;<sub>0</sub>, &beta;<sub>1</sub></span>,
... are the least squares estimates, <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, ...</p>
<p>The best estimate of <span class="green">σ<sup>2</sup></span> is the <strong>residual
sum of squares</strong>, divided by its <strong>degrees of freedom</strong>,</p>
<p class=eqn><img src="../images/varianceEst.gif" width="84" height="42"></p>
<p>where <em>n</em> is the number of observations and <em>p</em> is the number of
<span class="eqn">&beta;</span>-parameters (i.e. the number of explanatory variables <strong>plus
1</strong>).</p>
<p>The least squares estimates, <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, ...
are random quantities and have distributions. The formulae for their standard errors
are complex but statistical software will report their values.</p>
<p class="heading">Example</p>
<p>The equation below gives the least squares equation for predicting the percentage
body fat of men, based on other body measurements.</p>
<p class=eqn><img src="../../../en/glm/images/fatPredictionEqn.gif" width="490" height="58"></p>
<p>The table below shows the standard errors of these coefficients and the estimate
of the error standard deviation, &sigma;.</p>
<p class="eqn"><img src="../images/s_multiRegnSe.gif" width="444" height="339"></p>


<h2 class="pageName">9.6.5 &nbsp; Inference for general linear models</h2>

<p class="heading">Hypothesis tests for single parameters</p>
<p class=eqn><img src="../../../en/glm/images/paramHypoth.gif" width="83" height="46"></p>

<div class="centred"><div class="boxed">
<p>This test asks whether the corresponding explanatory
variable can be dropped from the full model.</p>
</div></div>

<p>The test statistic is the 'statistical distance' of the least squares estimate, <em>b<sub>i</sub></em>,
from zero,</p>
<p class=eqn><img src="../../../en/glm/images/paramTest.gif" width="78" height="41"></p>
<p>and its p-value is  found from the tail area of the t distribution with (<em>n</em>&nbsp;-&nbsp;<em>p</em>)
degrees of freedom.</p>
<p class="heading">Interpretation of p-values</p>
<p> The p-values are interpreted in the usual way as the strength of evidence
against the null hypothesis.</p>
<p>However each p-value assesses whether you can drop a  single explanatory
variable from the <strong>full</strong> model. After dropping one variable from
the full model, the p-values for the other variables will change and they may
no longer be unimportant.</p>

<div class="centred"><div class="boxed">
<p>If several explanatory variables have high p-values,
this does <strong>not</strong> give evidence that you can <strong>simultaneously</strong> drop
all of them from the model.</p>
</div></div>

<p class="heading">Example</p>
<p>The table below shows the p-values for testing whether the individual parameters
are zero in the body fat model. Several p-values are higher than 0.1, giving evidence
that these variables could be dropped from the full model but this does <strong>not</strong> mean
that we could drop all such variables simultaneously.</p>
<p class="eqn"><img src="../images/s_multiRegnPvalue.gif" width="528" height="339"></p>


<h1 class="sectionName">9.7 &nbsp; What will be assessed?</h1>


<p class="heading">What you need to know</p>
  
  
<p>You will not be examined about everything in this chapter. Some of the material has been included to explain <strong>why</strong> the chapter's methods are used, in the hope that it will help you to understand these methods better. What you need to learn for the exam is more limited.</p>
<p>We now describe what we expect you to be able to <strong>do</strong> in the assignment and exam after studying the regression chapter.</p>
<p class="heading">A.   Simple linear regression</p>
<p>Simple linear regression models are used to predict the value of a &quot;response&quot; from a single &quot;explanatory&quot; variable.</p>
<dl class="level1">
	<dt>1.   Identify the response and explanatory variable</dt>
	<dd>When a scenario is described in words, identify  the response and  explanatory variable.</dd>
	<dt>2.   Describe the relationship</dt>
	<dd>When shown a scatterplot of Y against X, describe the relationship between the variables:
		<ul>
			<li>Positive or negative?</li>
			<li>Linear or curved?</li>
			<li>Strong or weak?</li>
		</ul>
	</dd>
	<dt>3.   Given Excel regression output, you need to:</dt>
	<dd>(a)   Regression model:
		<ul>
			<li>Find the values of the slope and intercept in the Excel output.</li>
			<li>Write down the regression model using these  values.</li>
			<li>Use the regression model to predict the response from a value of the explanatory variable.</li>
		</ul>
	</dd>
	<dd>(b)   Slope and intercept:
		<ul>
			<li>Explain what the values of the slope and intercept describe, in a way that a non-statistician might understand.</li>
			<li>Use the p-value associated with the explanatory variable to test whether the variables are related.</li>
			<li>Explain your conclusion from this hypothesis test to a non-statistician.</li>
		</ul>
	</dd>
	<dd>(c)   Coefficient of determination, R<sup>2</sup>:
<ul>
			<li>Identify the coefficient of determination, R<sup>2</sup> from the Excel output.</li>
			<li>Interpret the R<sup>2</sup> value  (in terms of the percentage of response variation explained by the model).</li>
		</ul>
	</dd>
	<dt>4.   From a scatterplot of residuals against the explanatory variable,</dt>
	<dd class="lessIndent">
		<ul>
			<li>List the assumptions required for the model (linearity, constant variance, independence).</li>
			<li>Use the scatterplot to discuss whether the model assumptions hold.</li>
		</ul>
	</dd>
</dl>

<p class="heading">B.   Multiple regression</p>
<p>Multiple  regression models extend the idea of simple linear regression with two or more explanatory variables. Given Excel output from  a multiple regression model, you should be able to:</p>
<dl class="level1">
	<dt>1.   Regression model</dt>
	<dd class="lessIndent">
		<ul>
			<li>Write down the regression model that best predicts the response from all explanatory variables.</li>
			<li>Use the regression model to predict the response from particular values of all explanatory variables.</li>
		</ul>
	</dd>
	<dt>2.   Slope parameters</dt>
	<dd class="lessIndent">
		<ul>
		<li>Explain what the values of the slope parameters describe, in a way that a non-statistician might understand.</li>
		</ul>
	</dd>
	
	<dt>3.   Important explanatory variables</dt>
	<dd class="lessIndent">
		<ul>
			<li>Interpret the p-values associated with the different explanatory variables.</li>
			<li>Explain which explanatory variables are most important and which might be considered for dropping from the model.</li>
		</ul>
	</dd>
	
	<dt>4.   Coefficient of determination</dt>
	<dd class="lessIndent">
		<ul>
			<li>Identify the coefficient of determination, R<sup>2</sup>.</li>
			<li>Interpret the R<sup>2</sup> value  (in terms of the percentage of response variation explained by the model).</li>
		</ul>
	</dd>
</dl>


</body>
</html>
