<html>
<head>
<title>9. Regression</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 9 &nbsp; Regression</h1>
<h2>9.1 &nbsp; Simple linear regression</h2>
<h3>9.1.1 &nbsp; Linear models</h3>
<p>A line or curve is useful for predicting the value of Y from a known value of X. A straight line can often be used to predict one variable from another.</p>
<h3>9.1.2 &nbsp; Predictions and residuals</h3>
<p>The difference between the actual value of Y and the value predicted by a line is called a residual. Small residuals are clearly desirable.</p>
<h3>9.1.3 &nbsp; Least squares</h3>
<p>The sum of squared residuals describes the accuracy of predictions from a line. The method of least squares positions the line to minimise the sum of squared residuals.</p>
<h3>9.1.4 &nbsp; Interest in generalising from data</h3>
<p>Some bivariate data sets describe complete populations. Others are 'representative' of an underlying population or process.</p>
<h3>9.1.5 &nbsp; Normal linear model</h3>
<p>Bivariate data can be modelled with a response distribution for each possible X. In a normal linear model, we use a normal distribution whose mean is a linear function of X and whose standard deviation is constant.</p>
<h3>9.1.6 &nbsp; Another way to describe the model</h3>
<p>A normal linar model can be described in terms of 'errors'. In samples from the model, approximately 95% of errors are within 2 standard deviations of zero, so about 95% of the points in a scatterplot are within this distance of the regression line.</p>
<h3>9.1.7 &nbsp; Model parameters</h3>
<p>The normal linear model has 3 unknown parameters. For many data sets, these parameters have meaningful interpretations.</p>
<h3>9.1.8 &nbsp; Exercise: Pick the explanatory variable and response</h3>
<p>For several scenarios, you must identify the explanatory variable and response, then state whether the data are observational or experimental and whether the relationship is causal.</p>
<h3>9.1.9 &nbsp; Exercise: Draw a straight line</h3>
<p>This exercise shows the equation of a straight line and asks you to sketch it.</p>
<h3>9.1.10 &nbsp; Exercise: Find the slope and intercept</h3>
<p>The exercises on this page do the inverse of the previous exercise — you are shown a straight line and asked to find its equation.</p>
<h3>9.1.11 &nbsp; Exercise: Interpret the slope and intercept</h3>
<p>In this exercise, you are asked to select one of four statements that correctly describes the slope or intercept of a least squares line in the context of the data.</p>
<h3>9.1.12 &nbsp; Exercise: Find a residual</h3>
<p>This exercise requests the least squares residual for a cross on a scatterplot.</p>
<h2>9.2 &nbsp; Linear model assumptions</h2>
<h3>9.2.1 &nbsp; Assumptions in a normal linear model</h3>
<p>The normal linear model involves assumptions of linearity, constant variance, normal error distribution and independence of different observations. Residuals can be examined to assess whether these assumptions are appropriate for a particular data set.</p>
<h3>9.2.2 &nbsp; Residual plots</h3>
<p>Outliers and curvature in the relationship are often displayed more clearly in a plot of residuals.</p>
<h3>9.2.3 &nbsp; Probability plot of residuals</h3>
<p>Curvature in a probability plot of residuals can indicate problems with the model. Curvature can be caused by a non-normal error distribution but may also result from other problems with the model.</p>
<h3>9.2.4 &nbsp; Outliers</h3>
<p>An outlier is a response value that is unusually large or small.</p>
<h3>9.2.5 &nbsp; Standardised residuals (opt) (Optional (not examined))</h3>
<p>An extreme residual suggests an outlier and standardised residuals can be used to assess it.</p>
<h3>9.2.6 &nbsp; Outliers and leverage (opt) (Optional (not examined))</h3>
<p>If an outlier corresponds to an extreme x-value (a high leverage point) it may not show up as a large residual.</p>
<h3>9.2.7 &nbsp; Exercise: Pick the correct residual plot</h3>
<p>In this exercise, you must identify which of four scatterplots is the correct residual plot when a linear model is fitted to a data set.</p>
<h3>9.2.8 &nbsp; Exercise: Identify regression problems (opt) (Optional (not examined))</h3>
<p>This exercise asks you to identify the difficulties with using the least squares line to predict Y at a given X from the data in a displayed scatterplot (an outlier, curvature, a high-leverage point or extrapolation).</p>
<h2>9.3 &nbsp; Inference for regression parameters</h2>
<h3>9.3.1 &nbsp; Estimating the slope and intercept</h3>
<p>A least squares line provides estimates of the linear model's slope and intercept. These estimates are random values — they vary from sample to sample.</p>
<h3>9.3.2 &nbsp; Estimating the error standard devn</h3>
<p>The third parameter of the normal linear model is the error standard deviation. It can be estimated using the residuals from the least squares line.</p>
<h3>9.3.3 &nbsp; Distn of least squares estimates</h3>
<p>The least squares estimate of the model's slope has a normal distribution that is centred on the true value.</p>
<h3>9.3.4 &nbsp; Standard error of least squares slope</h3>
<p>The distribution of the least squares slope may be estimated from a single data set.</p>
<h3>9.3.5 &nbsp; Testing whether slope is zero</h3>
<p>If the model's slope is zero, the response distribution does not depend on the explanatory variable. The p-value for testing this is the probability that its least squares estimate is as far from zero as the recorded value.</p>
<h3>9.3.6 &nbsp; Strength of evidence and relationship</h3>
<p>It is important to distinguish the strength of a relationship (summarised by the correlation coefficient) and the strength of evidence for existence of a relationship (summarised by the p-value).</p>
<h3>9.3.7 &nbsp; Exercise:  Standard error of slope</h3>
<p>Any estimator's standard error gives information about its accuracy. The exercise on this page gives the standard error of a least squares line's slope and asks for a roughly calculated interval that is likely to include the underlying model's slope. (T values are not required in this exercise.)</p>
<h3>9.3.8 &nbsp; Exercise:  Confidence interval for slope</h3>
<p>In the two exercises on this page, confidence intervals for a regression model's slope should be calculated from the least squares slope and its standard error. The second exercise is a little harder -- it asks for various confidence levels.</p>
<h3>9.3.9 &nbsp; Exercise:  Test for zero slope</h3>
<p>This exercise gives the least squares slope and its standard error. The p-value for testing whether the regression slope is zero should be calculated and interpreted.</p>
<h2>9.4 &nbsp; Predicting the response</h2>
<h3>9.4.1 &nbsp; Point estimates of the response</h3>
<p>The least squares intercept and slope allow us to predict the response corresponding to any x-value.</p>
<h3>9.4.2 &nbsp; Estimated response distn at X (opt) (Optional (not examined))</h3>
<p>The regression model means that the response has a normal distribution at any x-value. The estimates of the 3 linear model parameters provide an estimated response distribution corresponding to any x.</p>
<h3>9.4.3 &nbsp; Variability of estimate at X (opt) (Optional (not examined))</h3>
<p>The predicted response at any X varies from sample to sample. The prediction is more variable at x-values far from the mean of the 'training' data.</p>
<h3>9.4.4 &nbsp; Estimating the mean vs prediction (opt) (Optional (not examined))</h3>
<p>A distinction is made between estimating the mean response at X and predicting a new individual's response at X. Errors are larger (on average) when predicting a new individual's response.</p>
<h3>9.4.5 &nbsp; Confidence & prediction intervals (opt) (Optional (not examined))</h3>
<p>A 95% confidence interval is used to estimate the mean response at X. A 95% prediction interval is similar, but gives a range of likely values for a new response value. The prediction interval is wider than the confidence interval. </p>
<h3>9.4.6 &nbsp; Exercise: Predict the response</h3>
<p>The exercise on this page gives a least squares line and asks for a prediction of the response, given the value of the explanatory variable.</p>
<h2>9.5 &nbsp; Coefficient of determination</h2>
<h3>9.5.1 &nbsp; Sums of squares</h3>
<p>The overall variation in the response can be split into two component. The explained sum of squares summarises response variation that can be explained by X; the residual sum of squares summarises unexplained response variation.</p>
<h3>9.5.2 &nbsp; Coefficient of determination</h3>
<p>The relative sizes of the explained and residual sums of squares holds information about the strength of the relationship. The coefficient of determination describes the proportion of total variation that is explained.</p>
<h2>9.6 &nbsp; Multiple regression</h2>
<h3>9.6.1 &nbsp; More than one explanatory variable</h3>
<p>In many data sets, two or more explanatory variables could potentially affect the response. Using two or more explanatory variables may give more accurate predictions.</p>
<h3>9.6.2 &nbsp; Multiple regression equation</h3>
<p>A simple linear model with a single explanatory variable can be extended with extra terms to explain the additional effect of other explanatory variables.</p>
<h3>9.6.3 &nbsp; Interpreting coefficients</h3>
<p>The slope coefficient associated with an explanatory variable describes its effect if all other variables are held constant. It may have a different sign from the correlation coefficient between the variable and the response.</p>
<h3>9.6.4 &nbsp; Standard errors</h3>
<p>The error standard deviation can be estimated from the residual sum of squares. A simple matrix equation uses this estimate to find the standard errors of the least squares estimates.</p>
<h3>9.6.5 &nbsp; Inference for general linear models</h3>
<p>The significance of the individual parameters can be tested, but each such test assumes that all other variables are retained in the model.</p>
<h2>9.7 &nbsp; What will be assessed?</h2>
</body>
</html>
