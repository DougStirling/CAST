<html>
<head>
<title>14. Multiple Regression</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 14 &nbsp; Multiple Regression</h1>
<h2>14.1 &nbsp; Least squares for Y vs (X and Z)</h2>
<h3>14.1.1 &nbsp; More than one explanatory variable</h3>
<p>In many data sets, two or more explanatory variables could potentially affect the response. Using two or more explanatory variables may give more accurate predictions.</p>
<h3>14.1.2 &nbsp; Three-dimensional scatterplots</h3>
<p>Data sets with two explanatory variables and a response can be effectively displayed in a rotating 3-dimensional scatterplot.</p>
<h3>14.1.3 &nbsp; Linear equation and least squares plane</h3>
<p>The simple linear model can be extended by adding another linear term involving a second explanatory variable. This equation represents a plane in 3-dimensions.</p>
<h3>14.1.4 &nbsp; Understanding the parameters</h3>
<p>The intercept is the predicted y-value when x and z are zero. The two slope parameters give the predicted change in y when x and z increase by one.</p>
<h3>14.1.5 &nbsp; Fitted values and residuals</h3>
<p>The linear model provides predictions at all x- and z-values. The prediction for the x- and z-value corresponding to the i'th data point is its fitted value and the difference between this and the recorded y-value is its residual.</p>
<h3>14.1.6 &nbsp; Estimating the parameters</h3>
<p>Parameter estimates that result in small residuals are good.</p>
<h3>14.1.7 &nbsp; Least squares estimation</h3>
<p>An objective estimation method is to minimise the sum of squared residuals -- the principle of least squares.</p>
<h3>14.1.8 &nbsp; Interpreting the coefficients</h3>
<p>The slope coefficients give the predicted effect of changes to one variable, but only when the other variable remains the same.</p>
<h2>14.2 &nbsp; Normal linear model and inference</h2>
<h3>14.2.1 &nbsp; Normal linear model</h3>
<p>Randomness can be modelled by assuming that the response has a normal distribution whose mean is a linear function of the explanatory variables and whose standard deviation is constant.</p>
<h3>14.2.2 &nbsp; Sampling variability of least squares plane</h3>
<p>The normal linear model also implies that the least squares plane varies from sample to sample.</p>
<h3>14.2.3 &nbsp; Distribution of estimated coefficients</h3>
<p>The least squares estimate of each coefficient has a normal distribution whose mean is the underlying population parameter.</p>
<h3>14.2.4 &nbsp; Estimate of error standard deviation</h3>
<p>The error variance is estimated by the sum of squared residuals divided by (n-3). The best estimate of the error standard deviation is the square root of this.</p>
<h3>14.2.5 &nbsp; 95% confidence intervals for coefficients</h3>
<p>The standard deviation of each parameter estimate depends on the error standard deviation. Replacing this with an estimate allows us to find a 95% confidence interval. </p>
<h3>14.2.6 &nbsp; Hypothesis tests for coefficients</h3>
<p>A t test-statistic can be found by dividing a parameter estimate by its standard error. The p-value for testing whether the parameter is zero is the tail-area of a t distribution with (n-3) degrees of freedom.</p>
<h2>14.3 &nbsp; Randomised blocks</h2>
<h3>14.3.1 &nbsp; Generalising the idea of paired data</h3>
<p>In some data sets, the values arise in blocks of 3 or more related measurements. Randomised block and repeated measure data are of this form.</p>
<h3>14.3.2 &nbsp; Example with baseline treatment</h3>
<p>Ignoring the blocking of values loses important information about the difference between treatments. Comparing treatments separately against a baseline treatment using paired differences may be possible.</p>
<h3>14.3.3 &nbsp; Use of blocking information</h3>
<p>If there is no baseline treatment against which to compare the other measurements in each block, it is possible to simultaneously test whether all treatment means are equal. Again, ignoring the blocks loses important information.</p>
<h3>14.3.4 &nbsp; Randomised block designs</h3>
<p>Data of this form often arises from a randomised block experiment in which the experimental units occur in related blocks and treatments are randomly allocated within each block.</p>
<h3>14.3.5 &nbsp; Model for randomised blocks</h3>
<p>Although blocks and treatments arise in different ways, they are modelled similarly. A 3-dimensional display of the data represents both blocks and treatments in the same way.</p>
<h3>14.3.6 &nbsp; Removing block effects</h3>
<p>The variation between blocks can be removed by adding/subtracting a value to each block to make all block means equal. This reduces the residual (unexplained) sum of squares.</p>
<h3>14.3.7 &nbsp; Sums of squares</h3>
<p>The total sum of squares can be split into sums of squares for blocks and treatments, and a residual sum of squares.</p>
<h3>14.3.8 &nbsp; Anova table and examples</h3>
<p>An anova table shows these sums of squares and associated degrees of freedom. The F-ratio for treatments in the table is the basis of a test for equal treatment means. Several examples are given.</p>
</html>
