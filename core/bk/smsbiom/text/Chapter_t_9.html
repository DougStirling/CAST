<html>
<head>
<title>9. General Modelling Ideas</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 9 &nbsp; General Modelling Ideas</h1>
<h2>9.1 &nbsp; Groups and regression</h2>
<h3>9.1.1 &nbsp; Additional variables in regression</h3>
<p>Correlation and least squares are used to describe the relationship between two numerical variables. Additional measurements from each individual can potentially help to refine our understanding of the relationship.</p>
<h3>9.1.2 &nbsp; Displaying groups</h3>
<p>Different symbols or colours can be used to represent a third categorical variable in a scatterplot.</p>
<h3>9.1.3 &nbsp; Regression with grouped data</h3>
<p>The relationship between Y and X can be separately described by a least squares line within each group. This should lead to improved prediction of the response if the relationship is different in different groups.</p>
<h3>9.1.4 &nbsp; Parallel regression lines</h3>
<p>If regression lines for the different groups are parallel, it is easy to summarise the group differences numerically and interpret these differences.</p>
<h3>9.1.5 &nbsp; Transformed variables and groups ((advanced))</h3>
<p>Transformations may linearise the relationship between the response and explanatory variables in each group and also give parallel regression lines.</p>
<h3>9.1.6 &nbsp; Grouping with a numerical variable ((optional))</h3>
<p>A numerical variable can be used to split the individuals into groups.</p>
<h3>9.1.7 &nbsp; Scatterplot matrix with groups</h3>
<p>Groups can also be represented with different symbols or colours on a scatterplot matrix that describes the relationships between 3 or more other variables.</p>
<h2>9.2 &nbsp; Multiple regression</h2>
<h3>9.2.1 &nbsp; More than one explanatory variable</h3>
<p>In many data sets, two or more explanatory variables could potentially affect the response. Using two or more explanatory variables may give more accurate predictions.</p>
<h3>9.2.2 &nbsp; Multiple regression equation</h3>
<p>A simple linear model with a single explanatory variable can be extended with extra terms to explain the additional effect of other explanatory variables.</p>
<h3>9.2.3 &nbsp; Interpreting coefficients</h3>
<p>The slope coefficient associated with an explanatory variable describes its effect if all other variables are held constant. It may have a different sign from the correlation coefficient between the variable and the response.</p>
<h3>9.2.4 &nbsp; Least squares estimation</h3>
<p>An objective estimation method is to minimise the sum of squared residuals -- the principle of least squares.</p>
<h2>9.3 &nbsp; All-or-nothing anova (Optional)</h2>
<h3>9.3.1 &nbsp; Components of variation for Y vs X and Z</h3>
<p>The difference between each response value and the overall mean can be split into a component explained by the explanatory variables and a residual.</p>
<h3>9.3.2 &nbsp; Sums of squares for Y vs X and Z</h3>
<p>The total, regression and residual sum of squares contain information about how well the explanatory variables explain variability in the response. The coefficient of determination is a useful summary statistic.</p>
<h3>9.3.3 &nbsp; F test for regression of Y vs X and Z</h3>
<p>The ratio of the mean regression and residual sums of squares has an F distribution if the response is unrelated to the explanatory variables but is larger if they are related. It can be used as a test statistic for whether there is a relationship.</p>
<h3>9.3.4 &nbsp; All-or-nothing F test for any GLM</h3>
<p>A similar F test can simultaneously test whether all slope parameters in a GLM are zero.</p>
<h3>9.3.5 &nbsp; Different interpretations of R-sqr and F</h3>
<p>The coefficient of determination, R-sqr, describes the proportion of response variation that is explained by the model. The F ratio describes the strength of evidence for there being any relationship at all. In large samples, R-sqr can be small even when F is large.</p>
<h2>9.4 &nbsp; Sequential sums of squares (Optional)</h2>
<h3>9.4.1 &nbsp; Sequentially adding variables</h3>
<p>As explanatory variables are added to the model, the regression model gets closer to the data points.
 
This section also shows how the ANOVA table extends to the more complex situations discussed in the previous topics.  It is taken from a different book in CAST and therefore does not have the associated videos. </p>
<h3>9.4.2 &nbsp; Splitting the explained sum of squares</h3>
<p>Each additional variable reduces the residual sum of squares by an amount that is the sum of squares of differences between the least squares fits of the two models.</p>
<h3>9.4.3 &nbsp; Order of adding X and Z</h3>
<p>The explained sum of squares for X can be different, depending on whether Z is already in the model.</p>
<h3>9.4.4 &nbsp; Anova tests for individual variables</h3>
<p>There are two ways to split the total sum of squares in an anova table. The F-test for the final variable added to the model gives identical results to the t-test for the coefficient in the full model.</p>
<h3>9.4.5 &nbsp; Orthogonal variables</h3>
<p>When the two explanatory variables are uncorrelated (orthogonal), the results are easier to interpret. The slope coefficients for X are the same, whether or not Z is in the model, and the two anova tables are identical.</p>
<h3>9.4.6 &nbsp; Orthogonal variables and experimental design</h3>
<p>Orthogonal variables usually only arise from designed experiments. They result in the most accurate parameter estimates and results that are relatively easy to interpret.</p>
<h3>9.4.7 &nbsp; Other sequences of models</h3>
<p>For any sequence of models with increasing complexity, component sums of squares can be defined that compare successive models in the sequence.</p>
<h2>9.5 &nbsp; Logistic regression</h2>
<h3>9.5.1 &nbsp; Categorical responses</h3>
<p>With a categorical response and numerical explanatory variable, stacked bar charts at each X are an effective display.</p>
<h3>9.5.2 &nbsp; Fitted values and predictions</h3>
<p>Using a straight line to describe how the proportion in a category depends on X is not appropriate. A curve is required.</p>
<h3>9.5.3 &nbsp; Logistic curve</h3>
<p>A 'logistic' curve can be used to model how a proportion depends on X.</p>
<h3>9.5.4 &nbsp; Obtaining a good fit</h3>
<p>A logistic curve is fitted to an example data set.</p>
</body>
</html>
