<html>
<head>
<title>8. Simple Linear Regression</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 8 &nbsp; Simple Linear Regression</h1>
<h2>8.1 &nbsp; Leverage, outliers and influence</h2>
<h3>8.1.1 &nbsp; Leverage</h3>
<p>The most effective x-value at which to take a new response observation is one where predictions are most variable. The variance of predictions at x, divided by sigma-squared is called the leverage at x.</p>
<h3>8.1.2 &nbsp; Outliers and leverage</h3>
<p>If an outlier is also a high-leverage point, it can badly 'pull' the least squares line and the resulting residual often does not indicate that it is an outlier.</p>
<h3>8.1.3 &nbsp; Variances of the residuals</h3>
<p>Even when all data points come from a normal linear model, all residuals do not have the same standard deviation.</p>
<h3>8.1.4 &nbsp; Standardised residuals</h3>
<p>Dividing the residuals by an estimate of their standard deviation gives values that can be compared to ±2 and ±3 to look for outlliers.</p>
<h3>8.1.5 &nbsp; Deleted residuals</h3>
<p>Standardised residuals still do not show up outliers that are high leverage points. Deleted residuals are based on the difference between the response and the prediction from the data without that observation.</p>
<h3>8.1.6 &nbsp; Externally studentised residuals</h3>
<p>Rather than standardising each residual by dividing by its standard deviation based on the mean squared residual for the whole data set, it is better to standardise with the mean squared residual from the data set without that value.</p>
<h3>8.1.7 &nbsp; Influence on fitted values</h3>
<p>Leverage describes the potential of each point to influence the results. DFITS describes its actual influence on the fitted values.</p>
<h3>8.1.8 &nbsp; Influence on regression coefficients</h3>
<p>An alternative measure of influence describes the influence of each point on the least squares coefficients.</p>
<h3>8.1.9 &nbsp; Summary and examples</h3>
<p>This page summarises the various measures of residual and influence and gives a few examples where residuals, leverage and influence are interpreted.</p>
<h2>8.2 &nbsp; Logistic regression</h2>
<h3>8.2.1 &nbsp; Categorical responses</h3>
<p>With a categorical response and numerical explanatory variable, stacked bar charts at each X are an effective display.</p>
<h3>8.2.2 &nbsp; Fitted values and predictions</h3>
<p>Using a straight line to describe how the proportion in a category depends on X is not appropriate. A curve is required.</p>
<h3>8.2.3 &nbsp; Logistic curve</h3>
<p>A 'logistic' curve can be used to model how a proportion depends on X.</p>
<h3>8.2.4 &nbsp; Obtaining a good fit</h3>
<p>A logistic curve is fitted to an example data set.</p>
</html>
