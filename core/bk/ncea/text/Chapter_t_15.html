<html>
<head>
<title>15. Bivariate Data (3.9)</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 15 &nbsp; Bivariate Data (3.9)</h1>
<h2>15.1 &nbsp; Correlation</h2>
<h3>15.1.1 &nbsp; Correlation coefficient and its properties</h3>
<p>The correlation coefficient summarises the strength of the relationship between X and Y. It is +1 when the scatterplot crosses are on a straight line with positive slope, -1 when on a line with negative slope, and zero when X and Y are unrelated.</p>
<h3>15.1.2 &nbsp; Nonlinear relationships</h3>
<p>The correlation coefficient is only a good measure of the strength of a relationship if the points in a scatterplot are scattered round a straight line, not a curve.</p>
<h3>15.1.3 &nbsp; R does not tell the whole story</h3>
<p>The correlation coefficient cannot identify curvature, outliers or clusters and can be misleading if these features are present. A scatterplot must always be examined too.</p>
<h3>15.1.4 &nbsp; Exercise: Estimate r by eye</h3>
<p>In this exercise, you will make a rough estimate of correlation coefficient by eye from a scatterplot.</p>
<h3>15.1.5 &nbsp; Exercise: r for non-standard data</h3>
<p>This exercise presents four scatterplots (one elliptical, one with an outlier, one with two clusters and one with curvature) and asks for these to be matched with the corresponding values of r.</p>
<h2>15.2 &nbsp; Least squares</h2>
<h3>15.2.1 &nbsp; Predicting Y from X</h3>
<p>A line or curve is useful for predicting the value of Y from a known value of X.</p>
<h3>15.2.2 &nbsp; Linear models</h3>
<p>A straight line can often be used to predict one variable from another.</p>
<h3>15.2.3 &nbsp; Fitted values and residuals</h3>
<p>The difference between the actual value of Y and the value predicted by a line is called a residual. Small residuals are clearly desirable.</p>
<h3>15.2.4 &nbsp; Least squares</h3>
<p>The sum of squared residuals describes the accuracy of predictions from a line. The method of least squares positions the line to minimise the sum of squared residuals.</p>
<h3>15.2.5 &nbsp; Curvature and outliers</h3>
<p>A linear model is not appropriate if there are either curvature or outliers in a scatterplot of the data. Outliers should be carefully examined.</p>
<h3>15.2.6 &nbsp; Residual plots</h3>
<p>Outliers and curvature in the relationship are often displayed more clearly in a plot of residuals.</p>
<h3>15.2.7 &nbsp; Predicting Y and predicting X</h3>
<p>Least squares does not treat Y and X symmetrically. The best line for predicting Y from X is different from the best line for predicting X from Y.</p>
<h3>15.2.8 &nbsp; Exercise: Pick the explanatory variable and response</h3>
<p>For several scenarios, you must identify the explanatory variable and response, then state whether the data are observational or experimental and whether the relationship is causal.</p>
<h3>15.2.9 &nbsp; Exercise: Draw a straight line</h3>
<p>This exercise shows the equation of a straight line and asks you to sketch it.</p>
<h3>15.2.10 &nbsp; Exercise: Find the slope and intercept</h3>
<p>The exercises on this page do the inverse of the previous exercise -- you are shown a straight line and asked to find its equation.</p>
<h3>15.2.11 &nbsp; Exercise: Interpret the slope and intercept</h3>
<p>In this exercise, you are asked to select one of four statements that correctly describes the slope or intercept of a least squares line in the context of the data.</p>
<h3>15.2.12 &nbsp; Exercise: Find a residual</h3>
<p>This exercise requests the least squares residual for a cross on a scatterplot.</p>
<h3>15.2.13 &nbsp; Exercise: Match data and residual plot</h3>
<p>In this exercise, you must identify which of four scatterplots is the correct residual plot when a linear model is fitted to a data set.</p>
<h3>15.2.14 &nbsp; Exercise: Predictions</h3>
<p>The exercise on this page gives a least squares line and asks for a prediction of the response, given the value of the explanatory variable.</p>
<h2>15.3 &nbsp; Coefficient of determination</h2>
<h3>15.3.1 &nbsp; Sums of squares</h3>
<p>Variability is often described in terms of sums of squares. The residual sum of squares summarises response variability that is unexplained by the explanatory variable. Sums of squares also describe total and explained variation.</p>
<h3>15.3.2 &nbsp; Coefficient of determination</h3>
<p>The relative sizes of the explained and residual sums of squares holds information about the strength of the relationship. The coefficient of determination describes the proportion of total variation that is explained.</p>
<h2>15.4 &nbsp; Nonlinear relationships (advanced)</h2>
<h3>15.4.1 &nbsp; Transformations and correlation</h3>
<p>The correlation coefficient does not adequately describe the strength of a nonlinear relationship. Transforming the variables to linearise the relationship helps.</p>
<h3>15.4.2 &nbsp; Transformations and models</h3>
<p>If a relationship is nonlinear, a linear model can often be fitted to transformed response or explanatory variables.</p>
<h3>15.4.3 &nbsp; Quadratic models</h3>
<p>An alternative solution to nonlinearity is to fit a quadratic curve the data, again using the principle of least squares.</p>
<h3>15.4.4 &nbsp; Dangers of extrapolation</h3>
<p>Since the form of a relationship is unknown beyond the range of x-values in the data, it is always dangerous to extrapolate.</p>
<h3>15.4.5 &nbsp; Exercise: Regression problems</h3>
<p>This exercise asks you to identify the difficulties with using the least squares line to predict Y at a given X from the data in a displayed scatterplot (an outlier, curvature, a high-leverage point or extrapolation).</p>
<h3>15.4.6 &nbsp; Exercise: Transformations of X and Y</h3>
<p>You are asked whether a logarithmic transformation of X or Y might linearise the data in a scatterplot (and also give constant variability).</p>
<h3>15.4.7 &nbsp; Exercise: Predictions and nonlinearity</h3>
<p>In this exercise, a least squares line is fitted to a model that involves log(X) and/or log(Y). You are asked to use the equation of the line to predict Y from X.</p>
</html>
