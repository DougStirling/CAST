<!DOCTYPE HTML>
<html>
<head>
  <title>8. Non-Normal Data</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 8 &nbsp; Non-Normal Data</h1>
<h1 class="sectionName">8.1 &nbsp; Distribution of sample mean</h1>
<h2 class="pageName">8.1.1 &nbsp; Normal distributions</h2>

<p class="heading">Shape of a probability density function</p>
	<p>A probability density function  is usually a fairly smooth curve, though a single sample histogram  provides limited information about its likely shape.</p>
	<p class=eqn><img src="../../../en/probDensity/images/unknownPopn.gif" alt="sample &mdash;> popn?" width="362" height="236" class="summaryPict"> </p>
	<p class="heading">Normal distributions</p>
	<p>One  flexible group of  continuous probability density functions is the family of <strong>normal 
		distributions</strong>. Normal distributions:</p>
	<ul>
		<li>Are symmetric.</li>
		<li>Have a parameter called µ that is the mean of the population.</li>
		<li> Have a parameter σ that is the population's standard deviation.
</li>
	</ul>
	<p>Changing the parameters µ and σ changes where the distribution is centred and its spread, but its shape remains otherwise the same.</p>
	<p>The parameters are often <strong>estimated</strong> from a sample. Details will be given later, but the resulting normal pdf will be close in shape to a histogram of the sample data.</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_bestFit.gif" width="404" height="202"></p>




<h2 class="pageName">8.1.2 &nbsp; Means from normal populations</h2>

<p class="heading">Shape of the mean's distribution</p>
	<p><strong>Whatever</strong> the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
	<p>However skewness in the population distribution 
		leads to some <strong>skewness in the distribution of the mean</strong>.</p>
	<p class="heading">Samples from normal populations</p>
	<p>When the population distribution is normal, the sample mean 
	also has a normal distribution.</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-12px"><img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (μ , &nbsp;</span><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"><span style="position:relative; top:-12px">)</span></span></p>
	<br>
	<p class=eqn><img src="../../../en/randomMean/images/s_normalMean.gif" width="428" height="344"></p>




<h2 class="pageName">8.1.3 &nbsp; Large-sample normality of means</h2>

<p class="heading">Means from non-normal populations</p>
	<p>Irrespective of the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
<p>If the population is not a normal distribution, the sample mean does not 
		have a normal distribution. However  the <strong>Central Limit 
		Theorem</strong> states that...</p>
<div class="centred"><div class="boxed"><p>For most non-normal population distributions, the distribution of 
				the sample mean becomes close to normal when the sample size increases.</p></div></div>
<p class="eqn"><img src="../../../en/randomMean/images/s_gammaMean.gif" width="428" height="344"></p>




<h2 class="pageName">8.1.4 &nbsp; Distribution of mean from a sample</h2>

<p class="heading">Need for multiple values to assess variability</p>
	<p>We usually need to make two or more measurements of a variable 
		to get any information about its variability. A single value contains no information about the quantity's variability.</p>
<p class="heading">Achieving the impossible?</p>
<p>Fortunately, <strong>we do not need multiple sample</strong> means to assess the variability of a sample mean. Its distribution can be estimated from a <strong>single sample</strong> using </p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> </p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>
	<p>The distribution of the mean can be approximated with a normal distribution with this mean and standard deviation, if we replace µ 
	and σ with <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"> and <em>s</em>.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/timeOnHold.gif" width="540" height="469" class="summaryPict"></p>




<h2 class="pageName">8.1.5 &nbsp; Requirement of independence</h2>

<p class="heading">Independent random samples</p>
	<p>The formula for the standard deviation of a sample mean,</p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
<p>is only accurate if the sample values are <strong>independent</strong>.</p>
<p class="heading">Dependent random samples</p>
	<p>When  sample values are correlated with each other, they are said to be <strong>dependent</strong> and the formula</p>
	<p class="eqn"><img class="gif" src="../../../en/randomMean/images/estSDMean.gif" id="gif_image_1_5_1" width="64" height="32"><iframe class="svg" src="../../../en/randomMean/images/estSDMean.svg" id="svg_image_1_5_1" width="64" height="32" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_1_5_1");</script></p>
	<p> can badly underestimate the variability (and hence accuracy) of the sample 
		mean of dependent random samples.</p>
	<p>Always check that a random sample is independently selected from the whole 
		population before using the formula for the standard deviation of the sample mean.</p>




<h2 class="pageName">8.1.6 &nbsp; Normal probability plots</h2>

<p class="heading">Do the data come from a normal distribution?</p>
	<p>A histogram  may indicate that a sample is unlikely to come from a normal distribution, but a <strong>normal probability plot</strong> can indicate more subtle departures from a normal distribution.</p>
	<ol>
		<li>Sort the data values into order, <em>x</em><sub>(1)</sub>&nbsp;&lt;&nbsp;<em>x</em><sub>(2)</sub>&nbsp;&lt;&nbsp;...&nbsp;&lt;&nbsp;<em>x</em><sub>(<em>n</em>)</sub></li>
		<li>Find ordered values that are spaced out as you would <strong>expect</strong> from a normal distribution, <em>q</em><sub>1</sub>&nbsp;&lt;&nbsp;<em>q</em><sub>2</sub>&nbsp;&lt;&nbsp;...&nbsp;&lt;&nbsp;<em>q</em><sub><em>n</em></sub>. 
			The quantiles of the normal distribution corresponding to probabilities <sup>1</sup>/<sub>(<em>n</em>+1)</sub>, <sup>2</sup>/<sub>(<em>n</em>+1)</sub>, ..., <sup><em>n</em></sup>/<sub>(<em>n</em>+1)</sub> are commonly used.</li>
		<li>Plot <em>x</em><sub>(<em>i</em>)</sub> against <em>q</em><sub><em>i</em></sub></li>
	</ol>
	<p>If the data set is from a normal distribution, the data should be spaced out in a similar way to the normal quantiles, so the crosses in the normal probability 
		plot should lie close to a straight line.</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_probPlot.gif" width="359" height="357"></p>
	<p class="heading">How much curvature is needed to suggest non-normality?</p>
	<p>This is a difficult question to answer and we will not address it here.</p>





<h1 class="sectionName breakBefore">8.2 &nbsp; Discrete data (counts)</h1>
<h2 class="pageName">8.2.1 &nbsp; Discrete and continuous data</h2>

<p>It is important to distinguish  two types of numerical data. </p>
<dl>
	<dt>Discrete data</dt>
	<dd>When the values in the batch are whole numbers (<strong>counts</strong>), 
		the data set is called <strong>discrete</strong>.		</dd>
</dl>
<dl>
	<dt>Continuous data</dt>
	<dd>When the data are not constrained to be whole numbers, the data set is 
		called <strong>continuous</strong>.		</dd>
</dl>
<p class=heading>Dot plots for counts</p>
<p>Dot plots can be used to display count data. However since discrete values
are often repeated several times in a data set, the crosses need to be jittered
or, preferably, stacked.</p>
<p class="eqn"><img src="../../../en/counts/images/s_stackedDots.gif" width="499" height="102"></p>
<p>If there is a stack for each integer value, the stacked dot plot is a complete representation of the data.</p>




<h2 class="pageName">8.2.2 &nbsp; Histograms for counts</h2>

<p class="heading notPrinted">Displaying moderate or large counts</p>
	<p>For discrete data sets whose values are large counts, a histogram can be
used to give a 'smooth' summary of the shape of the distribution of values.</p>
	<p>If the counts are a bit smaller, the exact definition of the histogram classes becomes important. The class boundaries should end in '.5' to ensure that data values 
		do not occur on the boundary of two classes.</p>
	<p class="eqn"><img src="../../../en/counts/images/s_histo.gif" width="503" height="309"></p>




<h2 class="pageName">8.2.3 &nbsp; Bar charts</h2>

<p class="heading">Displaying small counts</p>
	<p>When the range of values in a discrete data set is small, a histogram can be drawn with class width 1 (and with class boundaries ending in '.5'). These classes are <strong>centred 
		</strong>on  1, 2, 3, etc. </p>
	<p>This can be improved by narrowing the histogram rectangles into bars to emphasise
the discrete nature of the data. This is called a <strong> bar chart</strong>.
<p class="eqn"><img src="../../../en/counts/images/s_barchart.gif" width="451" height="189"></p>




<h1 class="sectionName breakBefore">8.3 &nbsp; Distribution of sample proportion</h1>
<h2 class="pageName">8.3.1 &nbsp; Estimating means and proportions (also in Chapter/Topic 3)</h2>

<p class=heading>Estimating means and proportions</p>
	<p>A random sample is often used to <strong>estimate</strong> some  numerical characteristic of the population, such as...</p>
	<ul>
		<li>The mean of some variable</li>
		<li>The proportion in some category</li>
	</ul>
	<p>The difference between an estimate and the population value being estimated is called 
		its <strong>sampling error</strong>. </p>
	<p class="eqn"><img src="../../../en/popSamp/images/s_propnMean.gif" width="494" height="394"></p>




<h2 class="pageName">8.3.2 &nbsp; Proportion and probability</h2>

<p class="heading">A sample proportion has a distribution</p>
	<p>If a categorical data set is modelled as a random sample from a categorical 
		population, the sample proportions   must be treated as 
		random quantities &mdash; they vary from sample to sample.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_samplingPropns.gif" width="446" height="365"></p>
	<p>The population proportion in a category  is called 
		its <strong>probability</strong>, and is often denoted by π. 
		The corresponding sample proportion is usually denoted by <em>p</em>. </p>
	<div class="centred"><table class="centred" border="0" cellspacing="0" cellpadding="4">
<tr>
<th>&nbsp; </th>
<th align="CENTER">Sample Statistic</th>
<th align="CENTER">Population Parameter</th>
</tr>
<tr>
<th align="left">Mean</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top"><img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top">µ</td>
</tr>
<tr>
<th align="left">Standard deviation</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top"><em>s</em></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top">σ</td>
</tr>
<tr>
<th align="left">Proportion/probability</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top bottom"><em>p</em></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top bottom">π</td>
</tr>
</table></div>
<p>In practice, we only have a single sample and must use it to get information about the underlying population.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_onePropn.gif" width="442" height="140"></p>




<h2 class="pageName">8.3.3 &nbsp; Properties of counts and proportions</h2>

<p class="heading">Properties of a sample proportion</p>
	<p>A sample proportion  from a random sample
	of size <i>n</i> has a distribution that ... </p>
	<ul>
		<li>is centred on the underlying population probability,
			π, and</li>
		<li>has a spread that decreases as the sample size <i>n</i> increases.</li>
	</ul>
	<p class="heading">Count and proportion of successes</p>
	<p>Although the sample proportion in a category, <span class="em black">p</span> , 
		is a good summary statistic, the raw count of sample values in the category, <span class="em black">x</span> <span class="black">&nbsp;=&nbsp;<em>np</em></span>, 
		contains equivalent information and is often easier to use. They have distributions with the same shape (other than the scaling constant <i><span class="black">n</span></i>).</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_propnDist.gif" width="550" height="286"></p>




<h2 class="pageName">8.3.4 &nbsp; Binomial distribution</h2>

<p class="heading">General notation</p>
	<p>In a categorical population, we choose one category of interest and call it <em><strong>success</strong></em>; all other categories are collectively called <em><strong>failures</strong></em>. The population proportion of successes is denoted by <span class="black">π</span>.</p>
	<p>When a random sample of <i><span class="black">n</span></i> values is selected, 
		we denote the number of successes by <span class="em black">x</span>  and the proportion of successes by <span class="em black">p</span> <span class="black">&nbsp;=&nbsp;<em><span style="vertical-align:20%">x</span></em>/<em><span style="vertical-align:-20%">n</span></em></span>. </p>
	<p class="heading">Distribution of a sample proportion</p>
	<p>The number of successes, <span class="em black">x</span> , 
		has a 'standard' discrete distribution called a <strong>binomial distribution</strong> which has two parameters, <span class="em black">n</span>  and <span class="black">π</span>.</p>
	<p>In practical applications, <span class="em black">n</span>  is a known constant, but <span class="black">π</span> may be unknown. The sample proportion, <span class="em black">p</span> , 
		has a distribution with the same shape, but is scaled by <span class="em black">n</span> .</p>
	<ul>
		<li>The distribution of <em>p</em> is centred on π.</li>
		<li>The spread of the distribution of <em>p</em> decreases as <em>n</em> increases.</li>
		<li>The distribution is symmetric when π 
			= 0.5, but becomes more skew as π 
			approaches 0 or 1.</li>
	</ul>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_binomials.gif" width="466" height="515"></p>




<h2 class="pageName">8.3.5 &nbsp; Binomial probability examples</h2>

<p class="heading">Assumptions underlying the binomial distribution</p>
	<ul>
		<li>Each observation has the same probability, <span class="black">π</span>, 
			of being a 'success'.</li>
		<li>Each of the <span class="em black">n</span>  observations is independently obtained.</li>
		<li>We record the number (or proportion) of successes in the <span class="em black">n</span>  observations.</li>
	</ul>
	<p class="heading">Evaluating binomial probabilities</p>
	<p>They may be obtained using ... </p>
	<ul>
		<li>a computer (preferred),</li>
		<li>a mathematical formula, or</li>
		<li>tables of binomial probabilites.</li>
	</ul>
	<p class="heading">A range of counts</p>
	<p>Finding the probability that the number of successes is within an interval involves adding the binomial probabilities for all integer values in the interval.</p>
	<p>Think carefully about the wording of the interval &mdash; does it include the values at the end? Adding or subtracting <sup>1</sup>/<sub>2</sub> to the endpoints of the interval makes it clearer. (This is also particularly useful when using the normal approximations that are described in the following pages.)</p>
	<div class="centred"><table border="0" class="centred" cellpadding="3" cellspacing="0">
<tr>
<th align="center" scope="col">In words...</th>
<th align="center" scope="col">&nbsp;&nbsp;&nbsp;&nbsp;In terms of X&nbsp;&nbsp;&nbsp;&nbsp;</th>
<th align="center" scope="col">&nbsp;&nbsp;&nbsp;&nbsp;Using <sup>1</sup>/<sub>2</sub>&nbsp;&nbsp;&nbsp;&nbsp;</th>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF" class="top">More than 5</td>
<td align="center" bgcolor="#FFFFFF" class="top">X &gt; 5</td>
<td align="center" bgcolor="#FFFFFF" class="top">X&nbsp;&gt;&nbsp;5.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">Greater than or equal to 5</td>
<td align="center" bgcolor="#FFFFFF">X ≥ 5</td>
<td align="center" bgcolor="#FFFFFF">X &gt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">No more than 5</td>
<td align="center" bgcolor="#FFFFFF">X ≤ 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 5.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">At least 5</td>
<td align="center" bgcolor="#FFFFFF">X ≥ 5</td>
<td align="center" bgcolor="#FFFFFF">X &gt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">Fewer than 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF" class="bottom">5 or fewer</td>
<td align="center" bgcolor="#FFFFFF" class="bottom">X ≤ 5</td>
<td align="center" bgcolor="#FFFFFF" class="bottom">X &lt; 5.5</td>
</tr>
</table></div>
<p>The following example illustrates the use of <sup>1</sup>/<sub>2</sub> in this way.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_binomExample_c.gif" width="550" height="272"></p>





<h2 class="pageName">8.3.6 &nbsp; Normal approximation to binomial</h2>

<p class="heading">Mean and standard deviation of <em>x</em> and <em>p</em></p>
	<p>The mean and standard deviation are given below for the proportion of successes <span class="em black">p</span> , and number of successes,<span class="em black"> x</span> <span class="black">&nbsp;=&nbsp;<em>np</em></span></p>
	<p class=eqn><img class="gif" src="../../../en/randomPropn/images/normalApprox.gif" id="gif_image_3_6_1" width="389" height="126"><iframe class="svg" src="../../../en/randomPropn/images/normalApprox.svg" id="svg_image_3_6_1" width="389" height="126" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_3_6_1");</script> </p>
	<p>The fact that both <span class="em black">x</span>  and <span class="em black">p</span>  are approximately normally distributed in large samples is justified below.</p>
	<p class="heading">Proportions and means</p>
	<p>If we assign a code of '1' to the successes and '0' to the failures in the 
		random sample, then the resulting values are called an <strong>indicator variable</strong>. Its mean is identical to the proportion of successes. </p>
	<p class=eqn><img class="gif" src="../../../en/randomPropn/images/propnAsMeanEqn.gif" id="gif_image_3_6_2" width="397" height="100"><iframe class="svg" src="../../../en/randomPropn/images/propnAsMeanEqn.svg" id="svg_image_3_6_2" width="397" height="100" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_3_6_2");</script> </p>
	<p>Since the proportion of successes in a sample is a kind of mean, its distribution is close to a normal distribution if the sample size is large enough.<br>
	</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_normalApprox.gif" width="502" height="375"></p>




<h2 class="pageName">8.3.7 &nbsp; Normal approximation examples</h2>

<p class="heading notPrinted">Use of the normal approximation to the binomial distribution</p>
	<p>To avoid adding large numbers of binomial probabilities, the normal approximation can be used to find the probability that a binomial variable is within a certain range when the sample size, <span class="em black">n</span> , is large.</p>
	<p>A common rule-of-thumb for when this kind of normal approximation can be used is:</p>
	
	<div class="centred"><div class="boxed percent50">
		<p><em>n</em>π &gt; 5 &nbsp; &nbsp;and &nbsp; &nbsp; <em>n</em>(1-π) &gt; 5 </p>
	</div></div>
	
	<p>An example is given below:</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_normalApproxEx_c.gif" width="550" height="460"></p>
	<p>Note the translation of the range of values into one involving <sup>1</sup>/<sub>2</sub>. It is called a <strong>continuity correction</strong> in this context.</p>





<h1 class="sectionName breakBefore">8.4 &nbsp; Estimating proportions</h1>
<h2 class="pageName">8.4.1 &nbsp; General framework for estimation</h2>

<p class="heading notPrinted">General framework</p>
<p>A similar approach to that for estimating a population mean is used for estimating population proportions and other parameters.</p>
<dl>
		<dt>Point estimate</dt>
		<dd>A population parameter is usually estimated with the corresponding sample statistic.</dd>
		<dt>Error distribution</dt>
		<dd>The  
			estimation error has a distribution.</dd>
		<dt>Standard error and bias</dt>
		<dd>A good estimator usually has an error distribution whose mean is zero 
			(unbiased). The standard deviation of the error distribution is called the 
			standard error.</dd>
		<dt>From data:</dt>
		<dd>We can usually find a numerical value for the standard error (or an approximation 
			to it).</dd>
		<dt>Confidence interval</dt>
		<dd>A 95% confidence interval can be found from </dd>
	</dl>
	<p class="eqn black"><em>estimate</em> - 2 <em>s.e.&nbsp; </em>&nbsp;to<em> &nbsp;&nbsp;estimate</em> + 2 <em>s.e.</em></p>
	<dl>
		<dd>(The interval can sometimes be improved by replacing 2 by another similar 
			value. )</dd>
	</dl>

  


<h2 class="pageName">8.4.2 &nbsp; Estimating a proportion</h2>

<p class="heading">Population proportions and probabilities</p>
	<p>Categorical data are usually treated as a random sample from some  population. We concentrate on a single category which we will call <strong>success</strong> and we collectively call the other categories <strong>failures</strong>. The 
	population proportion of successes is denoted by π. It is also the <strong>probability</strong> that a single randomly selected value from the population is a success.</p>
	<p>We are interested in estimating an underlying probability, π. Although it is more general to treat π as a probability, it is  usually easier to interpret π as a 'population proportion'.</p>
	<p class="heading">Parameter estimate and error</p>
	<p>The 
		sample proportion of successes is denoted by <span class="em black">p</span>  and is an estimate of  <span class="black">π</span>.</p>
	<p class="eqn"><img class="gif" src="../../../en/estPropn/images/pEqn.gif" id="gif_image_4_2_1" width="246" height="35"><iframe class="svg" src="../../../en/estPropn/images/pEqn.svg" id="svg_image_4_2_1" width="246" height="35" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_4_2_1");</script></p>
	<p>Since <span class="em black">p</span>  is based on a random sample, it varies from sample to sample and has a distribution. The estimation error is:</p>
	<p class="eqn black"><em>error</em> = &nbsp; <em>p</em> - π</p>
	<p>Since <span class="black">π</span> is unknown, we never know the value of the error, but we can find its approximate <strong>distribution</strong>. This will allow us to assess the likely size of the error.</p>
	
	


<h2 class="pageName">8.4.3 &nbsp; Error distribution</h2>

<p class="heading">Distribution of proportion</p>
	<p>The <strong> proportion</strong> of successes 
		from a random sample with probability <span class="black">π</span> of success, <span class="em black">p</span> , 
		has a distribution with  mean and standard deviation</p>
	<p class=eqn><span class="black">&mu;<sub><em>p</em></sub> &nbsp;=&nbsp; &pi;</span> </p>
	<p class=eqn><span class="black"><span style="position:relative; top:-14px">&sigma;<sub><em>p</em></sub>&nbsp; =&nbsp; </span><img src="../../../en/../images/symbol.sePi.png" width="89" height="46"></span> </p>
	<p class="heading">Distribution of estimation error</p>
	<p>The estimation error is <span class="black"><em>p</em>&nbsp;-&nbsp;π</span> and its distribution  has the same shape as that of<em> <span class="black">p</span></em>, 
		but is shifted to have mean zero. The bias and standard error  are therefore</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-14px"><em>bias</em>&nbsp; =&nbsp; &mu;<sub>error</sub>&nbsp; =&nbsp; 0<br><em>standard error</em>&nbsp; =&nbsp; &sigma;<sub>error</sub>&nbsp; =&nbsp; </span><img src="../../../en/../images/symbol.sePi.png" width="89" height="46"></span></p>
	<p class="heading">Standard error from data</p>
	<p>Unfortunately, the formula for the standard error of <span class="em black">p </span>  involves <span class="black">π</span>, 
		and this is unknown in practical problems. To get a numerical value for the 
		standard error, we  therefore replace <span class="black">π</span> with our best estimate of its value, <span class="em black">p</span> .</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-14px"><em>bias</em>&nbsp; =&nbsp; &mu;<sub>error</sub>&nbsp; =&nbsp; 0<br><em>standard error</em>&nbsp; =&nbsp; &sigma;<sub>error</sub>&nbsp; =&nbsp; </span><img src="../../../en/../images/symbol.seP.png" width="80" height="44"></span></p>
	<p class="heading">Example</p>
	<p>In a random sample of <em>n</em> = 36 values, there were <em>x</em> = 17 successes. Our best estimate of <span class="black">π</span> is the sample proportion, <span class="black em">p</span>  = <sup>17</sup>/<sub>36</sub>. Using this estimate, the distribution of the number of successes in similar samples would be</p>
	<p class="eqn">X ~ binomial (<em>n</em> = 36, <span class="black">π</span> = <sup>17</sup>/<sub>36</sub>)</p>
	<p class="eqn"><img src="../../../en/estPropn/images/s_binomEx.gif" width="404" height="147"></p>
	<p>The proportion of successes in similar samples would have a scaled form of this distribution</p>
	<p class="eqn"><img src="../../../en/estPropn/images/s_binomEx2.gif" width="404" height="147"></p>
	<p>and the error distribution would shift this to have mean zero:</p>
	<p class="eqn"><img src="../../../en/estPropn/images/s_binomEx3.gif" width="404" height="147"></p>
	<p>From this error distribution, it is unlikely that our estimate of the proportion of successes (<span class="eqn"><sup>17</sup>/<sub>36</sub></span>) would be in error by more than 0.2.</p>




<h2 class="pageName">8.4.4 &nbsp; Normal approximation to error distribution</h2>

<p class="heading notPrinted">Normal approximation to the error distribution</p>
	<p>If the sample size, <em>n</em>, is large enough, <a href="javascript:showNamedPage('randomPropn5')">the 
		binomial distribution is approximately normal</a>, so we have the approximation</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-14px"><em>error</em> &nbsp;=&nbsp; <em>p</em> &minus; &pi; &nbsp;~&nbsp; <font face="Arial, Helvetica, sans-serif">normal</font> (0, </span><img src="../../../en/../images/symbol.sePi.png" width="89" height="46"><span style="position:relative; top:-14px"> )</span></span></p>
	<p class="heading">Example</p>
	<p align="center"><img src="../../../en/estPropn/images/printErrors.gif" width="550" height="284" class="summaryPict"></p>
	<p>The error distribution (and standard error) give a good indication of how far our point estimate (0.554) will be from the true proportion of adults in the park.</p>




<h2 class="pageName">8.4.5 &nbsp; Confidence interval for proportion</h2>

<p class="heading">95% bounds on the estimation error</p>
	<p>When sample proportion <span class="em black">p</span>  is used to 
		estimate a corresponding population proportion, <span class="black">π</span>, 
		the resulting error has the approximate distribution,</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-14px"><em>error</em> &nbsp;=&nbsp; <em>p</em> &minus; &pi; &nbsp;~&nbsp; <font face="Arial, Helvetica, sans-serif">normal</font> (0, </span><img src="../../../en/../images/symbol.sePi.png" width="89" height="46"><span style="position:relative; top:-14px"> )</span></span> </p>
	<p>Replacing <span class="black">π</span> by our best estimate, <span class="em black">p</span> , and using the 
		properties of the normal distribution,</p>
	<p class=eqn><span class="black">Prob( error is between ± 2<span style="position:relative; top:14px"><img src="../../../en/../images/symbol.seP.png" width="80" height="44" align="baseline"></span> )&nbsp;&nbsp;&asymp;&nbsp;&nbsp;0.95</span></p>
<p class="heading">95% confidence interval</p>
	<p>A <strong>95% confidence interval</strong> for <span class="black">π</span> is therefore...</p>
	<p class=eqn><img class="gif" src="../../../en/estPropn/images/ciP.gif" id="gif_image_4_5_1" width="141" height="46"><iframe class="svg" src="../../../en/estPropn/images/ciP.svg" id="svg_image_4_5_1" width="141" height="46" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_4_5_1");</script></p>
	<p class="heading">Example</p>
	<p>In a random sample of <em>n</em> = 36 values, there were <em>x</em> = 17 successes. We estimate the population proportion, <span class="black">π</span>, with <em>p</em> =<sup>&nbsp;17</sup>/<sub>36</sub>&nbsp;=&nbsp;0.472. The approximate normal distribution for the errors 
	is shown below.</p>
	<p class=eqn> <img src="../../../en/estPropn/images/s_errorBounds.gif" width="277" height="164"> </p>
	<p>A 95% confidence interval for <span class="black">π</span>  is therefore</p>
	<p class=eqn>0.472 ± 0.166</p>
	<p class=eqn>i.e. 0.306 &nbsp; to &nbsp; 0.638</p>
	<p>We are therefore 95% confident that  the population proportion of successes is between 30.6% and 
		63.8%. A sample size of <em>n</em> = 36 is clearly too small to give a very accurate estimate.</p>
	




<h2 class="pageName">8.4.6 &nbsp; Properties of 95% CI for proportion</h2>

<p class="heading notPrinted">Properties</p>
<p>As with all other confidence intervals, a 95% confidence interval for a population proportion, π, is  random. Although it correctly includes π in 95% of random samples, there is a 5% probability that it will not include the true value of π.</p>
<p>The  simulation below took 100 random samples of size <em>n</em> = 200 from a population with π = 0.6. Most of the confidence intervals included π = 0.6, but some did not. If the simulation was repeated many more times, the proportion including 0.6 would be close to 0.95.</p>
<p class="eqn"><img src="../../../en/estPropn/images/s_ciSim.gif" width="550" height="299"></p>
<p>In practice, you only have a single sample and a single confidence interval, but we have &quot;95% confidence&quot; that it will include the true (and usually unknown) value of π.</p>
<p class="heading">Problems with small sample sizes</p>
<p>This confidence interval for π was based on a normal approximation to the distribution of the sample proportion. If the sample size, <em>n</em>, is small or π 
	is close to either 0 or 1, this normal approximation is inaccurate and the confidence level for the interval can be considerably less than 95%.</p>
<p>Many textbooks give the following 
	guideline for using the confidence interval:</p>
<div class="centred">
<table border="1" cellspacing="0" cellpadding="10" style="background-color:#FFFFFF" class="centred">
<tr>
<td align="left"><strong>Only use the confidence interval for π when <span class="red">all</span> of
the following hold...</strong>
<div class="centred">
<table class="centred">
<tr>
<td align="center"><strong>&nbsp;&nbsp;<em>n</em>&nbsp;&nbsp;&gt;&nbsp;&nbsp;30<br>
<em>np</em>&nbsp; &gt;&nbsp; 5<br>
&nbsp;<em>n</em>(1-<em>p</em>)&nbsp; &gt;&nbsp; 5</strong></td>
</tr>
</table>
</div></td>
</tr>
</table>
</div>
<p>These guidelines can be relaxed a little provided you accept that the confidence level may be a little less than 95%.</p>




<h2 class="pageName">8.4.7 &nbsp; Confidence interval examples</h2>

<p class=heading>Interpretation of a confidence interval</p>
	<p>We never know in practice whether or not the confidence interval that we obtain 
		actually includes π.</p>
	
<div class="centred"><div class="boxed">
<p>Being right most of the time is the best one can hope for, 
					since there is always the possibility of being misled by an unlucky sample.</p>
</div></div>

	<p>The <strong>method</strong> that we use to obtain the confidence interval has 
		probability 0.95 of including π. 
		We cannot tell whether the single interval that we evaluate from our data set 
		is one of these 'lucky' intervals, but knowing that the <strong>method</strong> works so often gives us <strong>95% confidence</strong> in this interval.</p>
	<p class=heading>Example</p>
	<p class="eqn"><img src="../../../en/estPropn/images/printErrors2.gif" width="550" height="285" class="summaryPict"></p>




<h1 class="sectionName breakBefore">8.5 &nbsp; Simulation &amp; bootstrap</h1>
<h2 class="pageName">8.5.1 &nbsp; Need for simulation</h2>

<p class=heading>Standard errors and CIs from formulae</p>
	<p>If a formula can be found for the standard error of an estimator, an approximate 95% confidence interval can be found 
	from</p>
	
<div class="centred"><div class="boxed">
<p><span class="bold black em">estimate</em> - 2 <em>s.e.</em><em>&nbsp; </em><strong>&nbsp;to</strong><em> &nbsp;&nbsp;estimate</em> + 2 <em>s.e.</span></p>
</div></div>

	<p>For some estimators, there is no formula for the standard error, so a different approach is needed.</p>
	<p class=heading>Rainfall example</p>
	<p>Understanding of the distribution of rainfall lets farmers make better choices about the crops 
		that are grown and when they are planted, especially in areas prone to drought. A useful summary is the <strong>upper quartile</strong> of the rainfall distribution in a month &mdash; the rainfall that is exceeded in only 1 out of 4 years.</p>
	<p>The diagram below shows October rainfall in Samaru, Nigeria for the 56 years between 1928 
		and 1983.</p>
	<p class=eqn><img class="gif" src="../../../en/estOther/images/samaruOctUQ.gif" id="gif_image_5_1_1" width="397" height="130"><iframe class="svg" src="../../../en/estOther/images/samaruOctUQ.svg" id="svg_image_5_1_1" width="397" height="130" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_1_1");</script></p>
	<p>Assuming that there is no climate change, the sample upper quartile is our best point estimate of the upper quartile for the underlying population distribution, but there is no  convenient formulae for its standard error.</p>




<h2 class="pageName">8.5.2 &nbsp; Error distribution by simulation</h2>

<p class="heading">Standard error of a proportion</p>
	<p>If statistical theory does <strong>not</strong> provide 
		the error distribution for the estimator of interest, 
		a <strong>simulation</strong> can often be used to find properties of the 
		error distribution numerically.</p>
	<p>This methodology is illustrated with a simulation to find the 
		standard error of a sample proportion. Since we already have a formula,</p>
	<p class=eqn><span style="position:relative; top:-14px"><span class="black"><em>standard error</em>&nbsp;&nbsp;=&nbsp;&nbsp;</span></span><img src="../../../en/../images/symbol.seP.png" width="80" height="44" align="baseline"></p>
	<p> a simulation is  unnecessary, but it allows us to simply illustrate the method. </p>
	<p class="heading">Example</p>
	<p>A sample of <em>n</em> = 36 values are selected from a population with probability π of success, so the number of successes will have a binomial distribution,</p>
	<p class=eqn><span class="black"><em>X</em>&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">binomial</font> (<em>n</em> = 36, &nbsp;&pi;)</span></p>
<p>If we knew the value of π, we could take repeated samples from this binomial distribution, find the estimation 
		error, (<em>p</em>&nbsp;-&nbsp;π) 
	for each sample, and build up the error distribution.</p>
<p>If <em>x</em> = 17 successes are observed, our best estimate of π is  <em>p</em> =<sup>&nbsp;17</sup>/<sub>36</sub>&nbsp;=&nbsp;0.472, so we could perform this simulation using <em>p</em> instead of π.</p>
<p class="eqn"><img src="../../../en/estOther/images/s_binomSim.gif" width="339" height="240"></p>
<p>This simulation of 100 samples provides approximations to the bias (-0.004) and standard error (0.080) for this type of estimator. These are  fairly close to the values  from the formulae,</p>
<p class=eqn><span style="position:relative; top:-14px"><em>bias</em>&nbsp; =&nbsp; 0<br><em>standard error</em>&nbsp; =&nbsp; </span><img src="../../../en/../images/symbol.seP.png" width="80" height="44" align="baseline"><span style="position:relative; top:-14px">&nbsp; =&nbsp; 0.0832</span></p>
<p>In practice, the formula would be used for the standard error of a proportion, but we can use simulations for other examples where a formula does not exist.</p>

	


<h2 class="pageName">8.5.3 &nbsp; Simulations with normal distns</h2>

<p class="heading">Normal distribution parameters</p>
	<p>The mean, µ, 
		of normal populations is usually of most interest, but we may also want to estimate:</p>
	<ul>
		<li>standard deviation</li>
		<li>interquartile range (upper quartile - lower quartile)</li>
		<li>median</li>
		<li>other percentiles (e.g. the upper quartile)</li>
	</ul>
	<p>The corresponding sample statistics provide point estimates of these parameters, but formulae for standard errors may be difficult to find.</p>
	<p class="heading">Simulation</p>
	<p>If the values of µ 
	and σ were known, we could perform a simulation with repeated samples to find the error distribution for the type of estimate that we were using. The standard deviation of the error distribution gives the standard error of the estimate.</p>
	<p>In practice, µ 
	and σ are unknown, but we can perform a similar simulation, replacing them with the sample mean and standard deviation.</p>
	<p class=heading>Example</p>
<p>Low assets-to-liabilities ratios are usually regarded as undesirable for companies. We
want to find the assets-to-liabilities ratio that  only one in four
healthy companies will not meet &mdash;
i.e the <strong>lower
quartile</strong> of
the distribution. The diagram below shows the assets-to-liabilities ratios
of a sample of 68 healthy Greek companies and the point estimate of the lower
quartile.</p>
<p class=eqn><img class="gif" src="../../../en/estOther/images/assetLiabilitiesLQ.gif" id="gif_image_5_3_1" width="396" height="136"><iframe class="svg" src="../../../en/estOther/images/assetLiabilitiesLQ.svg" id="svg_image_5_3_1" width="396" height="136" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_3_1");</script></p>
<p>The assets-to-liabilities ratios have a fairly symmetric distribution and
the diagram below shows a normal distribution whose mean and standard deviation
are the same as those of our actual data.</p>
<p class=eqn><img class="gif" src="../../../en/estOther/images/assetLiabilitiesModel.gif" id="gif_image_5_3_2" width="396" height="136"><iframe class="svg" src="../../../en/estOther/images/assetLiabilitiesModel.svg" id="svg_image_5_3_2" width="396" height="136" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_3_2");</script></p>
<p>This approximate normal distribution has lower quartile 1.295, so we can perform
a simulation with samples of <em>n</em> = 68 values from this distribution and
find how far the sample lower quartiles are from this theoretical value &mdash; the
error distribution.</p>
<p class="eqn"><img src="../../../en/estOther/images/s_normalSim_c.gif" width="339" height="240"></p>
	<p>From the 70-95-100 rule-of-thumb, the error has approximately 95% chance of being within 2 s.e. of 
	zero and will be almost certainly within 3 s.e. of zero. This means that  our point estimate of the upper quartile (292 secs) is likely to be less than 9 sec from the underlying population parameter and will be almost certain to be less than 13.5 sec from it.</p>




<h2 class="pageName">8.5.4 &nbsp; Bootstrap error distribution</h2>

<p class="heading notPrinted">Bootstrap simulation</p>
	<p>The simulations can be based on random samples from any approximation to the population distribution. If a normal approximation does not seem reasonable, the actual data can be used as an approximate 'population' and random samples selected <strong>with replacement</strong> from it. Such 
	samples are called <strong>bootstrap samples</strong>.</p>
<p>A simulation with these bootstrap samples can again show the error distribution 
		and provide approximate values for the bias and standard error.</p>
	<p class=heading>Example</p>
	<p>The October rainfall data below is highly skew, so a normal approximation should not be used to generate simulated samples.</p>
	<p class=eqn><img class="gif" src="../../../en/estOther/images/samaruOctUQ.gif" id="gif_image_5_4_1" width="397" height="130"><iframe class="svg" src="../../../en/estOther/images/samaruOctUQ.svg" id="svg_image_5_4_1" width="397" height="130" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_4_1");</script></p>
	<p>A typical bootstrap sample is shown below:</p>
	<p class="eqn"><img src="../../../en/estOther/images/s_bootstrapSamp.gif" width="319" height="155"></p>
	<p>The diagram below shows the 'errors' from 100 of these simulated bootstrap samples &mdash; the differences between the sample upper quartiles and that from the 'population' underlying the simulation, 57.4.</p>
	<p class="eqn"><img src="../../../en/estOther/images/s_bootstrapSim.gif" width="339" height="292"></p>
<p>Using the 70-95-100 rule-of-thumb, our point estimate of the upper quartile, 57.4, is unlikely to be in error by more than about 20.</p>




<h1 class="sectionName breakBefore">8.6 &nbsp; Exercises</h1>
</body>
</html>
