<!DOCTYPE HTML>
<html>
<head>
  <title>3. Making Good Generalisations</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 3 &nbsp; Making Good Generalisations</h1>
<h1 class="sectionName">3.1 &nbsp; Finite populations</h1>
<h2 class="pageName">3.1.1 &nbsp; Census or sample?</h2>

<p class="heading">Population and census</p>
	<p>We often want to find information about a particular group of individuals (people, 
		fields, trees, bottles of beer or some other collection of items). This target 
		group is called the <strong>population</strong>.</p>
	<p>Collecting measurements  from every item in the  population is called a <strong>census</strong>. A census is rarely feasible,  because of the  cost and time involved.</p>
	<p class="heading">Simple random sample</p>
	<p>We can usually obtain sufficiently accurate information by only 
	collecting information from a selection of units from the population &mdash; a <strong>sample</strong>. Although a sample gives less accurate information than a census, the savings in cost and time often outweigh this.</p>
	<p>The simplest way to select a representative sample  is  
		a <strong>simple random sample</strong>. In it, each unit has the same chance 
		of being selected and some random mechanism is used to determine whether any particular unit is 
		included in the sample.	</p>
	<p class="heading">Sampling from a population of <span class="red">values</span></p>
	<p>It is convenient 
		to define the population and sample to be sets of <strong>values</strong> (rather 
		than people or other items). This abstraction &mdash; a population of values and a 
		corresponding sample of values &mdash; can be applied to a wide range of applications.</p>
	<p class="eqn"><img src="../../../en/popSamp/images/s_popSamp.gif" width="550" height="267"></p>




<h2 class="pageName">3.1.2 &nbsp; Variability in a sample</h2>

<p class="heading notPrinted">Variability</p>
	<p>Sampling from a population results in sample-to-sample variability in the information that we obtain from the samples.</p>
	<p class="eqn"><img class="gif" src="../../../en/popSamp/images/samplingHistos.gif" id="gif_image_1_2_1" width="428" height="336"><iframe class="svg" src="../../../en/popSamp/images/samplingHistos.svg" id="svg_image_1_2_1" width="428" height="336" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_1_2_1");</script></p>
	<p class="heading">Sample information about the population</p>
	<p>In practice, we only have a single sample and this provides  <strong>incomplete information about the population</strong>.</p>
	<p class="eqn"><img class="gif" src="../../../en/popSamp/images/samplingHistos2.gif" id="gif_image_1_2_2" width="423" height="120"><iframe class="svg" src="../../../en/popSamp/images/samplingHistos2.svg" id="svg_image_1_2_2" width="423" height="120" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_1_2_2");</script></p>
	<p class="heading">Effect of sample size</p>
	<p>Bigger samples mean more stable and reliable information about the underlying population.</p>




<h2 class="pageName">3.1.3 &nbsp; Sampling error</h2>

<p class=heading>Estimating means and proportions</p>
	<p>A random sample is often used to <strong>estimate</strong> some  numerical characteristic of the population, such as...</p>
	<ul>
		<li>The mean of some variable</li>
		<li>The proportion in some category</li>
	</ul>
	<p>The difference between an estimate and the population value being estimated is called 
		its <strong>sampling error</strong>. </p>
	<p class="eqn"><img src="../../../en/popSamp/images/s_propnMean.gif" width="494" height="394"></p>




<h2 class="pageName">3.1.4 &nbsp; Selecting a random sample</h2>

<p class="heading">Selecting a sample manually (raffle tickets)</p>
	<ol>
		<li>Write the names (or other identification) of all population members on identical 
			pieces of paper,</li>
		<li>Mix them thoroughly in a box</li>
		<li>Select <em>n</em> pieces of paper (with or without replacement).</li>
	</ol>
	<p>This method is  rarely used in research applications.</p>
	<p class="heading">Selecting a  sample with random numbers</p>
	<p>To select a random sample without replacement using random numbers,</p>
	<ol>
		<li>Number all population members, starting from index 0.</li>
		<li>Generate a random index between 0 and the largest population index.</li>
		<li>If sampling <strong>without</strong> replacement and the generated index has already been selected, go back to step 2 and select another index.<br>
		</li>
		<li>Add the selected population member to the sample, then repeat steps 2. and 3. until a large enough sample has been selected.</li>
	</ol>
	<p class="heading">Random number between 0 and <em>k</em></p>
	<p>The easiest way to generate a random number between 0 and 357 is to use a spreadsheet such as Excel &mdash; it has a function designed for this purpose, &quot;=RANDBETWEEN(0, 357)&quot;. A computer-generated random value is strictly called a <em>pseudo-random</em> number.</p>
	<p>If a computer is not available, a sequence of random digits can be generated:</p>
	<ul>
		<li><img src="../../../en/popSamp/images/die.jpg" width="150" height="123" align="right">Roll a 10-sided 
			die several times.</li>
		<li>In a printed book of random digits, start at a random position in a random page, then use 
			a sequence of digits starting from there.</li>
	</ul>
	<p>A random number that is equally likely to have any value between 0 and 357 can be found by repeatedly generating 3-digit numbers (between 0 and 999) until a value between 0 and 357 is obtained.</p>




<h1 class="sectionName breakBefore">3.2 &nbsp; Infinite populations</h1>
<h2 class="pageName">3.2.1 &nbsp; Data as representatives</h2>

<p class="heading notPrinted">Generalising from data</p>
	<p>Most data sets do <strong>not</strong> arise from randomly sampling individuals from a finite population. However we are still rarely interested in the specific individuals from whom data 
		were collected.</p>
	
<div class="centred"><div class="boxed">
<p>The recorded data are often 'representative' of something 
					more general.</p>
</div></div>

	<p>The main aim  is to <strong>generalise from the data</strong>.</p>
	<p class="eqn"><img src="../../../en/infPopn/images/dental.gif" width="559" height="209" class="summaryPict"></p>

  
  


<h2 class="pageName">3.2.2 &nbsp; Randomness of data</h2>

<p class=heading>Randomness of data</p>
	<p>Not only do we usually have little interest in the specific individuals from whom data were collected, but we must also acknowledge that our data would have been different if, by chance, we had selected different individuals or even made our measurements at a different time.</p>
	<p>We must acknowledge 
		this sample-to-sample variability when interpreting the data. The data are <strong>random</strong>.</p>
	
<div class="centred"><div class="boxed">
<p>All graphical and numerical summaries would 
					be different if we repeated data collection.</p>
</div></div>

	<p>This randomness in the data must be taken into account when we interpret graphical and numerical summaries. Our conclusions should not be dependent on features  that are specific to our particular data but would (probably) be different if the data were collected again.</p>
	<p>The more data that we collect, the more accurately our  data will reflect population characteristics, but randomness always exists.</p>




<h2 class="pageName">3.2.3 &nbsp; Model to explain randomness</h2>

<p class=heading>Data that are <span class="red">not</span> sampled from a finite population</p>
	<p>There is   no real finite population underlying most data sets   from which the values can be treated 
		as being sampled. The randomness in such data must be explained in a different way.</p>
	<p class=heading>Sampling from an abstract  population</p>
	<p>&quot;Random sampling from a population&quot; is  also used to explain variability even when there is no <strong>real</strong> finite population from which the data were sampled.</p>
	<p>We imagine an <strong>abstract</strong> population of <strong>all values that might have been obtained</strong> if the data collection had been repeated. We can then treat the observed data as a random sample from this abstract population.</p>
	<p>Defining such an underlying population therefore not only explains sample-to-sample variability but also gives us a focus for generalising from  our specific data.</p>




<h2 class="pageName">3.2.4 &nbsp; Infinite populations (distributions)</h2>

<p class=heading>Distributions</p>
	<p>When an abstract population is imagined to underlie a data set, it  often
contains an infinite number of values. For example, consider the lifetimes of
a sample of light bulbs. The population of <strong>possible</strong> failure
times contains <strong>all</strong> values greater than zero, and this includes
an infinite number of values. Moreover, some of these possible values will be
more likely than others.</p>
	<p>This kind of underlying population is called a <strong>distribution</strong>.</p>
	<p class=heading>Positions of cow in a field</p>
	<p>Consider the positions of a cow in a field at 6 different times where all locations are equally likely.</p>
	<p class="eqn"><img src="../../../en/infPopn/images/s_cows.gif" width="400" height="377"></p>
	<p>The population here contains all possible positions and is therefore infinite.</p>
	<p>The idea of a distribution also allows for some possible values to be more likely than others &mdash; the cow may be more likely to be in some particular part of the field.</p>


  


<h2 class="pageName">3.2.5 &nbsp; Information from a sample</h2>

<p class="heading">Sampling from a population</p>
	<p>Sampling from an underlying population (whether finite or infinite) gives us a mechanism to explain the randomness of data. The underlying population also gives us a <strong>focus</strong> for generalising from our sample data &mdash; the distribution of values in the population is fixed and does not depend on the specific sample data.</p>
	<p class="heading">Unknown population</p>
	<p>Unfortunately the population underlying most data sets is unknown and, in practice, we only have a <strong>single sample</strong>. However this single sample does throw light on the population distribution.</p>
	<p>The diagram below describes a sample from a categorical distribution. Although the underlying population is unknown, the sample proportion of successes, <em>p</em>, is an <strong>estimate</strong> of the unknown proportions of successes in the population (denoted by π).</p>
	<p class="eqn"><img src="../../../en/infPopn/images/s_unknownP.gif" width="326" height="301"></p>




<h1 class="sectionName breakBefore">3.3 &nbsp; Normal distributions</h1>
<h2 class="pageName">3.3.1 &nbsp; Importance of normal distributions</h2>

<p class="heading">Normal distribution parameters</p>
	<p>The <a href="javascript:showNamedPage('probDensity5')">family of 
		normal distributions</a> consists of symmetric bell-shaped distributions that 
		are defined by two parameters, µ 
		and σ, the distribution's mean and standard deviation.</p>
	<p class="heading">Normal distributions as models for data</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_bestFit.gif" width="404" height="202"></p>
	<p>The sample data rarely gives enough information for us to be <strong>sure</strong> that the underlying population is  normal, but a normal model is often used unless there is <strong>obvious</strong> non-normality in the data.</p>
<p>Even if the sample data are obviously skew, a normal distribution may be a reasonable
model for a nonlinear transformation of the values (e.g. a log transformation).</p>
<p class="heading">Distribution of summary statistics</p>
	<p>A more important reason for the importance of the normal distribution in statistics 
		is that...</p>
	
<div class="centred"><div class="boxed">
<p>Many summary statistics have normal distributions (at least approximately).</p>
</div></div>

	<p>The Central Limit Theorem shows that the mean of a random sample has a distribution that is close to normal when the sample size is moderate or large, <strong>irrespective 
	of the shape of the distribution of the individual values</strong>. The following are also 
	approximately normal when the sample size is moderate or large...</p>
	<ul>
		<li>A sample proportion</li>
		<li>The slope and intercept of a least squares line</li>
		<li>The difference between the means of two samples</li>
	<li>The difference between two proportions</li></ul>




<h2 class="pageName">3.3.2 &nbsp; Shape of normal distributions</h2>

<p class="heading notPrinted">Effect of normal parameters on distribution</p>
	<p>Distributions from the normal family have different locations and spreads, 
		but other aspects of their shape are the same. Indeed, if the scales on the horizontal 
		and vertical axes are suitably chosen, ...</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_normalAxes.gif" width="514" height="295"></p>





<h2 class="pageName">3.3.3 &nbsp; Sketching a normal distribution</h2>

<p class="heading">A common diagram for <span class="red">all</span> normal 
		distributions</p>
	<p>All normal distributions have basically the same shape.</p>
	<ul>
		<li>The distribution almost disappears at 3σ 
			from µ</li>
		<li>The probability (area) further than 2σ 
			from µ 
			is small &mdash; only about <sup>1</sup>/<sub>20</sub> of the total area.</li>
	</ul>
	<p>This should allow you to sketch a normal distribution, given any values of µ and σ.</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_sketch.gif" width="505" height="237"></p>




<h2 class="pageName">3.3.4 &nbsp; Some normal probabilities</h2>

<p class="heading notPrinted">Some probabilities for normal distributions</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_empiricalRule.gif" width="499" height="553"></p>
	<p>A more precise version of the middle probability is</p>
	<ul>
		<li>P&nbsp;(within 1.96σ 
			of µ) 
			&nbsp;=&nbsp; 0.95</li>
	</ul>
	<p class="heading">70-95-100 rule of thumb and the normal distribution</p>
	<p>These probabilities are the basis of the <a href="javascript:showNamedPage('centerSpread7')">70-95-100 
		rule of thumb</a> for 
		'bell-shaped' data sets.</p>
	<ul>
		<li>About 70% of values are within <em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
		<li>About 95% of values are within 2<em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
		<li>Almost all values are within 3<em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
	</ul>




<h1 class="sectionName breakBefore">3.4 &nbsp; Distribution of sample mean</h1>
<h2 class="pageName">3.4.1 &nbsp; Parameters and statistics</h2>

<p class="heading">Sampling mechanism</p>
	<p>The mechanism of sampling from a population explains randomness in data.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_samplingHistos.gif" width="223" height="184"></p>
	<p>In practice, we must use  a <strong>single</strong> sample to find information about the population.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_oneHisto.gif" width="221" height="70"></p>
	<p class="heading">Parameters and statistics</p>
	<p>We usually focus attention on a small number of numerical characteristics.</p>
	<ul>
		<li>Populations are summarised by values called <strong> parameters</strong>.</li>
		<li>The corresponding sample values are <strong>sample statistics</strong> and provide <strong>estimates</strong> of the parameters.</li>
	</ul>
	<p class="eqn"><img src="../../../en/randomMean/images/s_oneMean.gif" width="221" height="70"></p>
	<p class="heading">Variability of sample statistics</p>
	<p>The variability in random samples also implies sample-to-sample variability in sample statistics.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_samplingMeans.gif" width="223" height="183"></p>




<h2 class="pageName">3.4.2 &nbsp; Variability of sample mean</h2>

<p class="heading notPrinted">Distribution of the sample mean</p>
	<p>The mean of a random sample of <em>n</em> values is a random quantity. Its
distribution  is centred on the population mean but its spread is lower then
that of the population distribution.</p>


<h2 class="pageName">3.4.3 &nbsp; Standard devn of sample mean</h2>

<p class="heading notPrinted">Centre and spread of the sample mean's distribution</p>
	<ul>
		<li>The sample mean has a distribution that is centred on the 
			population mean.</li>
		<li>Its variability decreases as the sample size increases. </li>
	</ul>
<p>We can be more precise. If the population has mean µ and standard deviation σ, then  the   mean of a sample of <em>n</em> values, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">, 
		has a distribution with mean and standard deviation:</p>
<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> </p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>




<h2 class="pageName">3.4.4 &nbsp; Means from normal populations</h2>

<p class="heading">Shape of the mean's distribution</p>
	<p><strong>Whatever</strong> the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
	<p>However skewness in the population distribution 
		leads to some <strong>skewness in the distribution of the mean</strong>.</p>
	<p class="heading">Samples from normal populations</p>
	<p>When the population distribution is normal, the sample mean 
	also has a normal distribution.</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-12px"><img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (μ , &nbsp;</span><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"><span style="position:relative; top:-12px">)</span></span></p>
	<br>
	<p class=eqn><img src="../../../en/randomMean/images/s_normalMean.gif" width="428" height="344"></p>




<h2 class="pageName">3.4.5 &nbsp; Large-sample normality of means</h2>

<p class="heading">Means from non-normal populations</p>
	<p>Irrespective of the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
<p>If the population is not a normal distribution, the sample mean does not 
		have a normal distribution. However  the <strong>Central Limit 
		Theorem</strong> states that...</p>
<div class="centred"><div class="boxed"><p>For most non-normal population distributions, the distribution of 
				the sample mean becomes close to normal when the sample size increases.</p></div></div>
<p class="eqn"><img src="../../../en/randomMean/images/s_gammaMean.gif" width="428" height="344"></p>




<h1 class="sectionName breakBefore">3.5 &nbsp; Introduction to estimation</h1>
<h2 class="pageName">3.5.1 &nbsp; Interest in populations</h2>

<p class=heading>Inference about a population</p>
	<p>Data  are usually collected to provide information about some 
		<strong>population or process underlying the data</strong>. The data are often modelled as a <strong>random sample </strong>from 
		this population.</p>
	<p class="eqn"><img src="../../../en/estIntro/images/d_dental.gif" width="550" height="275" class="summaryPict"></p>
	<p>More generally, inference is applicable to <strong>any</strong> situation where 
		data are obtained through a random mechanism. We may understand some aspects of 
		the process that generated the data (our <strong>model</strong> for the data-collection 
		process), but other aspects of this process must usually be estimated from data 
		that have been collected &mdash; inference.</p>




<h2 class="pageName">3.5.2 &nbsp; Interest in parameters</h2>

<p class=heading>Estimating parameters</p>
	<p>Because of the limited amount of sample data available, we usually restrict attention to  a few specific numerical characteristics of the population distribution &mdash; parameters.</p>
	<p>After identifying the population parameters that are of most interest &mdash; for example the distribution's mean, µ, or the proportion of values in a category, π, 	&mdash; we can usually <strong>estimate</strong> these values using the corresponding summary statistics from the sample. This is called <strong>inference</strong> about the parameter.</p>
	<p class="eqn"><img src="../../../en/estIntro/images/p_concrete.gif" width="550" height="275" class="summaryPict"></p>
	<p>&nbsp;</p>




<h2 class="pageName">3.5.3 &nbsp; Standard error</h2>

<p class="heading">Using an estimate of σ</p>
<p>The error distribution for a sample mean has:</p>
	<p class=eqn><span class="black"><em>error</em> &nbsp;=&nbsp; &mu;<sub>error</sub> &nbsp;=&nbsp; 0</span> </p>
	<p class=eqn><span class="black"><span style="position:relative; top:-11px"><em>standard error</em> &nbsp;=&nbsp; &sigma;<sub>error</sub> &nbsp;=&nbsp; </span><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>
	<p>In practice, the value of σ 
		is usually unknown and must be replaced by the sample standard deviation, <em>s</em>, in the formula:</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-11px"><em>standard error</em> &nbsp;=&nbsp; &sigma;<sub>error</sub> &nbsp;=&nbsp; </span><img src="../../../en/../images/symbol.sOverRootN.png" width="26" height="31" align="baseline"></span> </p>
	<p class=heading>Example</p>
	<p class="eqn"><img src="../../../en/seMean/images/dental2.gif" width="550" height="255" class="summaryPict"></p>




<h2 class="pageName">3.5.4 &nbsp; Standard error vs standard deviation</h2>

<p class="heading notPrinted">Difference between standard error and standard deviation</p>
	<p>The standard deviation of a sample mean is closely related to the population's
standard deviation:</p>
	<p class="eqn"><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
	<p>For example,</p>
	<p class="eqn"><img src="../../../en/seMean/images/s_seSd.gif" width="550" height="156"></p>
	<p>There is a similar relationship between the standard deviation of a sample and the standard error of the mean. Do not confuse them:</p>
	<p class="eqn"><img src="../../../en/seMean/images/s_seSd2.gif" width="540" height="174"></p>
	<dl>
		<dt>Standard deviation (SD)</dt>
		<dd>This describes the spread of values in the sample. It is roughly the same whatever the sample size.</dd>
		<dt>Standard error of the mean (SE)</dt>
		<dd>This is the standard deviation of the sample mean, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">, 
			and decreases as the sample size increases. </dd>
	</dl>
	<p class="heading">Common mistakes in interpretation</p>
	<dl>
		<dt>Standard error does not describe the variability of individual values</dt>
		<dd>A new value has about 95% probability of being within 2 <strong>standard 
			deviations</strong> of sample mean.</dd>
		<dt>Standard deviation does not describe the accuracy of the sample mean</dt>
		<dd>The sample mean has about 95% probability of being within 2 <strong>standard 
			errors</strong> of the population mean.<br>
	</dd>
	</dl>
<p class="heading">Warning</p>
	<p>Be particularly careful when reading journal articles. Some papers use standard 
		deviations (SD)  to describe the distribution of variables, but others 
		give the standard errors (SE) of the means of the variables.</p>

	


<h1 class="sectionName breakBefore">3.6 &nbsp; Confidence interval for mean</h1>
<h2 class="pageName">3.6.1 &nbsp; Confidence interval from standard error</h2>

<p class="heading">95% bounds on the error</p>
	<p>If we know  the error distribution of an estimator (or an approximation to it), we can find a range of values within which the 
		error will lie with probability 0.95,</p>
	<p class=eqn> <img class="gif" src="../../../en/ciMean/images/errorBounds.gif" id="gif_image_6_1_1" width="276" height="114"><iframe class="svg" src="../../../en/ciMean/images/errorBounds.svg" id="svg_image_6_1_1" width="276" height="114" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_1_1");</script> </p>
	<p>Expressed in an equation,</p>
	<p class=eqn> <span class="black">Prob (&nbsp;-<em>e*&nbsp; </em>&lt;&nbsp; <em>error</em> &nbsp;&lt;<em> &nbsp;e*</em> ) &nbsp;= &nbsp;0.95</span></p>
	<p class="heading">95% confidence interval</p>
	<p>Since the error is the difference between the estimator and the unknown parameter, this can be rewritten as:</p>
	<p class=eqn> <span class="black">Prob (&nbsp;<em>estimate</em> - <em>e*&nbsp; </em>&lt;&nbsp; <em>parameter</em> &nbsp;&lt;<em> &nbsp;estimate</em> + <em>e*</em> ) &nbsp;= &nbsp;0.95</span></p>
	<p>The interval</p>
	<p class=eqn> <span class="black"><em>estimate</em> - <em>e*&nbsp; </em><strong>&nbsp;to</strong><em> &nbsp;&nbsp;estimate</em> + <em>e*</em></span> </p>
	<p>is called a <strong>95% confidence interval</strong> and we have <strong>95% confidence</strong> that it will include the unknown parameter value.</p>
	<p class="heading">Confidence interval from standard error</p>
	<p>The 70-95-100 rule of thumb states that about 95% of values in most distributions 
		are within 2 standard deviations of the mean. For unbiased estimators (with 
		zero mean), we therefore have the approximation:</p>
	<p class=eqn> <img class="gif" src="../../../en/ciMean/images/errorBounds2.gif" id="gif_image_6_1_2" width="276" height="145"><iframe class="svg" src="../../../en/ciMean/images/errorBounds2.svg" id="svg_image_6_1_2" width="276" height="145" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_1_2");</script> </p>
	<p>This leads to the approximate 95% confidence interval</p>
	<p class="eqn"><span class="bold black"><em>estimate</em> - 2 <em>s.e.</em><em>&nbsp; </em><strong>&nbsp;to</strong><em> &nbsp;&nbsp;estimate</em> + 2 <em>s.e.</em></span>  </p>
	<p>Since the standard error of most commonly used estimators can be readily found 
		by either a formula or statistical software, a 95% confidence interval can be easily found for most estimators.</p>
	<p class="heading">Refinements</p>
	<p>If we can only find an approximation to the error distribution, the method above would only give an approximate 95% confidence interval. The '±&nbsp;2&nbsp;s.e.' 
	approximation is a useful guide in most circumstances, but we will refine this type of confidence interval for some estimators to make the confidence level closer to 95%.</p>
	



<h2 class="pageName">3.6.2 &nbsp; Confidence level</h2>

<p class="heading">Unknown population standard deviation</p>
	<p>If we know the value of the population standard deviation, σ, 
		an interval estimate of the form</p>
	<p class=eqn><img class="gif" src="../../../en/ciMean/images/ci.gif" id="gif_image_6_2_2" width="87" height="31"><iframe class="svg" src="../../../en/ciMean/images/ci.svg" id="svg_image_6_2_2" width="87" height="31" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_2_2");</script> </p>
	<p>has a confidence level of 0.95 &mdash; i.e. it is a 95% confidence interval.</p>
	
<div class="centred"><div class="boxed">
<p>In practice however, the value of σ 
					is rarely known.</p>
</div></div>

	<p>It is tempting to simply replace σ in the formula by its sample equivalent, <em>s</em>.</p>
	<p class=eqn><img class="gif" src="../../../en/ciMean/images/bad95CI.gif" id="gif_image_6_2_1" width="86" height="31"><iframe class="svg" src="../../../en/ciMean/images/bad95CI.svg" id="svg_image_6_2_1" width="86" height="31" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_2_1");</script> </p>
	<p>However replacing σ with <em>s</em> makes the confidence interval more variable and this means that it is less likely include µ &mdash; the confidence level is less than 95%. If the sample size, <em>n</em>, is large, the confidence level is close to 95%, but with smaller sample sizes the true confidence level can be much less than the target 95%. For example,</p>
	<ul>
		<li>If <em>n</em> = 5, confidence intervals of this form only include µ in 88% of random samples, not 95%</li>
	</ul>
	<p>The confidence interval must be modified if σ is unknown.</p>
	



<h2 class="pageName">3.6.3 &nbsp; Confidence interval for mean</h2>

<p class="heading">Confidence interval using a t-value</p>
	<p>The interval estimate</p>
	<p class=eqn><img class="gif" src="../../../en/ciMean/images/bad95CI.gif" id="gif_image_6_3_1" width="86" height="31"><iframe class="svg" src="../../../en/ciMean/images/bad95CI.svg" id="svg_image_6_3_1" width="86" height="31" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_3_1");</script> </p>
	<p>has a lower confidence level than 95%. In order to achieve a 95% confidence 
		level, the interval must be widened. This is done by replacing 1.96 by a slightly larger number:</p>
	<p class=eqn><img class="gif" src="../../../en/ciMean/images/plusMinusTSE.gif" id="gif_image_6_3_2" width="93" height="32"><iframe class="svg" src="../../../en/ciMean/images/plusMinusTSE.svg" id="svg_image_6_3_2" width="93" height="32" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_3_2");</script></p>
	<p>The replacement constant <em>t<sub>n-1</sub></em> is a value greater than 1.96 that depends on the sample size, <em>n</em>. The value <i>n</i> - 1 is called the <strong>degrees of freedom</strong> of the 
	constant.</p>
	<p class="eqn"><img src="../../../en/ciMean/images/s_tValues.gif" width="405" height="270"></p>
	<p>For any number of degrees of freedom, the t-value can be obtained from a table, graph or computer 
		software.</p>




<h2 class="pageName">3.6.4 &nbsp; Properties of 95% confidence interval</h2>

<p class="heading">Simulation of properties</p>
	<p>The diagram below shows results from a simulation of 100 samples, each of size <em>n</em> = 20, from a normal population. We now treat the population standard deviation, s, as being unknown and find a 95% confidence interval for µ from each sample using the formula</p>
	<p class=eqn><img class="gif" src="../../../en/ciMean/images/plusMinusTSE.gif" id="gif_image_6_4_1" width="93" height="32"><iframe class="svg" src="../../../en/ciMean/images/plusMinusTSE.svg" id="svg_image_6_4_1" width="93" height="32" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_6_4_1");</script> </p>
	<p>where <em>t</em><sub>19</sub> = 2.093.</p>
	<p class="eqn"><img src="../../../en/ciMean/images/s_ciSimT.gif" width="550" height="361"></p>
	<p>Approximately 95% of these confidence intervals include the actual parameter value, µ = 12. If more simulations had been conducted, the proportion of intervals including µ would have been closer to 95%.</p>
	<p>Note that the confidence intervals in the simulation do not all have the same width since the CI width depends on<em> s</em> and that varies from sample to sample.</p>




<h2 class="pageName">3.6.5 &nbsp; Examples</h2>

<p class=heading>Interpretation of a confidence interval</p>
	<p>In practice we only have a <strong>single</strong> sample (and a single confidence 
		interval). We do not know whether it actually includes the unknown population 
		mean, but knowing that confidence intervals obtained in this way will <strong>usually</strong> include µ 
		is very helpful. In practice,...</p>
	
<div class="centred"><div class="boxed">
<p>Being right most of the time is the best one can hope for, 
					since there is always the possibility of being misled by an unlucky sample.</p>
</div></div>

	<p>The <strong>method</strong> that we use to obtain the confidence interval has 
		probability 0.95 of including µ. 
		We cannot tell whether the single interval that we evaluate from our data set 
		is one of these 'lucky' intervals, but knowing that the <strong>method</strong> works so often gives us <strong>95% confidence</strong> in it.</p>
	<p align="center"><img src="../../../en/ciMean/images/dental3.gif" width="550" height="315" class="summaryPict"></p>




<h1 class="sectionName breakBefore">3.7 &nbsp; Exercises</h1>
</body>
</html>
