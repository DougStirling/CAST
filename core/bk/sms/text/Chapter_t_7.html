<html>
<head>
<title>7. Explaining Variability</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 7 &nbsp; Explaining Variability</h1>
<h2>7.1 &nbsp; More about variation</h2>
<h3>7.1.1 &nbsp; Understanding means and st devns</h3>
<p>It is possible to roughly guess the mean and standard deviation from a histogram and roughly sketch a symmetric histogram matching any given mean and standard deviation.</p>
<h3>7.1.2 &nbsp; Effect of outliers</h3>
<p>If a data set contains an outlier, the mean and especially the standard deviation can be badly affected. The values may be obviously wrong when the 70-95-100 rule is applied in the context of the data but examining a dot plot or box plot is best.</p>
<h3>7.1.3 &nbsp; Standard deviation of grouped data</h3>
<p>The standard deviation within groups is usually lower than the overall standard deviation.</p>
<h3>7.1.4 &nbsp; Explained and unexplained variation</h3>
<p>Splitting a data set into groups of 'similar' values results in more accurate predictions of future values if the group membership is known. The grouping is said to explain some of the overall variation.</p>
<h3>7.1.5 &nbsp; Variance and degrees of freedom</h3>
<p>The square of the standard deviation is called the variance; its value is harder to understand but it is the basis of important advanced statistical methods. The degrees of freedom are the number of pieces of information contributing to the standard deviation (or variance).</p>
<h2>7.2 &nbsp; Comparing several means</h2>
<h3>7.2.1 &nbsp; Model</h3>
<p>To compare the means of several groups, a model of normal distributions in all groups is used but all group standard deviations must be assumed to be the same.</p>
<h3>7.2.2 &nbsp; Parameter estimates</h3>
<p>The sample standard deviations in the separate groups can be combined to give a pooled estimate of the common standard deviation, Ïƒ.</p>
<h3>7.2.3 &nbsp; Variation between and within groups</h3>
<p>Both variability between group means and variability within groups must be used to assess whether the groups differ.</p>
<h3>7.2.4 &nbsp; Sums of squares</h3>
<p>Variability within groups and between groups are described by sums of squares.</p>
<h3>7.2.5 &nbsp; Coefficient of determination</h3>
<p>The coefficient of determination (R-squared) is the ratio of the between-groups and total sums of squares. It is the proportion of variation that can be explained by differences between the groups.</p>
<h3>7.2.6 &nbsp; Test for differences between groups</h3>
<p>The F-ratio is a test statistic that is based on the between- and within-groups sums of squares. The associated p-value tests whether all groups have the same mean.</p>
<h3>7.2.7 &nbsp; Examples</h3>
<p>The F-test is applied to a few data sets.</p>
<h2>7.3 &nbsp; Anova for simple linear model</h2>
<h3>7.3.1 &nbsp; Components for regression model</h3>
<p>In regression data, the difference between the response and its overall mean can be split into an explained component and a residual.</p>
<h3>7.3.2 &nbsp; Sums of squares</h3>
<p>The total sum of squares equals the explained sum of squares plus the residual sum of squares.</p>
<h3>7.3.3 &nbsp; Coefficient of determination</h3>
<p>The relative sizes of the explained and residual sums of squares holds information about the strength of the relationship. The coefficient of determination describes the proportion of total variation that is explained.</p>
<h3>7.3.4 &nbsp; Analysis of variance test</h3>
<p>The F ratio can be used to test whether the variables are related (i.e. to test whether the model slope is zero). Since the F ratio is the square of the t statistic for this test, the conclusions for the F and t tests are identical.</p>
<h2>7.4 &nbsp; Trying a quadratic curve</h2>
<h3>7.4.1 &nbsp; Linear and quadratic models</h3>
<p>Linearity can be assessed by comparing the fits of a linear and quadratic model. The total sum of squares can be split into linear, quadratic and residual sums of squares.</p>
<h3>7.4.2 &nbsp; Understanding the sums of squares</h3>
<p>The quadratic sum of squares compares the fit of a linear and quadratic model and therefore holds information about whether there is curvature in the data.</p>
<h3>7.4.3 &nbsp; Testing for linearity</h3>
<p>An F ratio comparing the quadratic and residual mean sums of squares can be used to test for linearity.</p>
<h2>7.5 &nbsp; Exercises</h2>
<h3>7.5.1 &nbsp; Standard deviation from general knowledge</h3>
<p>In this exercise, general knowledge about the type of measurement is enough to roughly guess the value of the standard deviation.</p>
<h3>7.5.2 &nbsp; Rough graph from mean and st devn</h3>
<p>The mean and standard deviation of a data set should give you a good idea of the likely distribution of values. The first exercise in this page asks you to sketch a stacked dot plot to match a given mean and standard deviation. In the second, a histogram should be drawn.</p>
<h3>7.5.3 &nbsp; Clusters and outliers (advanced)</h3>
<p>The two exercises on this page ask the effect of combining different groups of values or adding an outlier on the mean and standard deviation.</p>
</body>
</html>
