<!DOCTYPE HTML>
<html>
<head>
  <title>11. Comparing Groups</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/maths/mathStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/maths/theorems.js"></script>
  <script src="../../../structure/maths/mathJax/MathJax.js?config=TeX-AMS-MML_SVG,statMacros.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 11 &nbsp; Comparing Groups</h1>
<h1 class="sectionName">11.1 &nbsp; Models for two groups</h1>
<h2 class="pageName">11.1.1 &nbsp; Interest in underlying population</h2>

<p class="heading notPrinted">Data from two groups</p>
<p>When data are  collected from two groups, we are usually interested in  differences
between the groups
<strong>in general</strong>. The <strong>specific</strong> individuals  are of
less interest. Questions are therefore about the characteristics of the populations
or processes that we assume <strong>underlie</strong> the
data.</p>
<p class="heading">Example</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/hypnosis.gif" width="523" height="299" class="summaryPict"></p>
<p>The  questions do not refer to the 16 specific subjects &mdash; they ask about
whether anticipation of hypnosis affects the ventilation rate <strong>in general</strong>.
We
would like to use the answers to predict what will happen to other people.</p>




<h2 class="pageName">11.1.2 &nbsp; Model for two groups</h2>

<p class="heading notPrinted">Data and model</p>
<p>Data from two groups can be displayed with two histograms:</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_histos.gif" width="325" height="281"> </p>
<p>The diagram below illustrates a possible model for the data above.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_pdfs.gif" width="321" height="251"></p>




<h2 class="pageName">11.1.3 &nbsp; Parameters of the normal model</h2>

<p class="heading notPrinted">Parameters</p>
<p>A normal model for two groups has four unknown parameters (the mean and standard
deviation for each normal distribution). These parameters give considerable flexibility
and allow the model to be used for a variety of different data sets.</p>
<p>(The number of parameters can be reduced to three if it is assumed that the two
standard deviations are the same, but we will not consider this type of model here.)</p>




<h2 class="pageName">11.1.4 &nbsp; Parameter estimates</h2>

<p class="heading notPrinted">Parameter estimates</p>
<p>A normal model for 2-group data involves 4 unknown parameters, µ<sub>1</sub>,
µ<sub>2</sub>, σ<sub>1</sub> and σ<sub>2</sub>. The means and standard deviations
in the two samples provide objective estimates of the four parameters.</p>
<p class="eqn"><img src="../../../en/twoGroupModel/images/s_bestFit.gif" width="330" height="289"></p>




<h2 class="pageName">11.1.5 &nbsp; Difference between means</h2>

<p class="heading notPrinted">Comparing the populations</p>
<p>Although  standard deviations in the two populations may also differ, we are usually
most interested in the difference between the population means. Differences between
the means can be expressed in terms of the model parameters with the following questions.</p>
<ul>
<li>Is µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> = 0?</li>
<li>What is the value of µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub>?</li>
</ul>
<p class="heading">Randomness of sample difference</p>
<p>These questions  are about µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub> and the
best estimate of it is <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
However, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span> cannot
give  definitive answers <sub></sub> since
it is  random   &mdash; it varies from sample to sample.</p>
<p class="eqn"><img class="gif" src="../../../en/twoGroupModel/images/birthWt2.gif" width="523" height="296"></p>
<p>Without an understanding of the distribution of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
it is impossible to properly interpret what the sample difference, 0.104&nbsp;kg,
tells you about the difference between the underlying population means.</p>




<h1 class="sectionName breakBefore">11.2 &nbsp; Distn of sums and differences</h1>
<h2 class="pageName">11.2.1 &nbsp; Means and sums of samples</h2>

<p class="heading notPrinted">Sample mean and sum</p>
<p>The mean of a random
sample, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">,
has a distribution that is approximately normal if the sample size, <em>n</em>, is
large and alway has a
mean and standard deviation that depend on the population mean, µ, and standard deviation,
σ,</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></td>
			<td valign="middle">&nbsp;<span class="black">=&nbsp; &mu;</span></td>
		</tr>
	</table>
</div>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></td>
			<td valign="middle">&nbsp;<span class="black">=</span>&nbsp; </td>
			<td valign="middle"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></td>
		</tr>
	</table>
</div>
<p>Occasionally the sum of values in a random sample values is more useful than the
mean,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sampleSum.gif" width="220" height="18"></p>
<p>Its distribution is
a scaled version of the distribution of the mean &mdash; the same shape but different
mean and standard deviation.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumMean.gif" width="75" height="14"></p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumSD.gif" width="89" height="21"></p>
<p class="heading">Mean vs Sum</p>
<p>As the sample size increases,</p>
<ul>
<li>the standard deviation of the mean decreases, but</li>
<li>the standard deviation of the sum <strong>increases</strong>.</li>
</ul>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sdInequality.gif" width="130" height="16"></p>




<h2 class="pageName">11.2.2 &nbsp; Sum and difference</h2>

<p class="heading notPrinted">Sum and difference of two  variables</p>
<p>Applying the result about the sum of a random sample to a sample of size <em>n</em> = 2, <em>X</em><sub>1</sub>
and <em>X</em><sub>2</sub>,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn.gif" width="119" height="50"></p>
<p>If we generalise by allowing <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> to
have different means, µ<sub>1</sub> and µ<sub>2</sub>, but the same σ,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Distn2.gif" width="146" height="47"> </p>
<p>A similar result holds for the difference between <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>:</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/diff2Distn2.gif" width="146" height="47"></p>
<p>If <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> are independent and have
normal distributions, their sum and difference are also normally distributed.</p>




<h2 class="pageName">11.2.3 &nbsp; Sum and difference (cont)</h2>

<p class="heading notPrinted">General result</p>
<p>The results generalise further to independent variables that may have different
means <strong>and</strong> standard deviations.</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumDiffSD.gif" width="250" height="161"></p>
<p>The formulae for the standard deviations are more easily remembered in terms of
the <strong>variances</strong> of
the  quantities. For example,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sum2Variance.gif" width="151" height="25"></p>




<h2 class="pageName">11.2.4 &nbsp; Probabilities for sums and differences</h2>

<p class="heading notPrinted">Finding probabilities</p>
<p>To find the probability that a sum or difference satisfies an inequality, the
inequality should be translated into ones about a z-score, using the mean and standard
deviation of the quantity,</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/standardiseEqn.gif" width="95" height="31"></p>
<p>The standard normal distribution can then be used to find the  probabilities. The
examples below illustrate the method. </p>
<p class="heading">Example (total of several variables)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/totalExample.gif" width="496" height="516"> </p>
<p class="heading">Example (sum of two variables with different sd)</p>
<p class=eqn><img class="gif" src="../../../en/sumDiff/images/sumExample.gif" width="496" height="516"> </p>




<h1 class="sectionName breakBefore">11.3 &nbsp; Comparing means in two groups</h1>
<h2 class="pageName">11.3.1 &nbsp; Distn of difference between means</h2>

<p class="heading notPrinted">Difference between  means</p>
<p>The difference between <strong>any</strong> two independent
quantities <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> has a distribution
with</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/meanSDDiff2.gif" width="113" height="51"> </p>
<p>Applying this to  the difference between the
means of two random samples,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffMeanSD.gif" width="340" height="163"> </p>
<dl>
<dt>If the distributions are normal in each group,&nbsp;...</dt>
<dd>... the  sample means are normal, so their difference
also has a normal distribution.</dd>
<dt>Otherwise,&nbsp;...</dt>
<dd>... the two sample means are approximately normal if the sample sizes are
large,  so their difference is also close to normal.</dd>
</dl>

<div class="centred"><div class="boxed">
<p>Irrespective of the distributions within the
two groups, <br>
<img class="gif" src="../../../en/twoGroupInf/images/normalDistn.gif" width="331" height="44"></p>
</div></div>





<h2 class="pageName">11.3.2 &nbsp; SE of difference between means</h2>

<p class="heading notPrinted">Estimation error</p>
<p>The difference between the sample means, <span class="eqn"><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span></span>,
is a point estimate of the difference between the means of the underlying populations, <span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span>.
In order to properly interpret it, we must understand the distribution of
the estimation error.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/errorTwoDistn.gif" width="372" height="42"></p>
<p>Replacing σ<sub>1</sub><sup>2</sup> and σ<sub>2</sub><sup>2</sup> by <em>s</em><sub>1</sub><sup>2</sup> and
<em>s</em><sub>2</sub><sup>2</sup> gives an approximate error distribution,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/estErrorDistn2.gif" width="172" height="42"> </p>
<p>The standard deviation of these errors is the <strong>standard error</strong> of
the estimator.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisError.gif" width="514" height="417" class="summaryPict"></p>
<p>Our best estimate is that anticipation of hypnosis results in a mean ventilation
rate that is 0.491 higher than the control group.
From the error distribution, the error in this estimate is unlikely to be more
than about 0.6.</p>




<h2 class="pageName">11.3.3 &nbsp; CI for difference between means</h2>

<p class="heading">If <span class="black">σ<sub>1</sub></span> and <span class="black">σ<sub>2</sub></span> were known...</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><strong>Prob</strong> ( </td>
			<td valign="middle"><img src="../../../en/../images/symbol.xBarDiffRed.png" width="42" height="21" align="baseline"></td>
			<td valign="middle">&nbsp;<strong>is within</strong> &nbsp; &plusmn; &nbsp;1.96 &nbsp;</td>
			<td valign="middle"><img src="../../../en/../images/symbol.sdDiffGreen.png" width="41" height="26" align="baseline"></td>
			<td valign="middle">&nbsp; of &nbsp; <span style="color:#00F">&mu;<sub>2</sub>&nbsp;-&nbsp;&mu;<sub>1</sub></span>) &nbsp; = &nbsp; 0.95</td>
		</tr>
	</table>
</div>
<p>so a 95% confidence interval for <span class="eqn"><span class="black">µ<sub>2</sub>&nbsp;-&nbsp;µ<sub>1</sub></span></span> would
be</p>
<div class="centred">
	<table border="0" cellpadding="0" cellspacing="0" class="centred">
		<tr>
			<td valign="middle"><img src="../../../en/../images/symbol.xBarDiffRed.png" width="42" height="21" align="baseline"></td>
			<td valign="middle">&nbsp; &plusmn; &nbsp; 1.96 &nbsp;</td>
			<td valign="middle"><img src="../../../en/../images/symbol.sdDiffGreen.png" width="41" height="26" align="baseline"></td>
		</tr>
	</table>
</div>
<p class="heading">When <span class="black">σ<sub>1</sub></span> and <span class="black">σ<sub>2</sub></span> are
unknown...</p>
<p>We must replace <span class="black">σ<sub>1</sub> and <span class="black">σ<sub>2</sub></span></span> by <span class="black"><em>s</em><sub>1</sub> and <span class="black"><em>s</em><sub>2</sub></span></span> in
the confidence interval, and the constant '1.96' must  be replaced by a slightly
larger value from t-tables, </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/diffCIWithT.gif" width="257" height="70"></p>
<p>where the degrees of freedom for the t-value are </p>
<p class=eqn><span class="black">&nu; &nbsp; = &nbsp; min (<em>n</em><sub>1</sub>&minus;1, &nbsp;<em>n</em><sub>2</sub>&minus;1)</span> </p>
<p class="gray">(A more complex formula is available that gives a higher
value for &nu;. It is slightly better but the difference is usually
negligible.)</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisCI.gif" width="514" height="401" class="summaryPict"></p>




<h2 class="pageName">11.3.4 &nbsp; Testing a hypothesis</h2>

<p class="heading">Testing for a difference between  two  means</p>
<p>The difference between two groups that is of most practical
importance is a difference between their <strong>means</strong>. </p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>2</sub> &minus; </strong>&mu;<strong><sub>1</sub> &nbsp;&ne;&nbsp; 0</strong></span></p>
<p>The summary statistic that throws most light on these hypotheses is the difference
between the sample means, <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
Testing therefore involves assessment of whether this difference is unusually
far from zero. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/pValue.gif" width="454" height="266"> </p>
<p>As with all other hypothesis tests, a p-value near zero gives evidence that
the null hypothesis does not hold &mdash; evidence of a difference between the group
means. </p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupInf/images/hypnosisTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">General properties of p-values</p>
<p>A statistical hypothesis test cannot provide
a definitive answer about whether two groups have different means. The randomness
of sample data means that p-values are also random quantities.</p>
<p>It is possible to get a small p-value (supporting H<sub>A</sub>) when H<sub>0</sub> is
true, and it is possible to get a large p-value (consistent with H<sub>0</sub>)
when H<sub>A</sub> is true.</p>
<div class="centred"><div class="boxed"><p>There is some chance of being misled by an 'unlucky sample.</p></div></div>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>All p-values between 0 and 1 are equally likely. For example, there is a
5% probability of getting a p-value less than 0.05.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The p-value is more likely to be near zero, though there is still some chance
of a larger p-value.</dd>
</dl>
<p class="heading">Effect of increasing the sample size</p>
<dl>
<dt>If H<sub>0</sub> is true</dt>
<dd>The p-values remain equally likely between 0 and 1.</dd>
<dt>If H<sub>A</sub> is true</dt>
<dd>The distribution of p-values becomes more concentrated near zero, so you
are more likely to conclude that the population means are really different.</dd>
</dl>




<h2 class="pageName">11.3.5 &nbsp; One-tailed tests for differences</h2>

<p class="heading notPrinted">One- and two-tailed tests for differences</p>
<p>In a <strong>two-tailed test</strong>, the alternative hypothesis is that the two population
means are different. A <strong>one-tailed test</strong> arises when we want to test whether one
mean is <strong>higher</strong> than the other (or <strong>lower</strong> than the other).</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/hypotheses.gif" width="441" height="84"> </p>
<p class="heading">Test statistic, p-value and conclusion</p>
<p>Consider a test for the hypotheses,</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&mu;<strong><sub>2</sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub>1</sub> &nbsp;&gt;&nbsp; </strong>&mu;<strong><sub>2</sub></strong></span> </p>
<p>The alternative hypothesis is only supported by very small values of <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>.
This also corresponds to small values of the test statistic <span class="em black">t</span> ,
so the p-value is the <strong>lower</strong> tail probability of the t distribution. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupInf/images/oneTailedP.gif" width="453" height="265"> </p>
<p>A small  p-value is interpreted as giving evidence that H<sub>0</sub> is false, in a
similar way to all other kinds of hypothesis test.</p>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/twoGroupInf/images/bacteriaCarpetsTest.gif" width="514" height="468" class="summaryPict"></p>
<p class="heading">Properties of p-values</p>
<p>We again stress that a statistical hypothesis test cannot provide a definitive
answer. The randomness of sample data means that p-values are also random quantities,
so there is some chance of us being misled by an 'unlucky' sample:</p>
<ul>
<li>If µ<sub>1</sub> = µ<sub>2</sub>, it is still possible to get a small p-value
(e.g. a 5% probability of getting a p-value less than 0.05).</li>
<li>If µ<sub>1</sub> and µ<sub>2</sub> are different, large p-values are still
possible (though less likely than small p-values).</li>
</ul>




<h1 class="sectionName breakBefore">11.4 &nbsp; Comparing two proportions</h1>
<h2 class="pageName">11.4.1 &nbsp; Modelling two proportions</h2>

<p class="heading notPrinted">Two groups of successes and failures</p>
<p>We now consider data that are obtained as random samples from two populations,
with the sampled individuals being categorised into <em>successes</em> and <em>failures</em>.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/propnModel.gif" width="368" height="196"> </p>
<p>Since our  model  involves only two parameters, π<sub>1</sub> and π<sub>2</sub>,
 the two groups are the same only if π<sub>2</sub> - π<sub>1</sub> = 0. The value
of π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub> is usually unknown
but  can be estimated by <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>.
However   <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> is
a random quantity so its variability must
be taken into account when interpreting its value.</p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/birthSex.gif" width="514" height="336" class="summaryPict"></p>
<p>Note that the  questions do not refer to the specific 141  births in the
study. They ask about differences between winter and summer births 'in general'.</p>

<div class="centred"><div class="boxed"><p>We are interested in  π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub> rather
than  <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>, so we need
to understand the accuracy of our point estimate.</p></div></div>





<h2 class="pageName">11.4.2 &nbsp; Distribution of difference in proportions</h2>

<p class="heading notPrinted">Difference between two proportions</p>
<p>Within each group, the sample proportion of successes,<em> p</em>, has
a distribution that is approximately normal in large samples and has mean and
standard deviation</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/meanSDOfP.gif" width="175" height="42"></p>
<p>Applying the  general results about the difference between two independent
random quantities:</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/meanSD.gif" width="554" height="120"> </p>
<p>Since the individual proportions are approximately normal (in large samples),
their difference is also approximately normal:</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffPDistn.gif" width="417" height="70"></p>




<h2 class="pageName">11.4.3 &nbsp; CI for difference in proportions</h2>

<p class="heading">Standard error of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub></p>
<p>The  standard deviation of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub> is
also its standard error when it<sub></sub> is
used to estimate π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub>,</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/sdDiffEqn.gif" width="228" height="45"></p>
<p>In practice, π<sub>1</sub> and π<sub>2</sub> must be replaced by their sample
equivalents to estimate the standard error. </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/sdDiffEqn2.gif" width="228" height="45"></p>
<p class="heading">Confidence interval for difference</p>
<p>Most 95% confidence intervals are of the form</p>
<p class="eqn"><em>estimate</em>   ±   1.96 &times; se(<em>estimate</em>)</p>
<p>perhaps with a refinement of using a slightly higher value than 1.96 (e.g.
a t-value) if the standard error is estimated. Applying this to our estimate
of π<sub>2</sub>&nbsp;-&nbsp;π<sub>1</sub>and using 2 instead of 1.96 gives the
approximate 95% confidence interval</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffCIEqn2.gif" width="278" height="48"></p>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/bingeCI.gif" width="514" height="373" class="summaryPict"></p>




<h2 class="pageName">11.4.4 &nbsp; Testing for difference in probabilities</h2>

<p class="heading">Two-tailed test</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;=&nbsp; </strong>&pi;<strong><sub>2</sub></strong><br>
<strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&ne;&nbsp; </strong>&pi;<strong><sub>2</sub></strong></span></p>
<p>For this test, the steps involved in obtaining a p-value are: </p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/pValCalc.gif" width="436" height="261"> </p>
<p>The p-value is interpreted in the same way as for all previous tests. A p-value
close to zero is unlikely when <b>H<sub>0</sub></b> is true, but is more likely
when <b>H<sub>A</sub></b> holds. Small p-values therefore provide evidence of
a difference between the population probabilities. </p>
<p class="heading">One-tailed test</p>
<p>In a 1-tailed test, the alternative hypothesis is</p>
<p class=eqn><span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&gt;&nbsp; 0</strong></span> &nbsp;&nbsp; <span class="red"><strong>or</strong></span> &nbsp;&nbsp; <span class="blue"><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&pi;<strong><sub>1</sub> &nbsp;&minus;&nbsp; </strong>&pi;<strong><sub>2</sub> &nbsp;&lt;&nbsp; 0</strong></span></p>
<p>The test statistic is identical to that for a 2-tailed test and the p-value
is obtained in a similar way, but it is found from only a <strong>single</strong> tail
of the standard normal distribution. </p>
<p class="heading"><span class="black">Alternative test statistic</span></p>
<p>Since π<sub>1</sub> and
π<sub>2</sub> are equal if <b>H<sub>0</sub></b> is true, the overall proportion
of successes, <em>p</em>, can be used in the formula for the standard error
of <em>p</em><sub>2</sub>&nbsp;-&nbsp;<em>p</em><sub>1</sub>.</p>
<p class=eqn><img class="gif" src="../../../en/twoGroupPropn/images/diffSDEstEqn.gif" width="476" height="95"> </p>
<p>This refinement makes little difference in practice,
so the examples below use the 'simpler' formula that we gave earlier.</p>
<p class="heading">Two-tailed example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/birthSexTest.gif" width="541" height="476" class="summaryPict"></p>
<p class="heading">One-tailed example</p>
<p align="center"><img src="../../../en/twoGroupPropn/images/asprinTest.gif" width="541" height="476" class="summaryPict"></p>




<h1 class="sectionName breakBefore">11.5 &nbsp; Paired t test</h1>
<h2 class="pageName">11.5.1 &nbsp; Paired data</h2>

<p class="heading notPrinted">Paired data</p>
<p>When two types measurements, <em>X</em> and <em>Y</em>, are made from each
individual (or other unit), the data are called <strong>bivariate</strong>. Sometimes
the two measurements are of  closely related quantities and
may even describe the same quantity at different times. </p>
<div class="centred"><div class="boxed"><p>When the sum or difference of <em>X</em> and <em>Y</em> is a meaningful quantity,
the data are called <strong>paired data</strong>.</p></div></div>
<p class="heading">Hypotheses of interest</p>
<p>For paired data, We often want to test whether the means of the two variables
are equal,</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>Sometimes a one-tailed test is required, such as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &gt; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p class="heading">Examples</p>
<dl>
<dt>Pre-test, post-test data</dt>
<dd>This arises when a measurement is made from each individual, then a second
measurement of the same type is made after some kind of intervention (e.g. training
or medication). Has the intervention &quot;improved&quot; the measurement?</dd>
<dt>Twin studies</dt>
<dd>Some experiments or other studies are conducted with
identical twins, either human or animal. The  members
of each pair experience different environments &mdash; either two different experimental
treatments or two other differences. Are there differences between the two treatments?</dd>
<dt>Other types of pairing</dt>
<dd>For example, damaged cars may each be taken to two garages for estimates
of the cost of repair. The two estimates for each car are paired data. Does one
garage overcharge?</dd>
</dl>





<h2 class="pageName">11.5.2 &nbsp; Analysis of differences</h2>

<p class="heading notPrinted">Differences</p>
<p>Information about the difference between the means of <em>X</em> and <em>Y</em> is
contained in the values <em>D</em> = (<em>Y</em> - <em>X</em>)  for
each individual. The hypotheses</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> = </strong>&mu;<strong><sub><em>Y</em></sub></strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>X</em></sub> &ne; </strong>&mu;<strong><sub><em>Y</sub></em></strong></span></p>
<p>can then be expressed as</p>
<p class="eqn"><span class="darkblue"><strong><font size="+1">H</font><sub>0</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> = 0</strong><br><strong><font size="+1">H</font><sub>A</sub>&nbsp;:&nbsp;&nbsp;&nbsp;</strong>&mu;<strong><sub><em>D</em></sub> &ne; 0</strong></span></p>
<p>This reduces the paired data set to a <strong>univariate</strong> data set of
differences,
<em>D</em>, and reduces questions about (µ<sub><em>Y</em></sub> - µ<sub><em>X</em></sub>)
to questions about the mean of <em>D</em>.</p>
<p class="heading">Analysis of paired data</p>
<p>By taking differences between <em>Y</em> and <em>X</em>, much of the variability
between the individuals is eliminated, making it easier to see whether their means
are different. The example below shows paired data on the left with blue lines
joining the x- and y-values in each pair. The differences on the right make it clearer
that the y-values are usually higher than the corresponding x-values.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_pairing.gif" width="404" height="322"></p>





<h2 class="pageName">11.5.3 &nbsp; Paired t-test</h2>

<p class="heading notPrinted">Approach (paired t-test)</p>
<p>Testing whether two paired measurements, <em>X</em> and <em>Y</em>,
have equal means is done in terms of the differences</p>
<p class="eqn"><em><strong>D</strong></em><strong> = <em>Y</em> - <em>X</em></strong></p>
<p>The test is then expressed as</p>
<p class="eqn"><strong>H<sub>0</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> =
0</p>
<p class="eqn"><strong>H<sub>A</sub></strong>:&nbsp; &nbsp;µ<em><sub>D</sub></em> ≠
0</p>
<p>or a one-tailed variant. The hypotheses are therefore assessed with a standard
univariate t-test using test statistic</p>
<p class="eqn"><img class="gif" src="../../../en/testPaired/images/testStat.gif" width="86" height="58"></p>
<p>This is compared to a t distribution with <em>n</em>&nbsp;-&nbsp;1
degrees of freedom to find the p-value.</p>
<p class="heading">Example</p>
<p>The diagram below illustrates a 2-tailed test for equal means, based on <em>n</em> = 15
paired observations.</p>
<p class="eqn"><img src="../../../en/testPaired/images/s_example.gif" width="475" height="384"></p>
<p>From the p-value, we conclude that there is very strong evidence that the
means for <em>Y</em> and <em>X</em> are different.</p>





<h2 class="pageName">11.5.4 &nbsp; Pairing and experimental design</h2>

<p class="heading notPrinted">Choice between paired data or two independent samples</p>
<p>It is sometimes possible to answer questions about the difference
between two means by collecting two alternative types of data.</p>
<dl>
<dt>Two independent samples</dt>
<dd>Measurements are made from two samples of individuals from the groups whose
means are to be compared. A 2-sample t-test can be used to compare the means.</dd>
<dt>One paired sample</dt>
<dd>The 'individuals' can be re-defined as pairs of related values from the two
groups and a single sample of these pairs can be collected. A paired t-test
can be performed on the differences to compare the means.</dd>
</dl>
<div class="centred"><div class="boxed"><p>If the individuals in the 2 groups can be paired so that the pairs
are relatively similar, a paired design gives more accurate results.</p></div></div>
<p class="heading">Matched pairs in experiments</p>
<p>In experiments to compare two treatments, it may be possible to group the
experimental units into pairs that are similar in some way. These are called <strong>matched
pairs</strong>.
If the two experimental units in each pair are randomly assigned to the two
treatments, the data can be analysed as described in this section.</p>
<p>The difference between the treatments is estimated more accurately than in
a completely randomised experiment.</p>




<h1 class="sectionName breakBefore">11.6 &nbsp; Comparing several means</h1>
<h2 class="pageName">11.6.1 &nbsp; Model</h2>

<p class="heading notPrinted">Data</p>
<p>In this section, we examine data that may arise as:</p>
<ul>
<li>A separate sample of numerical measurements from each of <em>g</em> groups,
or </li>
<li>Bivariate data with a numerical response and a categorical 'explanatory'
variable with <em>g</em> levels. This categorical variable can be used to define
'groups' of individuals.</li>
</ul>
<p>We will model the data in terms of <em>g</em> groups. The data often arise
from <a href="javascript:showNamedPage('designIntro4')">completely randomised
experiments with <em>g</em> treatments</a>.</p>
<p class="heading">Model</p>
<p>The model that was used for 2 groups can be easily extended to to <em>g</em>&nbsp;&gt;&nbsp;2
groups, allowing different means and standard deviations in all groups.</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ<sub><em>i</em></sub>)</span></td>
</tr>
</table></div>
<p class="eqn"><img src="../../../en/multiGroup/images/s_multiModel.gif" width="401" height="315"></p>
<p>However to develop a test for equal group means with <em>g</em>&nbsp;&gt;&nbsp;2
groups, we must make an extra assumption that the
standard deviations in all groups are the same.</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p class="eqn"><img src="../../../en/multiGroup/images/s_multiModel2.gif" width="401" height="286"></p>
<p>If there are <i>g</i> groups, this model has <i>g</i>&nbsp;+&nbsp;1 unknown
parameters &mdash; the <i>g</i> group means and the common standard deviation, σ.
It is flexible enough to be useful for many data sets.</p>
<div class="centred"><div class="boxed"><p>If the assumptions of a normal distribution and constant variance
do not hold, a nonlinear transformation of the response may result in data for
which the model is appropriate.</p></div></div>




<h2 class="pageName">11.6.2 &nbsp; Parameter estimates</h2>

<p class="heading">Estimating the group means</p>
<p>We now assume a  normal model with the same standard deviation
in each group,</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p>The sample means provide estimates of the {µ<sub><em>i</em></sub>}:</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/groupMeanEst.gif" width="52" height="22"> </p>
<p class="heading">Estimating σ<sup>2</sup></p>
<p>The sample standard deviation in any single group, <em>s<sub>i</sub></em>,
is a valid estimate of σ, but we need to combine these <em>g</em> separate estimates
in some way.</p>
<p>It is easier to describe estimation of σ<sup>2</sup> rather than σ. If the sample
sizes are the same in all groups, a <strong>pooled</strong> estimate
of σ<sup>2</sup> is the average of the group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst3.gif" width="105" height="38"> </p>
<p>If the sample sizes are <strong>not</strong> equal in all groups, this is
generalised by adding the numerators and denominators of the formulae for the <em>g</em> separate
group variances,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst.gif" width="444" height="67"> </p>
<p>More mathematically,  <em>y<sub>ij</sub></em> denotes the <span class="em black">j</span> 'th
of the <span class="em black">n<sub>i</sub></span>  values in group <span class="em black">i</span> ,
for <span class="em black">i</span> &nbsp;=&nbsp;1 to <span class="em black">g</span> .
The pooled estimate of σ<sup>2</sup> can then be written as</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/varPooledEst2.gif" width="210" height="80"> </p>
<p>The pooled variance is influenced most by the sample variances in the groups
with biggest sample sizes.</p>




<h2 class="pageName">11.6.3 &nbsp; Revisiting two groups ((optional))</h2>

<p class="heading notPrinted">Revisiting the difference between <span class="red">two</span> group
means</p>
<p>In an earlier section, we described  confidence intervals and tests about the
difference between two group means, µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub>.
They can be improved if we can assume that</p>
<p class=eqn><span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span> </p>
<p>Inference is still based on <span style="position:relative; top:5px"><img src="../../../en/../images/symbol.xBarDiff.png" width="36" height="15" align="baseline"></span>,
but the equation for its standard deviation can be simplified</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/diffMeanSD.gif" width="261" height="45"> </p>
<p class="heading">Confidence interval</p>
<p>A 95% confidence interval for µ<sub>2&nbsp;</sub>-&nbsp;µ<sub>1</sub> has
the same general form as before,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/generalCI.gif" width="160" height="22"> </p>
<p>but the standard deviation and the degrees of freedom
for the t-value, &nu;, are different.</p>
<div class="centred">
	<table border="0" class="centred" cellpadding="10" cellspacing="0">
		<tr>
			<td style="border-bottom:1px solid #999999;">&nbsp;</td>
			<td align="center" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/sdEst.gif" width="42" height="22"></td>
			<th align="center" style="border-bottom:1px solid #999999;"><span class="black">degrees of freedom</span></th>
		</tr>
		<tr>
			<td style="border-bottom:1px solid #999999;">Allowing <span class="black">&sigma;<sub>1</sub> &ne; &sigma;<sub>2</sub></span></td>
			<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/sdEst1.gif" width="85" height="45"></td>
			<td align="center" bgcolor="#FFFFFF" class="black" style="border-bottom:1px solid #999999;">min( <em>n</em><sub>1</sub> - 1, <em>n</em><sub>2</sub> - 1)</td>
		</tr>
		<tr>
			<td style="border-bottom:1px solid #999999;">Assuming <span class="black">&sigma;<sub>1</sub> = &sigma;<sub>2</sub> = &sigma;</span></td>
			<td align="center" bgcolor="#FFFFFF" style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/diffMeanSDEst.gif" width="109" height="39"></td>
			<td align="center" bgcolor="#FFFFFF" class="black" style="border-bottom:1px solid #999999;"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> - 2</td>
		</tr>
	</table>
</div>
<p>If it can be assumed that &sigma;<sub>1</sub> = &sigma;<sub>2</sub>,  the
confidence interval is usually narrower.</p>
<p class="heading">Example</p>
<p>The diagram below shows 95% confidence intervals obtained by the two methods.</p>
<p align="center"><img src="../../../en/multiGroup/images/hypnosisPooled.gif" width="550" height="340"> </p>
<p>The p-value for this test is found from the tail area of the t distribution
with (<span class="black"><em>n</em><sub>1</sub> + <em>n</em><sub>2</sub> -
2</span>) degrees of freedom.</p>




<h2 class="pageName">11.6.4 &nbsp; Variation between and within groups</h2>

<p class="heading notPrinted">Comparing several groups</p>
<p>A new approach is needed to compare the means of three or more groups &mdash; the
 methods for two groups cannot be extended. We again assume a normal model with equal
standard deviations,</p>
<div class="centred"><table border="0" cellpadding="4" cellspacing="0" bordercolor="#CCCCCC" class="centred" style="margin-top:0; margin-bottom:0">
<tr>
<td>Group <em>i</em>:&nbsp;&nbsp;</td>
<td><span style="color:#000000"><em>Y</em> &nbsp; ~ &nbsp; <span class="arial">normal</span> (µ<sub><em>i&nbsp;</em></sub>,
σ)</span></td>
</tr>
</table></div>
<p>Testing whether there are differences between the groups involves the hypotheses,</p>

<p style="margin-left:200px; color:#000000"><font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>0</sub></strong></font> : &nbsp; µ<sub><em>i</em></sub> &nbsp;=&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
all i and j</em><br>
<font size="+1" face="Arial, Helvetica, sans-serif"><strong>H</strong></font><font face="Arial, Helvetica, sans-serif"><strong><sub>A</sub></strong></font>: &nbsp; µ<sub><em>i</em></sub> &nbsp;≠&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
at least some i, j</em></p>

<p class="heading">Variation between and within groups</p>
<p>Testing whether the model means, {µ<sub><em>i</em></sub>}, are equal is done
by assessing the <strong>variation between the group means</strong> in the data.
However, because of randomness in sample data, the means are unlikely be the same,
even if <strong>H<sub>0</sub></strong> is true.</p>
<p>In the example on the left below, the group means vary so much that the 
{µ<sub><em>i</em></sub>} are almost certainly not equal. However the group means
on the right are relatively similar and their differences may simply be randomness.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_variationBetween.gif" width="518" height="295"></p>
<p>To assess whether the means are 'unusually different',
we must also take account of the <strong>variation within
the groups</strong>. The data set on the left below gives much stronger evidence
of group differences than that on the right, even though the group means are
the same in both data sets.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_variationWithin.gif" width="518" height="295"></p>
<p>The evidence against <strong>H<sub>0</sub></strong> depends on the <strong>relative size</strong> of the
variation within groups and between groups.</p>




<h2 class="pageName">11.6.5 &nbsp; Sums of squares</h2>

<p class="heading notPrinted">Notation</p>
<p>In the formulae in this page, the values in the <em>i</em>'th group are denoted
by <span class="em black">y<sub>i</sub></span> <span class="black"><sub>1</sub></span>, <span class="em black">y<sub>i</sub></span> <span class="black"><sub>2</sub></span>,
... . More generally, the <i>j</i>'th
value in the <i>i</i>'th group is called <span class="em black">y<sub>ij</sub></span>  and
the
mean of the values in the <i>i</i>'th group is  <span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yiBar.png" width="12" height="14" align="baseline"></span>.</p>
<p class="heading">Total variation</p>
<div class="centred">
	<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
		<tr>
			<td width="95"><img class="gif" src="../../../en/multiGroup/images/totalSsq.gif" width="79" height="27"></td>
			<td class="green" style="text-align:left;">The <strong>total sum of squares</strong> reflects the 
				total variability of the response.</td>
		</tr>
	</table>
</div>
<p>The overall variance of all values (ignoring 
groups) is the total sum of squares divided by (<em>n</em>&nbsp;-&nbsp;1).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_totalSsq.gif" width="413" height="277"></td>

<div class="centred">
	<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
		<tr>
			<td width="95"><img class="gif" src="../../../en/multiGroup/images/regnSsq.gif" width="75" height="26"></td>
			<td class="red" style="text-align:left;">The <strong> sum of squares between groups</strong> measures 
				the variability of the group means.</td>
		</tr>
	</table>
</div>
<p>Variation between groups is summarised by the differences between the group
means and the overall mean. Note that the summation  is <span class="black bold">over
all observations in the data set</span>.</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_betweenSsq.gif" width="413" height="277"></td>
	<div class="centred">
		<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
			<tr>
				<td width="95"><img class="gif" src="../../../en/multiGroup/images/residSsq.gif" width="83" height="27"></td>
				<td class="blue" style="text-align:left;">The <strong> sum of squares within groups</strong> quantifies 
					the spread of values within each group.</td>
			</tr>
		</table>
	</div>
<p>This is also called the <strong>residual</strong> sum of squares since it
describes variability that is unexplained by differences between the groups.
Note that the pooled estimate of the common variance, σ<sup>2</sup>, is the sum
of squares within groups divided by (<em>n</em>&nbsp;-&nbsp;<em>g</em>).</p>
<p class="eqn"><img src="../../../en/multiGroup/images/s_withinSsq.gif" width="413" height="277"> </p>




<h2 class="pageName">11.6.6 &nbsp; Coefficient of determination</h2>

<p class="heading notPrinted">Sums of squares</p>
<div class="centred"><table border="0" class="centred" cellpadding="4" cellspacing="0">
<tr>
<th align="left" style="border-bottom:1px solid #999999;">Sum of squares</th>
<th style="border-bottom:1px solid #999999;">Interpretation</th>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/totalSsq2.gif" width="141" height="25">
<td>Overall variability of <em>Y</em>, taking no account of the groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residSsq2.gif" width="154" height="25">
<td>Variability
that <strong>cannot be explained</strong> by the model.</td>
</tr>
<tr>
<td style="border-bottom:1px solid #999999;"><img class="gif" src="../../../en/multiGroup/images/regnSsq2.gif" width="149" height="24">
<td style="border-bottom:1px solid #999999;">Variability that is <strong>explained</strong> by
the model.</td>
</tr>
</table></div>
<p class="heading">Coefficient of determination</p>
<p>The <strong>proportion</strong> of
the total sum of squares that is explained by the model is called
the <strong>coefficient of determination</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/rSquaredDefn.gif" width="97" height="38"> </p>
<ul>
<li>0  ≤  R<sup>2</sup>  ≤  1</li>
<li>The proportion of <strong>unexplained</strong> variation is (1&nbsp;-&nbsp;R<sup>2</sup>)</li>
<li>When R<sup>2</sup> &asymp; 0, the group means are similar
to each other.</li>
<li>If R<sup>2</sup> &asymp; 1, the individual values must be  close
to their group means.</li>
</ul>
<p class="heading">Example</p>
<p align="center"><img src="../../../en/multiGroup/images/radiocarbonR2.gif" width="555" height="344" class="summaryPict"></p>





<h2 class="pageName">11.6.7 &nbsp; Test for differences between groups</h2>

<p class="heading notPrinted">Hypothesis test</p>
<p>The following hypotheses are used to test whether the group means are all
equal:</p>
<p style="margin-left:200px;" class="black"><span class="arial"><strong><span class="bigger">H</span><sub>0</sub></strong></span> : &nbsp; µ<sub><em>i</em></sub> &nbsp;=&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
all i and j</em><br>
<span class="arial"><strong><span class="bigger">H</span><sub>A</sub></strong></span>: &nbsp; µ<sub><em>i</em></sub> &nbsp;≠&nbsp; µ<sub><em>j</em></sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>for
at least some i, j</em></p>
<p>We will describe some of the steps for this test,  but cannot
justify them here.</p>
<p class="heading">Mean sums of squares</p>
<p>The three sums of squares are first divided by  values called their <strong>degrees
of freedom</strong>:</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/totalMss.gif" width="139" height="43"></td>
<td class="green">The <strong>mean total sum of squares</strong> is the
sample variance of the response (ignoring groups).</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/residMss.gif" width="143" height="43"></td>
<td class="blue">The <strong>mean within-group sum of squares</strong> is
the pooled estimate of the variance within groups.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/multiGroup/images/regnMss.gif" width="162" height="43"></td>
<td class="red">The <strong>mean between-group sum of squares</strong> is
harder to directly interpret.</td>
</tr>
</table></div>
<p>The numerators in these ratios add up:</p>
<p class="eqn"><span class="green">SS<sub>Total</sub></span>  =  <span class="red">SS<sub>Between</sub></span>  +  <span class="blue">SS<sub>Within</sub></span></p>
<p>and the same relationship holds for their denominators (degrees of freedom):</p>
<p class="eqn"><span class="green">df<sub>Total</sub></span>  =  <span class="red">df<sub>Between</sub></span>  +  <span class="blue">df<sub>Within</sub></span></p>
<p class="heading">F ratio and p-value</p>
<p>The test statistic is  an <strong>F-ratio</strong>,</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/fRatio.gif" width="114" height="37"> </p>
<p>This test statistic compares between- and within-group variation. The further
apart the group means, the larger <span class="red">SS<sub>Between</sub></span> and the larger the F-ratio.<br>
</p>

<div class="centred"><div class="boxed">
<p>Large values of F suggest that H<sub>0</sub> does
not hold &mdash; that the group means are not the same.</p>
</div></div>

<p>The p-value for the test is the probability of such a high F ratio if <strong>H<sub>0</sub></strong> is
true (all group means are the same). It is based on a standard distribution called
an <strong>F distribution</strong> and is interpreted in the same way as other
p-values. </p>

<div class="centred"><div class="boxed">
<p>The closer the p-value to zero, the stronger the evidence
that H<sub>0</sub> does not hold.</p>
</div></div>

<p class="heading">Analysis of variance table</p>
<p>An <strong>analysis
of variance table</strong> (<strong>anova table</strong>) describes some of the calculations
above:</p>
<p class=eqn><img class="gif" src="../../../en/multiGroup/images/anovaTable2.gif" width="596" height="192"></p>




<h2 class="pageName">11.6.8 &nbsp; Examples</h2>

<p align="center"><img src="../../../en/multiGroup/images/radiocarbonF.gif" width="550" height="434" class="summaryPict"></p>
<p align="center"><img src="../../../en/multiGroup/images/teachingF.gif" width="550" height="434" class="summaryPict"></p>




<h1 class="sectionName breakBefore">11.7 &nbsp; Randomised blocks</h1>
<h2 class="pageName">11.7.1 &nbsp; Generalising the idea of paired data</h2>

<p>In paired data, two related measurements, <em>X</em> and <em>Y</em>, are made
from each sampled individual and we are interested in testing whether their means
are equal.</p>
<p class="heading">Groups of 3 or more values</p>
<p>The idea of paired data can be extended to situations in which 3 or more related
measurements are made from each 'individual'. Two important situations that give
rise to this type of data are:</p>
<dl>
<dt>Experiment with blocks</dt>
<dd>Paired data can arise when the experimental units are grouped into blocks of
size 2 (e.g. matched pairs) and two treatments are used. This can be extended to <em>g</em> treatments
with blocks of <em>g</em> experimental units and the treatments randomised within
each block.</dd>
<dt>Repeated measure data</dt>
<dd>Several comparable measurements may be made from each individual, often measurements
of the same quantity at different times.</dd>
</dl>
<p class="heading">Example (randomised blocks) </p>
<p>In an experiment to assess the effect of  codeine and acupuncture
for relieving dental pain, 32 subjects  were grouped into
blocks of 4 according to an initial assessment of their tolerance to pain. Four treatments
were randomly given to the four subjects in each block and pain relief scores were
recorded.</p>
<div class="centred">
	<table border="0" class="centred" cellpadding="3" cellspacing="0">
		<tr>
			<th>&nbsp;</th>
			<th colspan="4">Pain relief score</th>
		</tr>
		<tr>
			<th>Tolerance<br>
				group</th>
			<th>&nbsp;&nbsp;Control&nbsp;&nbsp;</th>
			<th>Codeine<br>
				only</th>
			<th>Acupuncture<br>
				only</th>
			<th>Codeine +<br>
				Acupuncture</th>
		</tr>
		<tr align="center">
			<td><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">1<br>
						2<br>
						3<br>
						4<br>
						5<br>
						6<br>
						7<br>
						8</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border:1px solid #999999; border-right:0px;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">0.0<br>
						0.3<br>
						0.4<br>
						0.4<br>
						0.6<br>
						0.9<br>
						1.0<br>
						1.2</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">0.6<br>
						0.7<br>
						0.8<br>
						0.9<br>
						1.5<br>
						1.6<br>
						1.7<br>
						1.6</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">0.5<br>
						0.6<br>
						0.8<br>
						0.7<br>
						1.0<br>
						1.4<br>
						1.8<br>
						1.7</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border:1px solid #999999; border-left:0px;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">1.2<br>
						1.3<br>
						1.6<br>
						1.5<br>
						1.9<br>
						2.3<br>
						2.1<br>
						2.4</td>
				</tr>
			</table></td>
		</tr>
	</table>
</div>
<p class="heading">Example (repeated measures)</p>
<p>An experiment investigated the use of nicotine to control tics in patients with
Tourette's syndrome. For each patient, the number
of tics was recorded before a nicotine gum was chewed and at different times afterwards.</p>
<div class="centred">
	<table border="0" class="centred" cellpadding="3" cellspacing="0">
		<tr>
			<th>&nbsp;</th>
			<th colspan="4">Number of tics during 30-min period</th>
		</tr>
		<tr>
			<th>Patient</th>
			<th>&nbsp;&nbsp;Baseline&nbsp;&nbsp;</th>
			<th>  Chewing gum  </th>
			<th>  0-30 min after  </th>
			<th>  30-60 min after  </th>
		</tr>
		<tr align="center">
			<td><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">1<br>
						2<br>
						3<br>
						4<br>
						5<br>
						6<br>
						7<br>
						8<br>
						9<br>
						10</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border:1px solid #999999; border-right:0px;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">249<br>
						1095<br>
						83<br>
						569<br>
						368<br>
						326<br>
						324<br>
						95<br>
						413<br>
						332</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">108<br>
						593<br>
						27<br>
						363<br>
						141<br>
						134<br>
						126<br>
						41<br>
						365<br>
						293</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border-top:1px solid #999999; border-bottom:1px solid #999999;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">93<br>
						600<br>
						32<br>
						342<br>
						167<br>
						144<br>
						312<br>
						63<br>
						282<br>
						525</td>
				</tr>
			</table></td>
			<td bgcolor="#FFFFFF" style="border:1px solid #999999; border-left:0px;"><table border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="right">59<br>
						861<br>
						61<br>
						312<br>
						180<br>
						158<br>
						260<br>
						71<br>
						321<br>
						455</td>
				</tr>
			</table></td>
		</tr>
	</table>
</div>


<h2 class="pageName">11.7.2 &nbsp; Example with baseline treatment</h2>

<p>We start with a simple example in which  one of the <em>g</em> treatments is a
standard or 'baseline' treatment. The other (<em>g</em> &minus; 1)
treatments can be compared to it using standard confidence intervals for paired data.
These confidence intervals are usually narrower than the corresponding confidence
intervals that would be found for independent samples.</p>
<p class="heading">Example</p>
<p>In a randomised experiment about pain relief treatments in dental patients, 32
subjects were grouped into blocks of four according to an initial assessment of their
tolerance to pain. One treatment was a placebo (dummy treatment) that the others
could be compared to.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_indepCIs.gif" width="453" height="474"></p>
<p>If the initial grouping of paitents into blocks is ignored, 95% confidence intervals
for the improvement in pain relief over the placebo are wide. Taking account of the
initial grouping, differences are far more accurately estimated.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_pairedCIs.gif" width="453" height="474"></p>




<h2 class="pageName">11.7.3 &nbsp; Use of blocking information</h2>

<p class="heading notPrinted">Testing for equal treatment means</p>
<p>If  there is no baseline treatment, analysis should start with a single hypothesis
test for whether all treatment means are equal. The standard multi-group analysis
of variance test for equal means in a completely randomised experiment (ignoring
the blocks) should <strong>not</strong> be used for experiments with
blocks.</p>
<div class="centred"><div class="boxed"><p>Ignoring the existence of blocks makes it much harder to detect differences
between treatments.</p></div></div>
<p class="heading">Example</p>
<p>Five different observers each watched the same group of 10 cattle and reported
how long each animal spent grazing.</p>
<p style="color:#F00; font-weight:bold; font-size:larger; margin-bottom:0px">Wrong analysis</p>
<p style="margin-top:5px">Ignoring the fact that the same animals were observed by all five observers, the
data would be analysed with the anova table below. From the large p-value, we would
conclude that there were no differences between the observers.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_indepPvalue.gif" width="482" height="433"></p>
<p style="color:#F00; font-weight:bold; font-size:larger; margin-bottom:0px">Correct analysis</p>
<p style="margin-top:5px">Much of the variability in the data is due to differences
between the animals (blocks),
and an analysis that ignores this is much less sensitive to differences between the
observers. We will not explain the correct test for blocked data until later in this
section, but it gives a p-value that is interpreted in the same way as the p-value
above. It is shown below and
shows that there are almost certainly differences between the observers.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_blockedPvalue.gif" width="482" height="380"></p>



<h2 class="pageName">11.7.4 &nbsp; Randomised block designs</h2>

<p>In paired data, each of the two treatments is used once within each block (pair).
The previous
pages generalised this to more than two treatments, but each treatment was still
used once in each block. We now generalise further to allow the
block
size to be any multiple of the number of treatments.</p>
<p class="heading">Reducing unexplained variability</p>
<p>To assess the significance of differences between experimental treatments, variation
in the treatment means is compared to the amount of unexplained (random) variation.
With less unexplained variation, there is less chance of the differences between
treatment means having arisen by chance. There are two ways to reduce unexplained
variation:</p>
<dl>
<dt>Use experimental units that are as similar as possible.</dt>
<dd>Identical experimental units cannot usually be found but similar experimental
units give best accuracy.</dd>
<dt>Group the experimental units into blocks of similar units.</dt>
<dd>If the experimental units vary but can be grouped into blocks of similar units,
then these blocks 'explain' some of the variability, and unexplained variability
is reduced.</dd>
</dl>
<p>The simplest way to use blocks in an experiment is with a <strong>randomised block
design</strong>. In this, the block size is a multiple of the number of treatments.
Each treatment is used for the same number of experimental units within each block,
and the treatments are randomly allocated to units within the blocks.</p>
<p class="heading">Example</p>
<p>An experiment was conducted in which the experimental units were intestinal preparations
from fish, but each fish would only give six preparations. The six preparations from
each fish constitute a block of units. Two treatments were used, with  the six preparations
from each of four fish randomly split into three preparations for each treatment
(a randomised block experiment).</p>
<p style="color:#F00; font-weight:bold; font-size:larger; margin-bottom:0px">Wrong
analysis</p>
<p style="margin-top:5px">Ignoring possible differences between the four fish and
treating the data as a completely randomised experiment with 24 experimental units,
we would conclude that there is moderately strong evidence of a difference between
the two treatments.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_indepPvalue2.gif" width="466" height="300"></p>
<p style="color:#F00; font-weight:bold; font-size:larger; margin-bottom:0px">Correct
analysis</p>
<p style="margin-top:5px">There are considerable differences between the four fish
(blocks), with much lower variability within any single fish. The correct analysis
is explained later in this section and the resulting p-value gives much stronger
evidence of a difference between the two treatments.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_blockedPvalue2.gif" width="466" height="300"></p>




<h2 class="pageName">11.7.5 &nbsp; Model for randomised blocks ((optional))</h2>

<p class="heading">Three-dimensional scatterplot of data</p>
<p>Data from a randomised block experiment can be displayed in a three-dimensional
scatterplot:</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_threeD.gif" width="392" height="369"></p>
<p class="heading">Model</p>
<p>Both blocks and treatments explain some variability in the response measurement,
<em>Y</em>, but...</p>
<ul>
<li>Blocks are a grouping of the experimental units whereas treatments are allocated
by the researcher.</li>
<li>We are mainly interested in comparing the treatments.</li>
</ul>
<p>Blocks and treatments are modelled
in the same way:</p>

<div class="centred"><div class="boxed">
<p><em>y</em> = (overall
mean) + (effect depending on block)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; + (effect depending on treatment)
+ error</p>
</div></div>

<p>The error is again assumed to have a normal distribution with mean zero and constant
standard deviation. Within any block, changing the treatment simply adds or subtracts
a constant to the response.</p>




<h2 class="pageName">11.7.6 &nbsp; Removing block effects</h2>

<p class="heading notPrinted">Making all block means equal</p>
<p>Our model for randomised block data explains the effect of the blocks on <em>Y</em> as
a  addition  of a &quot;block effect&quot; to all values within each block. This
suggests eliminating differences between the blocks by adjusting the values in all
blocks to have the same block means.</p>
<p class="heading">Example</p>
<p>The diagram below shows results from an experiment with blocks of size five and
five treatments. Different colours are used for the different blocks and the block
means are shown with vertical lines.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_blockMeans.gif" width="482" height="217"></p>
<p>A lot of the variability in the response, <em>Y</em>, is caused by differences
between the blocks. The diagram below adjusts the values by adding a constant to
all values in each block, givving all blocks the same mean response.</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_blockSameMeans.gif" width="482" height="217"></p>
<p>Since there is now much less 'unexplained' variation and there are now no differences
between blocks, applying the standard anova test for equal treatment means to the
adjusted data seems reasonable and is much more sensitive to treatment differences:</p>
<p class="eqn"><img src="../../../en/randBlock/images/s_adjustedAnova.gif" width="387" height="87"></p>
<p>The treatment and residual sums of squares shown here are the <strong>basis</strong> for
testing whether the treatment means are  equal, but the analysis is not completely
correct.</p>
<div class="centred"><table width="80%" border="1" cellspacing="0" cellpadding="10" bgcolor="#FFFF00" class="centred">
<tr>
<td align="center" class="darkred bold">The  residual degrees of freedom
are too high.</td>
</tr>
</table></div>
<p>The correct analysis of variance table for testing equal treatment
means is a little more complex for randomised block data; it will be explained in
the following pages.</p>




<h2 class="pageName">11.7.7 &nbsp; Sums of squares</h2>

<p class="heading notPrinted">Notation</p>
<p>For randomised block data, we use the following notation:</p>
<div class="centred">
	<table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
		<tr>
			<td align="center" style="border-top:1px solid #999999;"><span class="em">y<sub>bgr</sub></span></td>
			<td style="border-top:1px solid #999999;">the <em>r</em><sup>th</sup> of the observations in block <em>b</em> that get treatment <em>g</em></td>
		</tr>
		<tr>
			<td align="center"><em><span style="position:relative; top:3px"><img src="../../../en/../images/symbol.yBar.png" width="9" height="15" align="baseline"></span><sub><em><span class="black">b</span></em></sub></em></td>
			<td>mean response in block <em>b</em></td>
		</tr>
		<tr>
			<td align="center"><em><span style="position:relative; top:3px"><img src="../../../en/../images/symbol.yBar.png" width="9" height="15" align="baseline"></span><sub><em><span class="black">g</span></em></sub></em></td>
			<td>mean response for all observations getting treatment <em>g</em></td>
		</tr>
		<tr>
			<td align="center" style="border-bottom:1px solid #999999;"><em><span style="position:relative; top:3px"><img src="../../../en/../images/symbol.yBar.png" width="9" height="15" align="baseline"></span></em></td>
			<td style="border-bottom:1px solid #999999;">overall mean response for all observations</td>
		</tr>
	</table>
</div>
<p>In many examples, there is only a single observation for each combination of block
and treatment, but our notation allows for two or more.</p>
<p class="heading">Sums of squares</p>
<p>For randomised block data, we again split the total sum of squares into components,
but now need to use <strong>three </strong>components.</p>
<p class=eqn><img class="gif" src="../../../en/randBlock/images/ssqEqn.gif" width="453" height="50"></p>
<p>The block and treatment sums of squares describe variation that is <strong>explained</strong> by
the randomised block model whereas the residual sum of squares is <strong>unexplained</strong>.</p>
<div class="centred"><table width="90%" border="0" class="centred" cellpadding="6" cellspacing="0" style="background-color:#FFFFFF">
<tr>
<td><img class="gif" src="../../../en/randBlock/images/totalSsq.gif" width="84" height="28"></td>
<td class="purple">The <strong>total sum of squares</strong> reflects the
total variability of the response.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/randBlock/images/blockSsq.gif" width="73" height="26"></td>
<td class="green">The <strong> sum of squares between blocks</strong> measures
the variability of the block means.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/randBlock/images/treatSsq.gif" width="73" height="28"></td>
<td class="red">The <strong> sum of squares between treatments</strong> measures
the variability of the treatment means.</td>
</tr>
<tr>
<td><img class="gif" src="../../../en/randBlock/images/residSsq.gif" width="95" height="28"></td>
<td class="blue">The <strong> residual sum of squares </strong> describes the
variation that is <strong>unexplained</strong> by blocks or treatments.</td>
</tr>
</table></div>
<p>Note that all summations are  over<strong> all observations in the data set</strong>.</p>
<p class="heading">Residuals and residual variation</p>
<p>As in regression, we define residuals to be the difference between the recorded
response values and the closest we can get from our model. For a randomised block
model, the best estimate is:</p>
<p class=eqn><span class="black"><span style="position:relative; top:4px"><img src="../../../en/../images/symbol.yiHat.png" width="11" height="18" align="baseline"></span>&nbsp;=&nbsp; <em>b</em><sub>0</sub> + <em>b</em><sub>1 </sub><em>x<sub>i</sub></em></span></p>
<p>This can be interpreted as:</p>
<ul>
<li>the block mean, with an adjustment for how far the treatment mean is different
from the overall mean, or</li>
<li>the treatment mean, with an adjustment for how far the observation's block mean
is different from the overall mean.</li>
</ul>




<h2 class="pageName">11.7.8 &nbsp; Anova table and examples</h2>

<p class="heading notPrinted">Anova table</p>
<p>The three components that add to total sum of squares 
are usually laid out in an <strong>analysis of variance</strong> table
(or simply <strong>anova</strong> table).</p>
<p class=eqn><img class="gif" src="../../../en/randBlock/images/randBlockAnovaTable.gif" width="592" height="186"> </p>
<p>The anova table adds a few extra columns:</p>
<dl>
<dt>Degrees of freedom</dt>
<dd>They also add up to the value in the <em>Total</em> row, (<em>n</em> &minus; 1).</dd>
<dt>Mean sums of squares</dt>
<dd>These are the sums of squares divided by their degrees of freedom.</dd>
<dt>F-ratios</dt>
<dd>For the blocks and treatments, the F-ratio divides the mean sum of squares by
the mean residual sum of squares.</dd>
</dl>
<p class="heading">Tests</p>
<p>The F-ratio for differences between the treatments compares the variability explained
by the treatments to the residual (unexplained) variation. The larger the F-ratio,
the stronger the evidence for a difference between treatments. A formal hypothesis
test is based on the F-ratio and its p-value is the probability of getting as big
an F-ratio as that recorded if all treatment means were equal. It is interpreted
in the same way as all other p-values.</p>
<dl>
<dt>p-value &gt; 0.1</dt>
<dd>No evidence of a difference between treatments</dd>
<dt>0.1 &lt; p-value &lt; 0.05</dt>
<dd>Very mild evidence of a difference between treatments</dd>
<dt>0.05 &lt; p-value &lt; 0.01</dt>
<dd>Moderately strong evidence of a difference between treatments</dd>
<dt>p-value &lt; 0.01</dt>
<dd> Strong evidence of a difference between treatments</dd>
</dl>
<p>A p-value can also be found to test whether there are differences between the
blocks, but this is usually of less interest.</p>
<div class="centred"><div class="boxed">
<p>In practice, computer software will produce the anova table for you, so you only
need to interpret the p-value associated with the treatments.</p>
</div></div>
<p class="heading">Examples</p>
<p align="center"><img src="../../../en/randBlock/images/painRelief.gif" width="550" height="544" class="summaryPict"></p>
<p align="center"><img src="../../../en/randBlock/images/penicillin.gif" width="550" height="544" class="summaryPict"></p>





</body>
</html>
