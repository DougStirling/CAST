<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE section SYSTEM "../../../structure/sectionXmlDefn.dtd">

<section name='Distribution of variance'>
<topText><![CDATA[
	<p>This section describes a little theory that underlies analysis of variance.</p>
]]></topText>


<page dir='en/ssq' filePrefix='ssq1'>The square of a standard normal variable has a chi-squared distribution with 1 degree of freedom.</page>
<page dir='en/ssq' filePrefix='ssq2'>The sum of n squared standard normal variables has a chi-squared distribution with n d.f.</page>
<page dir='en/ssq' filePrefix='ssq3'>Differences between values and the population mean can be written as the sum of two components and their sums of squares satisfy a similar relationship. This shows that the sum of squares about the sample mean is less than or equal to that about the population mean.</page>
<page dir='en/ssq' filePrefix='ssq4'>The sum of squares about the population mean can be split in different ways into component sums of squares with chi-squared distributions. The sum of squares about the sample mean has (n-1) degrees of freedom and its mean sum of squares is the sample variance.</page>
<page dir='en/ssq' filePrefix='ssq5'>The ratio of two independent sample variances (or mean sums of squares) has an F distribution whose degrees of freedom are those of the two variances.</page>
<page dir='en/ssq' filePrefix='ssq6'>The sum of squares about the sample mean can often be further split into component sums of squares. Comparison of the corresponding mean sums of squares can be used to test whether the model underlying the data has certain characteristics. </page>
<page dir='en/ssq' filePrefix='ssq7'>This page summarises the most important results from the section. </page>

</section>
