<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE section SYSTEM "../../../structure/sectionXmlDefn.dtd">

<section name='Sequential sums of squares'>


<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq1'>As explanatory variables are added to the model, the regression plane gets closer to the data points. The regression planes corresponding to models with only X or Z correspond to planes that have zero slope for the other variable.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq2'>Each additional variable reduces the residual sum of squares by an amount that is the sum of squares of differences between the least squares fits of the two models.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq3'>The explained sum of squares for X can be different, depending on whether Z is already in the model.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq4'>There are two ways to split the total sum of squares in an anova table. The F-test for the final variable added to the model gives identical results to the t-test for the coefficient in the full model.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq5'>When the two explanatory variables are uncorrelated (orthogonal), the results are easier to interpret. The slope coefficients for X are the same, whether or not Z is in the model, and the two anova tables are identical.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq6'>Orthogonal variables usually only arise from designed experiments. They result in the most accurate parameter estimates and results that are relatively easy to interpret.</page>
<page dir='en/seqRegnSsq' filePrefix='seqRegnSsq7'>For any sequence of models with increasing complexity, component sums of squares can be defined that compare successive models in the sequence.</page>

</section>
