<html>
<head>
<title>5. Anova Theory (Advanced)</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 5 &nbsp; Anova Theory (Advanced)</h1>
<h2>5.1 &nbsp; Distribution of variance</h2>
<h3>5.1.1 &nbsp; Distribution of Z-squared</h3>
<p>The square of a standard normal variable has a chi-squared distribution with 1 degree of freedom.</p>
<h3>5.1.2 &nbsp; Sums of squares</h3>
<p>The sum of n squared standard normal variables has a chi-squared distribution with n d.f.</p>
<h3>5.1.3 &nbsp; Sum of squares about sample mean</h3>
<p>Differences between values and the population mean can be written as the sum of two components and their sums of squares satisfy a similar relationship. This shows that the sum of squares about the sample mean is less than or equal to that about the population mean.</p>
<h3>5.1.4 &nbsp; Sums of squares tables</h3>
<p>The sum of squares about the population mean can be split in different ways into component sums of squares with chi-squared distributions. The sum of squares about the sample mean has (n-1) degrees of freedom and its mean sum of squares is the sample variance.</p>
<h3>5.1.5 &nbsp; Ratio of variances and F distribution</h3>
<p>The ratio of two independent sample variances (or mean sums of squares) has an F distribution whose degrees of freedom are those of the two variances.</p>
<h3>5.1.6 &nbsp; Overview of analysis of variance</h3>
<p>The sum of squares about the sample mean can often be further split into component sums of squares. Comparison of the corresponding mean sums of squares can be used to test whether the model underlying the data has certain characteristics. </p>
<h3>5.1.7 &nbsp; Summary of anova distributions</h3>
<p>This page summarises the most important results from the section. </p>
<h2>5.2 &nbsp; Inference for variances (optional)</h2>
<h3>5.2.1 &nbsp; Confidence interval for the variance</h3>
<p>A 95% CI for the population variance can be found from the chi-squared distn with (n-1) degrees of freedom.</p>
<h3>5.2.2 &nbsp; Properties of the confidence interval</h3>
<p>As with other 95% CIs, there is 95% probability that a confidence interval for the variance will include the underlying population variance.</p>
<h3>5.2.3 &nbsp; Warning about CI for variance</h3>
<p>The confidence level for the 95% CI is only accurate if the sample comes from a normal population. The CI should therefore be avoided unless you are sure about the shape of the population distribution.</p>
<h3>5.2.4 &nbsp; Independence of mean and variance</h3>
<p>For random samples from a normal distribution, the sample mean and variance are independent.</p>
<h3>5.2.5 &nbsp; Model and hypotheses</h3>
<p>For data that that arise as samples from normal distributions in both groups, we tested earlier whether the group means were the same. Equality of the group variances can also be examined.</p>
<h3>5.2.6 &nbsp; Test statistic</h3>
<p>The ratio of the two sample variances has an F distribution whose shape depends on the sample sizes in the two groups.</p>
<h3>5.2.7 &nbsp; F test</h3>
<p>To test equality of two variances, the F ratio is compared to an F distribution. The test is 2-tailed and the p-value is twice the smaller tail area.</p>
<h2>5.3 &nbsp; Anova in simple settings (optional)</h2>
<h3>5.3.1 &nbsp; Different approach to testing mean</h3>
<p>The two component sums of squares can be used to test the value of the population mean. The ratio of the mean sums of squares has an F distribution if the null hypothesis holds.</p>
<h3>5.3.2 &nbsp; P-value for F test</h3>
<p>The p-value for the test is the upper tail area of the F distribution.</p>
<h3>5.3.3 &nbsp; Equivalence of F and t tests</h3>
<p>The F test based on the anova table results in the same p-value and conclusion as a t test for the hypotheses.</p>
<h3>5.3.4 &nbsp; Components sums of squares for two groups</h3>
<p>Components can be defined whose sums of squares hold information about the difference between the group means, the variability within group 1 and the variability within group 2.</p>
<h3>5.3.5 &nbsp; Testing for equal  group variance</h3>
<p>The sums of squares of the two within-group components lead to the same F test that was described in an earlier section for whether the group variances are equal.</p>
<h2>5.4 &nbsp; Simple linear models</h2>
<h3>5.4.1 &nbsp; Distributions of sums of squares</h3>
<p>The residual sum of squares has a chi-squared distribution with (n - 2) d.f. The explained sum of squares only has a chi-squared distribution (1 d.f.) if Y is unrelated to X -- its distribution has a higher mean otherwise.</p>
<h3>5.4.2 &nbsp; F ratio</h3>
<p>The ratio of the mean explained and mean residual sums of squares has an F distribution with (1, n-2) d.f. if Y is unrelated to X. The F ratio is expected to be higher if the variables are related.</p>
<h3>5.4.3 &nbsp; Analysis of variance test</h3>
<p>The F ratio can be used to test whether the variables are related (i.e. to test whether the model slope is zero).</p>
<h2>5.5 &nbsp; Sums of squares for other models</h2>
<h3>5.5.1 &nbsp; Sums of squares and degrees of freedom</h3>
<p>This page gives general results about sums of squares, their degrees of freedom and their distributions, for any sequence of models.</p>
<h3>5.5.2 &nbsp; Distributions of sums of squares</h3>
<p>In a linear model with 2 numerical explanatory variables, the residual sums of squares has a chi-squared distribution with (n-3) degrees of freedom. The explained regression sum of squares has a chi-squared distribution with 2 degrees of freedom if the response is unrelated to the explanatory variables.</p>
<h3>5.5.3 &nbsp; Distribution of quadratic ssq</h3>
<p>In a quadratic model, the quadratic sum of squares has a chi-squared distribution if the true model is linear, but its distribution has a larger mean if there is curvature.</p>
<h3>5.5.4 &nbsp; Theory behind anova test</h3>
<p>The between-group and within-group sums of squares have chi-squared distributions and the anova F ratio has an F distribution if both group means are equal. The p-value for the anova test is the tail area of this distribution.</p>
<h3>5.5.5 &nbsp; Theory behind anova test (advanced)</h3>
<p>The sums of squares again have chi-squared distributions but with different degrees of freedom.</p>
</html>
