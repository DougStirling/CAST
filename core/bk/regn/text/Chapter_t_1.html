<html>
<head>
<title>1. Simple Linear Regression</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 1 &nbsp; Simple Linear Regression</h1>
<h2>1.1 &nbsp; Correlation</h2>
<h3>1.1.1 &nbsp; Strength of a relationship</h3>
<p>The main feature of interest in a scatterplot is the strength of the relationship between the two variables.</p>
<h3>1.1.2 &nbsp; Units for X and Y</h3>
<p>A numerical description of the strength of a relationship should not be affected by rescaling the variables.</p>
<h3>1.1.3 &nbsp; Units-free variables (z-scores)</h3>
<p>Standardising a variable gives z-scores that do not depend on the units of the original variable. (The correlation coefficient will be defined in terms of z-scores for X and Y.)</p>
<h3>1.1.4 &nbsp; Correlation coefficient</h3>
<p>The correlation coefficient summarises the strength of the relationship between X and Y. It is +1 when the scatterplot crosses are on a straight line with positive slope, -1 when on a line with negative slope, and zero when X and Y are unrelated.</p>
<h3>1.1.5 &nbsp; Scatterplots and the value of r</h3>
<p>You should be able to estimate the value of r from looking at a scatterplot and imagine a scatter of crosses corresponding to any value of r.</p>
<h3>1.1.6 &nbsp; Nonlinear relationships</h3>
<p>The correlation coefficient is only a good measure of the strength of a relationship if the points in a scatterplot are scattered round a straight line, not a curve.</p>
<h3>1.1.7 &nbsp; R does not tell the whole story</h3>
<p>The correlation coefficient cannot identify curvature, outliers or clusters and can be misleading if these features are present. A scatterplot must always be examined too.</p>
<h3>1.1.8 &nbsp; Outliers</h3>
<p>An extreme value of one or both of the variables is an outlier. An unusual combination of values is also called an outlier.</p>
<h3>1.1.9 &nbsp; Clusters</h3>
<p>If the crosses on a scatterplot separate into clusters, different groups of individuals are suggested.</p>
<h3>1.1.10 &nbsp; Dangers of over-interpretation</h3>
<p>In small data sets, there may be considerable variability, so patterns should be strongly evident before they are reported.</p>
<h2>1.2 &nbsp; Association &amp; causal relationships</h2>
<h3>1.2.1 &nbsp; Interest in relationships</h3>
<p>For most data sets, we are interested in understanding the relationships between the variables. However interpreting relationships must be done with care.</p>
<h3>1.2.2 &nbsp; Causal and non-causal relationships</h3>
<p>If the relationship between X and Y is causal, it is possible to predict the effect of changing the value of X.</p>
<h3>1.2.3 &nbsp; Detecting causal relationships</h3>
<p>Causality can only be deduced from how the data were collected — the data values themselves do not contain any information about causality.</p>
<h3>1.2.4 &nbsp; Observational and experimental data</h3>
<p>In an observational study, values are passively recorded from individuals. Experiments are characterised by the experimenter's control over the values of one or more variables.</p>
<h3>1.2.5 &nbsp; Data collection and causality</h3>
<p>Causal relationships can only be deduced from well-designed experiments.</p>
<h2>1.3 &nbsp; Least squares</h2>
<h3>1.3.1 &nbsp; Predicting Y from X</h3>
<p>A line or curve is useful for predicting the value of Y from a known value of X.</p>
<h3>1.3.2 &nbsp; Linear models</h3>
<p>A straight line can often be used to predict one variable from another.</p>
<h3>1.3.3 &nbsp; Fitted values and residuals</h3>
<p>The difference between the actual value of Y and the value predicted by a line is called a residual. Small residuals are clearly desirable.</p>
<h3>1.3.4 &nbsp; Least squares</h3>
<p>The sum of squared residuals describes the accuracy of predictions from a line. The method of least squares positions the line to minimise the sum of squared residuals.</p>
<h3>1.3.5 &nbsp; Curvature and outliers</h3>
<p>A linear model is not appropriate if there are either curvature or outliers in a scatterplot of the data. Outliers should be carefully examined.</p>
<h3>1.3.6 &nbsp; Residual plots</h3>
<p>Outliers and curvature in the relationship are often displayed more clearly in a plot of residuals.</p>
<h3>1.3.7 &nbsp; Predicting Y and predicting X</h3>
<p>Least squares does not treat Y and X symmetrically. The best line for predicting Y from X is different from the best line for predicting X from Y.</p>
<h2>1.4 &nbsp; Linear regression models</h2>
<h3>1.4.1 &nbsp; Interest in generalising from data</h3>
<p>Some bivariate data sets describe complete populations. Others are 'representative' of an underlying population or process.</p>
<h3>1.4.2 &nbsp; Distribution of Y for each X</h3>
<p>Bivariate data can be modelled by specifying a response distribution for each possible X.</p>
<h3>1.4.3 &nbsp; Normal linear model</h3>
<p>The response is often modelled with a normal distribution whose mean is a linear function of X and whose standard deviation is constant.</p>
<h3>1.4.4 &nbsp; Another way to describe the model</h3>
<p>A normal linear model can be described in terms of 'errors'. In samples from the model, approximately 95% of errors are within 2 standard deviations of zero, so about 95% of the points in a scatterplot are within this distance of the regression line.</p>
<h3>1.4.5 &nbsp; Model parameters</h3>
<p>The normal linear model has 3 unknown parameters. For many data sets, these parameters have meaningful interpretations.</p>
<h2>1.5 &nbsp; Estimating parameters</h2>
<h3>1.5.1 &nbsp; Estimating the slope and intercept</h3>
<p>A least squares line provides estimates of the linear model's slope and intercept. These estimates are random values — they vary from sample to sample.</p>
<h3>1.5.2 &nbsp; Estimating the error standard devn</h3>
<p>The third parameter of the normal linear model is the error standard deviation. It can be estimated using the residuals from the least squares line.</p>
<h3>1.5.3 &nbsp; Distn of least squares estimates</h3>
<p>The least squares estimate of the model's slope has a normal distribution that is centred on the true value.</p>
<h3>1.5.4 &nbsp; Standard error of least squares slope</h3>
<p>The distribution of the least squares slope may be estimated from a single data set.</p>
<h3>1.5.5 &nbsp; 95% confidence interval for slope</h3>
<p>A confidence interval for the model's slope can be obtained from its least squares estimate and its standard error.</p>
<h3>1.5.6 &nbsp; Properties of confidence interval</h3>
<p>Confidence intervals for the model's slope have the same properties as confidence intervals for population means or proportions.</p>
<h3>1.5.7 &nbsp; Influences on accuracy</h3>
<p>The standard error of the least squares slope depends on the response standard deviation round the model line, the sample size and the standard deviation of X. Collecting data with a big spread of x-values gives more accurate estimates but there are disadvantages.</p>
<h2>1.6 &nbsp; Testing regression parameters</h2>
<h3>1.6.1 &nbsp; Importance of zero slope</h3>
<p>If the model's slope is zero, the response distribution does not depend on the explanatory variable. This special case is particularly meaningful in many studies.</p>
<h3>1.6.2 &nbsp; Testing whether slope is zero</h3>
<p>The p-value for testing whether a linear model's slope is zero is the probability that its least squares estimate is as far from zero as the recorded value.</p>
<h3>1.6.3 &nbsp; Strength of evidence and relationship</h3>
<p>It is important to distinguish the strength of a relationship (summarised by the correlation coefficient) and the strength of evidence for existence of a relationship (summarised by the p-value).</p>
<h3>1.6.4 &nbsp; Properties of p-values</h3>
<p>As with other tests, all p-values between 0 and 1 are equally likely if the null hypothesis holds (model slope is zero), but p-values nearï¿½0 are more likely if the alternative hypothesis holds (model slope is non-zero).</p>
<h2>1.7 &nbsp; Predicting the response</h2>
<h3>1.7.1 &nbsp; Estimated response distn at X</h3>
<p>From estimates of the 3 linear model parameters, we can obtain an estimated response distribution at any x-value.</p>
<h3>1.7.2 &nbsp; Variability of estimate at X</h3>
<p>The predicted response at any X varies from sample to sample. The prediction is more variable at x-values far from the mean of the 'training' data.</p>
<h3>1.7.3 &nbsp; Estimating the mean vs prediction</h3>
<p>A distinction is made between estimating the mean response at X and predicting a new individual's response at X. Errors are larger (on average) when predicting a new individual's response.</p>
<h3>1.7.4 &nbsp; Confidence and prediction intervals</h3>
<p>A 95% confidence interval is used to estimate the mean response at X. A 95% prediction interval is similar, but gives a range of likely values for a new response value. The prediction interval is wider than the confidence interval.</p>
<h2>1.8 &nbsp; Linear model assumptions</h2>
<h3>1.8.1 &nbsp; Assumptions in a normal linear model</h3>
<p>The normal linear model involves assumptions of linearity, constant variance, normal error distribution and independence of different observations. Residuals can be examined to assess whether these assumptions are appropriate for a particular data set.</p>
<h3>1.8.2 &nbsp; Curvature &mdash; transforming X</h3>
<p>If the relationship between Y and X is nonlinear, a transformation of X may linearise the relationship.</p>
<h3>1.8.3 &nbsp; Curvature and non-constant variance</h3>
<p>Transforming the response may remove curvature in the relationship, but also affects whether the error standard deviation is constant. Fortunately, the same transformation of Y often removes curvature and non-constant standard deviation.</p>
<h3>1.8.4 &nbsp; Transformations and prediction</h3>
<p>If a normal linear model describes the relationship between a transformation of the response and a transformation of the explanatory variable, predictions can be made by fitting the linear model to the transformed data, then performing the inverse transformation on the prediction.</p>
<h3>1.8.5 &nbsp; Non-normal errors</h3>
<p>The errors in a normal linear model are assumed to have normal distributions. Violation of this assumption is less important than nonlinearity, non-constant variance or outliers, but a probability plot of the residuals can be used to assess normality.</p>
<h3>1.8.6 &nbsp; Correlated errors</h3>
<p>The errors in a normal linear model are assumed to be independent. In data where the observations are recorded sequentially, successive errors are sometimes found to be correlated. Correlated errors can arise whatever the x-variable, but are most often seen when the x-variable is time itself.</p>
<h2>1.9 &nbsp; Leverage, outliers and influence</h2>
<h3>1.9.1 &nbsp; Leverage</h3>
<p>The most effective x-value at which to take a new response observation is one where predictions are most variable. The variance of predictions at x, divided by sigma-squared is called the leverage at x.</p>
<h3>1.9.2 &nbsp; Outliers and leverage</h3>
<p>If an outlier is also a high-leverage point, it can badly 'pull' the least squares line and the resulting residual often does not indicate that it is an outlier.</p>
<h3>1.9.3 &nbsp; Variances of the residuals</h3>
<p>Even when all data points come from a normal linear model, all residuals do not have the same standard deviation.</p>
<h3>1.9.4 &nbsp; Standardised residuals</h3>
<p>Dividing the residuals by an estimate of their standard deviation gives values that can be compared to ±2 and ±3 to look for outlliers.</p>
<h3>1.9.5 &nbsp; Deleted residuals</h3>
<p>Standardised residuals still do not show up outliers that are high leverage points. Deleted residuals are based on the difference between the response and the prediction from the data without that observation.</p>
<h3>1.9.6 &nbsp; Externally studentised residuals</h3>
<p>Rather than standardising each residual by dividing by its standard deviation based on the mean squared residual for the whole data set, it is better to standardise with the mean squared residual from the data set without that value.</p>
<h3>1.9.7 &nbsp; Influence on fitted values</h3>
<p>Leverage describes the potential of each point to influence the results. DFITS describes its actual influence on the fitted values.</p>
<h3>1.9.8 &nbsp; Influence on regression coefficients</h3>
<p>An alternative measure of influence describes the influence of each point on the least squares coefficients.</p>
<h3>1.9.9 &nbsp; Summary and examples</h3>
<p>This page summarises the various measures of residual and influence and gives a few examples where residuals, leverage and influence are interpreted.</p>
</html>
