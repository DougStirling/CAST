<html>
<head>
<title>11. Regression and Correlation</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 11 &nbsp; Regression and Correlation</h1>
<h2>11.1 &nbsp; Linear regression models</h2>
<h3>11.1.1 &nbsp; Interest in generalising from data</h3>
<p>Some bivariate data sets describe complete populations. Others are 'representative' of an underlying population or process.</p>
<h3>11.1.2 &nbsp; Distribution of Y for each X</h3>
<p>Bivariate data can be modelled by specifying a response distribution for each possible X.</p>
<h3>11.1.3 &nbsp; Normal linear model</h3>
<p>The response is often modelled with a normal distribution whose mean is a linear function of X and whose standard deviation is constant.</p>
<h3>11.1.4 &nbsp; Another way to describe the model</h3>
<p>A normal linear model can be described in terms of 'errors'. In samples from the model, approximately 95% of errors are within 2 standard deviations of zero, so about 95% of the points in a scatterplot are within this distance of the regression line.</p>
<h3>11.1.5 &nbsp; Model parameters</h3>
<p>The normal linear model has 3 unknown parameters. For many data sets, these parameters have meaningful interpretations.</p>
<h2>11.2 &nbsp; Estimating parameters</h2>
<h3>11.2.1 &nbsp; Estimating the slope and intercept</h3>
<p>A least squares line provides estimates of the linear model's slope and intercept. These estimates are random values — they vary from sample to sample.</p>
<h3>11.2.2 &nbsp; Estimating the error standard devn</h3>
<p>The third parameter of the normal linear model is the error standard deviation. It can be estimated using the residuals from the least squares line.</p>
<h3>11.2.3 &nbsp; Distn of least squares estimates</h3>
<p>The least squares estimate of the model's slope has a normal distribution that is centred on the true value.</p>
<h3>11.2.4 &nbsp; Standard error of least squares slope</h3>
<p>The distribution of the least squares slope may be estimated from a single data set.</p>
<h3>11.2.5 &nbsp; 95% confidence interval for slope</h3>
<p>A confidence interval for the model's slope can be obtained from its least squares estimate and its standard error.</p>
<h3>11.2.6 &nbsp; Properties of confidence interval</h3>
<p>Confidence intervals for the model's slope have the same properties as confidence intervals for population means or proportions.</p>
<h3>11.2.7 &nbsp; Influences on accuracy</h3>
<p>The standard error of the least squares slope depends on the response standard deviation round the model line, the sample size and the standard deviation of X. Collecting data with a big spread of x-values gives more accurate estimates but there are disadvantages.</p>
<h2>11.3 &nbsp; Testing regression parameters</h2>
<h3>11.3.1 &nbsp; Importance of zero slope</h3>
<p>If the model's slope is zero, the response distribution does not depend on the explanatory variable. This special case is particularly meaningful in many studies.</p>
<h3>11.3.2 &nbsp; Testing whether slope is zero</h3>
<p>The p-value for testing whether a linear model's slope is zero is the probability that its least squares estimate is as far from zero as the recorded value.</p>
<h3>11.3.3 &nbsp; Strength of evidence and relationship</h3>
<p>It is important to distinguish the strength of a relationship (summarised by the correlation coefficient) and the strength of evidence for existence of a relationship (summarised by the p-value).</p>
<h3>11.3.4 &nbsp; Properties of p-values</h3>
<p>As with other tests, all p-values between 0 and 1 are equally likely if the null hypothesis holds (model slope is zero), but p-values nearï¿½0 are more likely if the alternative hypothesis holds (model slope is non-zero).</p>
<h2>11.4 &nbsp; Predicting the response</h2>
<h3>11.4.1 &nbsp; Estimated response distn at X</h3>
<p>From estimates of the 3 linear model parameters, we can obtain an estimated response distribution at any x-value.</p>
<h3>11.4.2 &nbsp; Variability of estimate at X</h3>
<p>The predicted response at any X varies from sample to sample. The prediction is more variable at x-values far from the mean of the 'training' data.</p>
<h3>11.4.3 &nbsp; Estimating the mean vs prediction</h3>
<p>A distinction is made between estimating the mean response at X and predicting a new individual's response at X. Errors are larger (on average) when predicting a new individual's response.</p>
<h3>11.4.4 &nbsp; Confidence and prediction intervals</h3>
<p>A 95% confidence interval is used to estimate the mean response at X. A 95% prediction interval is similar, but gives a range of likely values for a new response value. The prediction interval is wider than the confidence interval.</p>
<h2>11.5 &nbsp; Linear model assumptions</h2>
<h3>11.5.1 &nbsp; Assumptions in a normal linear model</h3>
<p>The normal linear model involves assumptions of linearity, constant variance, normal error distribution and independence of different observations. Residuals can be examined to assess whether these assumptions are appropriate for a particular data set.</p>
<h3>11.5.2 &nbsp; Curvature &mdash; transforming X</h3>
<p>If the relationship between Y and X is nonlinear, a transformation of X may linearise the relationship.</p>
<h3>11.5.3 &nbsp; Curvature and non-constant variance</h3>
<p>Transforming the response may remove curvature in the relationship, but also affects whether the error standard deviation is constant. Fortunately, the same transformation of Y often removes curvature and non-constant standard deviation.</p>
<h3>11.5.4 &nbsp; Transformations and prediction</h3>
<p>If a normal linear model describes the relationship between a transformation of the response and a transformation of the explanatory variable, predictions can be made by fitting the linear model to the transformed data, then performing the inverse transformation on the prediction.</p>
<h3>11.5.5 &nbsp; Outliers and leverage</h3>
<p>An outlier is a response value that is unusually large or small. An extreme residual suggests an outlier and standardised residuals can be used to assess it. However if the outlier corresponds to an extreme x-value (a high leverage point) it may not show up as a large residual.</p>
<h3>11.5.6 &nbsp; Non-normal errors</h3>
<p>The errors in a normal linear model are assumed to have normal distributions. Violation of this assumption is less important than nonlinearity, non-constant variance or outliers, but a probability plot of the residuals can be used to assess normality.</p>
<h3>11.5.7 &nbsp; Correlated errors</h3>
<p>The errors in a normal linear model are assumed to be independent. In data where the observations are recorded sequentially, successive errors are sometimes found to be correlated. Correlated errors can arise whatever the x-variable, but are most often seen when the x-variable is time itself.</p>
<h2>11.6 &nbsp; Least squares for Y vs (X and Z)</h2>
<h3>11.6.1 &nbsp; More than one explanatory variable</h3>
<p>In many data sets, two or more explanatory variables could potentially affect the response. Using two or more explanatory variables may give more accurate predictions.</p>
<h3>11.6.2 &nbsp; Three-dimensional scatterplots</h3>
<p>Data sets with two explanatory variables and a response can be effectively displayed in a rotating 3-dimensional scatterplot.</p>
<h3>11.6.3 &nbsp; Linear equation and least squares plane</h3>
<p>The simple linear model can be extended by adding another linear term involving a second explanatory variable. This equation represents a plane in 3-dimensions.</p>
<h3>11.6.4 &nbsp; Understanding the parameters</h3>
<p>The intercept is the predicted y-value when x and z are zero. The two slope parameters give the predicted change in y when x and z increase by one.</p>
<h3>11.6.5 &nbsp; Fitted values and residuals</h3>
<p>The linear model provides predictions at all x- and z-values. The prediction for the x- and z-value corresponding to the i'th data point is its fitted value and the difference between this and the recorded y-value is its residual.</p>
<h3>11.6.6 &nbsp; Estimating the parameters</h3>
<p>Parameter estimates that result in small residuals are good.</p>
<h3>11.6.7 &nbsp; Least squares estimation</h3>
<p>An objective estimation method is to minimise the sum of squared residuals -- the principle of least squares.</p>
<h3>11.6.8 &nbsp; Interpreting the coefficients</h3>
<p>The slope coefficients give the predicted effect of changes to one variable, but only when the other variable remains the same.</p>
<h2>11.7 &nbsp; Normal linear model and inference</h2>
<h3>11.7.1 &nbsp; Normal linear model</h3>
<p>Randomness can be modelled by assuming that the response has a normal distribution whose mean is a linear function of the explanatory variables and whose standard deviation is constant.</p>
<h3>11.7.2 &nbsp; Sampling variability of least squares plane</h3>
<p>The normal linear model also implies that the least squares plane varies from sample to sample.</p>
<h3>11.7.3 &nbsp; Distribution of estimated coefficients</h3>
<p>The least squares estimate of each coefficient has a normal distribution whose mean is the underlying population parameter.</p>
<h3>11.7.4 &nbsp; Estimate of error standard deviation</h3>
<p>The error variance is estimated by the sum of squared residuals divided by (n-3). The best estimate of the error standard deviation is the square root of this.</p>
<h3>11.7.5 &nbsp; 95% confidence intervals for coefficients</h3>
<p>The standard deviation of each parameter estimate depends on the error standard deviation. Replacing this with an estimate allows us to find a 95% confidence interval. </p>
<h3>11.7.6 &nbsp; Hypothesis tests for coefficients</h3>
<p>A t test-statistic can be found by dividing a parameter estimate by its standard error. The p-value for testing whether the parameter is zero is the tail-area of a t distribution with (n-3) degrees of freedom.</p>
</html>
