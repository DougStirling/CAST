<html>
<head>
<title>15. Comparing Means</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../../structure/tocPrintStyles.css" type="text/css">
<script type='text/javascript'>
	function toggleDescriptions() {
		var showNotHide = document.getElementById('descriptionCheck').checked;
		var descriptions = document.getElementsByTagName('p');
		for (var i=0 ; i<descriptions.length ; i++)
			descriptions[i].style.display = showNotHide ? 'block' : 'none';
	}
</script>
</head>

<body>
<div style='position:absolute; top:3em; right:5; color:#FF0000; border:solid 1px #FF0000; background-color:#FFFFCC; padding:4px; margin:0px; line-height:0.8em'>
<input type='checkbox' id='descriptionCheck' checked onChange='toggleDescriptions()'/>Long page<br>descriptions
</div>
<h1>Chapter 15 &nbsp; Comparing Means</h1>
<h2>15.1 &nbsp; Models for two groups</h2>
<h3>15.1.1 &nbsp; Interest in underlying population</h3>
<p>As with single-group data, the populations underlying two-group data sets are usually of more interest than the specific sample data.</p>
<h3>15.1.2 &nbsp; Model for two groups</h3>
<p>Two-group data sets are often modelled as separate random samples from two normal populations.</p>
<h3>15.1.3 &nbsp; Parameters of the normal model</h3>
<p>The normal model has four parameters — the means and standard deviations in the two groups.</p>
<h3>15.1.4 &nbsp; Parameter estimates</h3>
<p>The parameters of the normal model can be estimated by the sample means and standard deviations in the two groups.</p>
<h3>15.1.5 &nbsp; Difference between means</h3>
<p>The difference between the population means is of particular interest. The difference between the sample means provides an estimate. It varies from sample to sample and has a distribution.</p>
<h2>15.2 &nbsp; Distn of sums and differences</h2>
<h3>15.2.1 &nbsp; Means and sums of samples</h3>
<p>The mean of a random sample is approximately normal with s.d. equal to σ divided by √n. The sum of a random sample is also approximately normal, but its s.d. is σ times √n.</p>
<h3>15.2.2 &nbsp; Sum and difference</h3>
<p>The sum and difference of two independent normal variables is also normally distributed. If they have the same standard deviation, σ, the  sum and difference  both have standard deviation 1.414σ. (Their variance is 2σ².)</p>
<h3>15.2.3 &nbsp; Sum and difference (cont)</h3>
<p>This page generalises the results to the sum and difference of variables whose standard deviations may be different.</p>
<h3>15.2.4 &nbsp; Probabilities for sums and differences</h3>
<p>If two variables are independent and have normal distributions, probabilities relating to their sum and difference can be found using the formulae for the mean and standard deviation of sums and differences.</p>
<h2>15.3 &nbsp; Comparing means in two groups</h2>
<h3>15.3.1 &nbsp; Distn of difference between means</h3>
<p>The difference between the means of two samples from normal populations has a normal distribution whose mean and s.d. can be found from the population means and s.d.s. This is the approximate distribution even when the populations are non-normal.</p>
<h3>15.3.2 &nbsp; SE of difference between means</h3>
<p>When the difference between the sample means is used to estimate the difference between the underlying population means, there is likely to be an error. The error distribution is approximately normal with mean 0. A formula for its standard deviation is given.</p>
<h3>15.3.3 &nbsp; CI for difference between means</h3>
<p>A 95% confidence interval is given for the difference between two population means. Its properties are demonstrated.</p>
<h3>15.3.4 &nbsp; Testing a hypothesis</h3>
<p>A hypothesis test is developed for testing whether two group means are the same.</p>
<h3>15.3.5 &nbsp; One-tailed tests for differences</h3>
<p>If the alternative hypothesis is for one particular mean to be greater, then the p-value for the test is found from only one tail of the t distribution.</p>
<h2>15.4 &nbsp; Paired t test</h2>
<h3>15.4.1 &nbsp; Paired data</h3>
<p>Paired data are a type of bivariate data in which two similar measurements are made from each individual. We are usually interested in testing whether the means of both measurements are the same.</p>
<h3>15.4.2 &nbsp; Analysis of differences</h3>
<p>For paired data, differences between the two measurements hold all information about whether the means of both variables are the same.</p>
<h3>15.4.3 &nbsp; Paired t-test</h3>
<p>Testing for a difference between the means of the measurements is done with an ordinary t-test for whether the mean difference is zero.</p>
<h3>15.4.4 &nbsp; Pairing and experimental design</h3>
<p>To estimate or test the difference between two means, it is sometimes possible to collect data from two independent samples or from paired units. If the paired units are similar, a pair data gives more accurate results.</p>
<h2>15.5 &nbsp; Comparing several means</h2>
<h3>15.5.1 &nbsp; Model</h3>
<p>To compare the means of several groups, a model of normal distributions in all groups is used but all group standard deviations must be assumed to be the same.</p>
<h3>15.5.2 &nbsp; Parameter estimates</h3>
<p>The sample standard deviations in the separate groups can be combined to give a pooled estimate of the common standard deviation, σ.</p>
<h3>15.5.3 &nbsp; Inference about two groups (opt) (Optional (not examined))</h3>
<p>Earlier CIs and tests for equality of two group means can be improved when the group standard deviations are known to be the same. However this refinement is not recommended for general use.</p>
<h3>15.5.4 &nbsp; Variation between and within groups</h3>
<p>Both variability between group means and variability within groups must be used to assess whether the groups differ.</p>
<h3>15.5.5 &nbsp; Sums of squares</h3>
<p>Variability within groups and between groups are described by sums of squares.</p>
<h3>15.5.6 &nbsp; Coefficient of determination</h3>
<p>The coefficient of determination (R-squared) is the ratio of the between-groups and total sums of squares. It is the proportion of variation that can be explained by differences between the groups.</p>
<h3>15.5.7 &nbsp; Test for differences between groups</h3>
<p>The F-ratio is a test statistic that is based on the between- and within-groups sums of squares. The associated p-value tests whether all groups have the same mean.</p>
<h3>15.5.8 &nbsp; Examples</h3>
<p>The F-test is applied to a few data sets.</p>
<h2>15.6 &nbsp; Exercises</h2>
<h3>15.6.1 &nbsp; Probabilities about differences  (Optional) (Optional)</h3>
<p>This page asks for a probability about either the difference between two single variables or two group means.</p>
<h3>15.6.2 &nbsp; CI for difference between two means</h3>
<p>Two exercises on this page ask confidence intervals about the difference between two group means based on sample means and standard deviations; the second is a little harder than the first.</p>
<h3>15.6.3 &nbsp; Testing two group means for equality</h3>
<p>This page gives an exercise in which you will find p-values for hypothesis tests about equality of two means and interpreting them.</p>
<h3>15.6.4 &nbsp; 2-sample or paired t-test</h3>
<p>This exercise presents 3 different scenarios and asks whether the two means in each scenario should be compared with a 2-sample or a paired t-test.</p>
<h3>15.6.5 &nbsp; Conduct paired t-test</h3>
<p>In this exercise, you are presented with a set of paired data and asked to perform a test about whether the two means are equal.</p>
<h3>15.6.6 &nbsp; Anova table formulae</h3>
<p>The exercise on this page shows four formulae relating to an Anova table and ask you to match them with textual descriptions of how they are interpreted.</p>
<h3>15.6.7 &nbsp; Anova table values</h3>
<p>This exercise gives an analysis of variance table and asks you to find a few values from it.</p>
<h3>15.6.8 &nbsp; Conclusion from Anova table</h3>
<p>This exercise shows an analysis of variable table and asks for the conclusion, described in the context of the problem.</p>
</body>
</html>
