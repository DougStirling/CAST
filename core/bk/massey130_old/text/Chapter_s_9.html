<html>
<head>
  <title>9. Sample Means &amp; Proportions</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="../../../structure/printStyles.css" type="text/css">
  <link rel="stylesheet" href="../../../structure/summaryStyles.css" type="text/css">
  <script src="../../../structure/videoControls/jquery.js"></script>
  <script src="../../../structure/printFixes.js"></script>
</head>

<body id="body" onLoad="showPrintDialog(true)">
<div id='overlay'>
	<div id='dialogWindow'>
		<div class='printDialog'>
			<script type='text/javascript'>
				document.write("<div class='heading'>" + top.document.title + "</div>");
				if (top.url != null) {
					document.write("<p class='text'>A version of this chapter has already been generated in PDF format and we recommend that it is used for printing. The button below will download and display it.</p>");
					document.write("<p><button onClick='top.showPdf()'>Show PDF version of chapter</button></p>");
					document.write("<p class='text'>However downloading could be slow depending on your internet connection. If this is a problem, click the button below to print the chapter without downloading (but perhaps not formatted as well as the PDF version).</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>If you are <strong>not</strong> using the PDF version, the best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
				else {
					document.write("<p class='text'>Click the button below to print this chapter.</p>");
					document.write("<p><button onClick='top.doPrint()'>Show print dialog</button></p>");
					document.write("<p class='text'>The best print results are obtained if the text is reduced in size and printed on  sheets of paper that are smaller than A4. This can be done using your browser's Page Setup command to scale by 71% and then printing on A5 paper.</p>");
				}
			</script>
			
			<p class='text'>If you don't want to print now,</p>
			<p><button onClick='top.showPrintDialog(false)'>Browse formatted chapter</button></p>
		</div>
	</div>
</div>
<h1 class="chapterName">Chapter 9 &nbsp; Sample Means &amp; Proportions</h1>
<h1 class="sectionName">9.1 &nbsp; Samples from distributions</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Data as representatives</li>
<li>Randomness of data</li>
<li>Model to explain randomness</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='4'>
<li>Infinite populations (distributions)</li>
<li>Information from a sample</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">9.1.1 &nbsp; Data as representatives</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Generalising from data</p>
	<p>Most data sets do <strong>not</strong> arise from randomly sampling individuals from a finite population. However we are still rarely interested in the specific individuals from whom data 
		were collected.</p>
	
<div class="centred"><div class="boxed">
<p>The recorded data are often 'representative' of something 
					more general.</p>
</div></div>

	<p>The main aim  is to <strong>generalise from the data</strong>.</p>
	<p class="eqn"><img src="../../../en/infPopn/images/silkworms.gif" width="559" height="209" class="summaryPict"></p>

  
  


<h2 class="pageName">9.1.2 &nbsp; Randomness of data</h2><!DOCTYPE HTML>


<p class=heading>Randomness of data</p>
	<p>Not only do we usually have little interest in the specific individuals from whom data were collected, but we must also acknowledge that our data would have been different if, by chance, we had selected different individuals or even made our measurements at a different time.</p>
	<p>We must acknowledge 
		this sample-to-sample variability when interpreting the data. The data are <strong>random</strong>.</p>
	
<div class="centred"><div class="boxed">
<p>All graphical and numerical summaries would 
					be different if we repeated data collection.</p>
</div></div>

	<p>This randomness in the data must be taken into account when we interpret graphical and numerical summaries. Our conclusions should not be dependent on features  that are specific to our particular data but would (probably) be different if the data were collected again.</p>
	<p>The more data that we collect, the more accurately our  data will reflect population characteristics, but randomness always exists.</p>




<h2 class="pageName">9.1.3 &nbsp; Model to explain randomness</h2><!DOCTYPE HTML>


<p class=heading>Data that are <span class="red">not</span> sampled from a finite population</p>
	<p>There is   no real finite population underlying most data sets   from which the values can be treated 
		as being sampled. The randomness in such data must be explained in a different way.</p>
	<p class=heading>Sampling from an abstract  population</p>
	<p>&quot;Random sampling from a population&quot; is  also used to explain variability even when there is no <strong>real</strong> finite population from which the data were sampled.</p>
	<p>We imagine an <strong>abstract</strong> population of <strong>all values that might have been obtained</strong> if the data collection had been repeated. We can then treat the observed data as a random sample from this abstract population.</p>
	<p>Defining such an underlying population therefore not only explains sample-to-sample variability but also gives us a focus for generalising from  our specific data.</p>




<h2 class="pageName">9.1.4 &nbsp; Infinite populations (distributions)</h2><!DOCTYPE HTML>


<p class=heading>Distributions</p>
	<p>When an abstract population is imagined to underlie a data set, it  often
contains an infinite number of values. For example, consider the lifetimes of
a sample of light bulbs. The population of <strong>possible</strong> failure
times contains <strong>all</strong> values greater than zero, and this includes
an infinite number of values. Moreover, some of these possible values will be
more likely than others.</p>
	<p>This kind of underlying population is called a <strong>distribution</strong>.</p>
	<p class=heading>Positions of cow in a field</p>
	<p>Consider the positions of a cow in a field at 6 different times where all locations are equally likely.</p>
	<p class="eqn"><img src="../../../en/infPopn/images/s_cows.gif" width="400" height="377"></p>
	<p>The population here contains all possible positions and is therefore infinite.</p>
	<p>The idea of a distribution also allows for some possible values to be more likely than others &mdash; the cow may be more likely to be in some particular part of the field.</p>


  


<h2 class="pageName">9.1.5 &nbsp; Information from a sample</h2><!DOCTYPE HTML>


<p class="heading">Sampling from a population</p>
	<p>Sampling from an underlying population (whether finite or infinite) gives us a mechanism to explain the randomness of data. The underlying population also gives us a <strong>focus</strong> for generalising from our sample data &mdash; the distribution of values in the population is fixed and does not depend on the specific sample data.</p>
	<p class="heading">Unknown population</p>
	<p>Unfortunately the population underlying most data sets is unknown and, in practice, we only have a <strong>single sample</strong>. However this single sample does throw light on the population distribution.</p>
	<p>The diagram below describes a sample from a categorical distribution. Although the underlying population is unknown, the sample proportion of successes, <em>p</em>, is an <strong>estimate</strong> of the unknown proportions of successes in the population (denoted by π).</p>
	<p class="eqn"><img src="../../../en/infPopn/images/s_unknownP.gif" width="326" height="301"></p>




<h1 class="sectionName breakBefore">9.2 &nbsp; Probability &amp; probability density</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Finite populations</li>
<li>Probabilities with infinite populations</li>
<li>Bar charts of discrete probabilities</li>
<li>Probability density functions</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='5'>
<li>Normal distributions</li>
<li>Probability and area under the pdf</li>
<li>Properties of probability</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">9.2.1 &nbsp; Finite populations</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Probabilities for a finite population</p>
	<p>Random sampling from populations is described using <strong>probability</strong>. 
		If one value is sampled from a finite population of <em>N</em> distinct values, 
		we say that </p>
	<ul>
		<li>each population value has probability <sup>1</sup>/<sub><em>N</em></sub> of 
			being selected.</li>
		<li>there is zero probability of obtaining a value that is not in the population.</li>
	</ul>
	<p>Many populations contain values that occur more than once. When sampling from <strong>any</strong> population,</p>
	<div class="centred"><div class="boxed">
		<p>The probability that a single sampled value is either <em>x, <em>y</em>, ...</em> is the <strong>proportion 
			of population values that are either <em>x</em>, <em>y</em>, ... </strong>.</p>
	</div></div>
	<p>For numerical populations, the most useful form  of this result is:</p>
	<div class="centred"><div class="boxed">
		<p>Prob( <em>a</em> &lt; <em>X</em> &lt; <em>b</em> ) &nbsp; = &nbsp; propn of values between <em>a</em> and <em>b</em>.</p>
	</div></div>




<h2 class="pageName">9.2.2 &nbsp; Probabilities with infinite populations</h2><!DOCTYPE HTML>


<p class="heading">Probability and population proportion</p>
	<p>When sampling from any population, whether finite or infinite,</p>
	<div class="centred"><div class="boxed">
		<p>The probability of sampling any value or range of values equals the <strong>proportion of these values in the population</strong>.</p>
	</div></div>
	<p class="heading">Probability and long-term proportion</p>
	<p>An alternative but equivalent way to think about probability arises when we can imagine repeatedly selecting more and more values from the population (e.g. repeating an experiment). The probability of any value or range of values is the <strong>limiting proportion of these values as the sample size increases</strong>.</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_largeNos.gif" width="493" height="308"></p>
	<p>The equivalence of the two definitions is called the <strong>law of large numbers</strong>.</p>




<h2 class="pageName">9.2.3 &nbsp; Bar charts of discrete probabilities</h2><!DOCTYPE HTML>


<p class="heading">Describing   categorical and discrete populations</p>
	<p>Categorical and discrete samples can be described graphically with bar charts of the proportions for the distinct values. Since probabilities are defined to be population proportions, the underlying population can also be described by a bar chart.</p>
	<p class="eqn"><img class="gif" src="../../../en/probDensity/images/barChart.gif" id="gif_image_2_3_1" width="206" height="194"><iframe class="svg" src="../../../en/probDensity/images/barChart.svg" id="svg_image_2_3_1" width="206" height="194" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_2_3_1");</script></p>
	<p class="heading">Bar charts and the law of large numbers</p>
	<p>The law of large numbers states that sample proportions approach  the underlying probabilities as the sample size increases. This means that a sample bar chart will be close in shape to the unknown population bar chart if the sample size is big enough.</p>




<h2 class="pageName">9.2.4 &nbsp; Probability density functions</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Histograms and probability density functions</p>
	<p>The situation is a little more complicated for continuous numerical populations and samples. A standard histogram could be used to describe the population in the same way that it might be used for a sample:</p>
	<p class="eqn"><img class="gif" src="../../../en/probDensity/images/histogram.gif" id="gif_image_2_4_1" width="192" height="154"><iframe class="svg" src="../../../en/probDensity/images/histogram.svg" id="svg_image_2_4_1" width="192" height="154" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_2_4_1");</script></p>
	<p>However with an infinite population, we can narrow the histogram classes beyond what would be reasonable for a finite sample. Indeed, class widths can be reduced indefinitely, resulting in a smooth histogram called a <strong>probability density function</strong>. This is often abbreviated to a <strong>pdf</strong>.</p>
	<p class="eqn"><img class="gif" src="../../../en/probDensity/images/pdf.gif" id="gif_image_2_4_2" width="192" height="154"><iframe class="svg" src="../../../en/probDensity/images/pdf.svg" id="svg_image_2_4_2" width="192" height="154" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_2_4_2");</script></p>
	<p>Probability density functions are still essentially histograms and share all properties of histograms.</p>




<h2 class="pageName">9.2.5 &nbsp; Normal distributions</h2><!DOCTYPE HTML>


<p class="heading">Shape of a probability density function</p>
	<p>A probability density function  is usually a fairly smooth curve, though a single sample histogram  provides limited information about its likely shape.</p>
	<p class=eqn><img src="../../../en/probDensity/images/unknownPopn.gif" alt="sample &mdash;> popn?" width="362" height="236" class="summaryPict"> </p>
	<p class="heading">Normal distributions</p>
	<p>One  flexible group of  continuous probability density functions is the family of <strong>normal 
		distributions</strong>. Normal distributions:</p>
	<ul>
		<li>Are symmetric.</li>
		<li>Have a parameter called µ that is the mean of the population.</li>
		<li> Have a parameter σ that is the population's standard deviation.
</li>
	</ul>
	<p>Changing the parameters µ and σ changes where the distribution is centred and its spread, but its shape remains otherwise the same.</p>
	<p>The parameters are often <strong>estimated</strong> from a sample. Details will be given later, but the resulting normal pdf will be close in shape to a histogram of the sample data.</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_bestFit.gif" width="404" height="202"></p>




<h2 class="pageName">9.2.6 &nbsp; Probability and area under the pdf</h2><!DOCTYPE HTML>


<p class=heading>Probabilities from a histogram</p>
	<p>In the histogram of any finite sample or population, the area above any class <a href="javascript:showNamedPage('density5')">is 
		the proportion of values in the class</a>.</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_histoArea.gif" width="455" height="254"></p>
	<p class="heading">Probabilities from a probability density function</p>
	<p>Since a probability density function (pdf)  is a type of histogram, it satisfies the same property.</p>
	<div class="centred"><div class="boxed">
		<p>The probability that a sampled value is within two values, P(<em>a</em>&nbsp;&lt;&nbsp;<em>X</em>&nbsp;&lt;&nbsp;<em>b</em>), equals the area under the pdf.</p>
	</div></div>
	<p>This is the key to interpreting pdfs.</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_pdfArea.gif" width="402" height="212"></p>




<h2 class="pageName">9.2.7 &nbsp; Properties of probability</h2><!DOCTYPE HTML>


<p>For any events, <em>A</em> and <em>B</em>, the following properties always hold.</p>
	<p class="heading">Probabilities are always between 0 and 1</p>
	<div class="centred"><div class="boxed">
		<p>0  ≤  P(<em>A</em>)  ≤  1</p>
	</div></div>
	<p class="heading">Meaning of probabilities 0 and 1</p>
	<div class="centred"><div class="boxed">
		<p>If the event <em>A</em> cannot happen then P(<em>A</em>)  =  0</p>
		<p>If the event <em>A</em> is certain to happen then P(<em>A</em>)  =  1</p>
	</div></div>
	<p class="heading">Probability that an event does not happen</p>
	<div class="centred"><div class="boxed">
		<p>P(<em>A</em> does not happen)  =  1 - P(<em>A</em>)</p>
	</div></div>
	<p class="heading">Addition law</p>
	<p>When two events cannot happen together, they are said to be <strong>mutually 
		exclusive</strong>. If <em>A</em> and <em>B</em> are mutually exclusive,</p>
	<div class="centred"><div class="boxed">
		<p>P(<em>A</em> or <em>B</em>)  =  P(<em>A</em>) + P(<em>B</em>)</p>
	</div></div>
	<p>If the events <em>A</em> and <em>B</em> are <strong>not</strong> mutually exclusive,</p>
	<div class="centred"><div class="boxed">
		<p>P(<em>A</em> or <em>B</em>)  &lt;  P(<em>A</em>) + P(<em>B</em>)</p>
	</div></div>
	<p class="heading">Independence</p>
	<p>When sampling  <strong>with replacement</strong> from a finite population, the choice of each value does not depend on the values previously 
		selected. The successive values are then called <strong>independent</strong>. This also holds when sampling from an infinite population (distribution).</p>
	<p>On the other hand, if sampling without replacement from a finite population, 
		successive sample values are <strong>not</strong> independent since the second value 
		selected cannot be the same as the first value, so knowing the first value affects 
		the probabilities when the second value is selected.</p>




<h1 class="sectionName breakBefore">9.3 &nbsp; Distribution of sample mean</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Variability of sample statistics</li>
<li>Variability of sample mean</li>
<li>Standard devn of sample mean</li>
<li>Means from normal populations</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='5'>
<li>Large-sample normality of means</li>
<li>Distribution of mean from a sample</li>
<li>Requirement of independence</li>
<li>Sampling from finite populations</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">9.3.1 &nbsp; Variability of sample statistics</h2><!DOCTYPE HTML>


<p class="heading">Sampling mechanism</p>
	<p>The mechanism of sampling from a population explains randomness in data.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_samplingHistos.gif" width="223" height="184"></p>
	<p>In practice, we must use  a <strong>single</strong> sample to find information about the population.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_oneHisto.gif" width="221" height="70"></p>
	<p class="heading">Parameters and statistics</p>
	<p>We usually focus attention on a small number of numerical characteristics.</p>
	<ul>
		<li>Populations are summarised by values called <strong> parameters</strong>.</li>
		<li>The corresponding sample values are <strong>sample statistics</strong> and provide <strong>estimates</strong> of the parameters.</li>
	</ul>
	<p class="eqn"><img src="../../../en/randomMean/images/s_oneMean.gif" width="221" height="70"></p>
	<p class="heading">Variability of sample statistics</p>
	<p>The variability in random samples also implies sample-to-sample variability in sample statistics.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/s_samplingMeans.gif" width="223" height="183"></p>




<h2 class="pageName">9.3.2 &nbsp; Variability of sample mean</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Distribution of the sample mean</p>
	<p>The mean of a random sample of <em>n</em> values is a random quantity. Its
distribution  is centred on the population mean but its spread is lower then
that of the population distribution.</p>


<h2 class="pageName">9.3.3 &nbsp; Standard devn of sample mean</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Centre and spread of the sample mean's distribution</p>
	<ul>
		<li>The sample mean has a distribution that is centred on the 
			population mean.</li>
		<li>Its variability decreases as the sample size increases. </li>
	</ul>
<p>We can be more precise. If the population has mean µ and standard deviation σ, then  the   mean of a sample of <em>n</em> values, <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">, 
		has a distribution with mean and standard deviation:</p>
<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> </p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>




<h2 class="pageName">9.3.4 &nbsp; Means from normal populations</h2><!DOCTYPE HTML>


<p class="heading">Shape of the mean's distribution</p>
	<p><strong>Whatever</strong> the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
	<p>However skewness in the population distribution 
		leads to some <strong>skewness in the distribution of the mean</strong>.</p>
	<p class="heading">Samples from normal populations</p>
	<p>When the population distribution is normal, the sample mean 
	also has a normal distribution.</p>
	<p class=eqn><span class="black"><span style="position:relative; top:-12px"><img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline">&nbsp; ~ &nbsp;<font face="Arial, Helvetica, sans-serif">normal</font> (μ , &nbsp;</span><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"><span style="position:relative; top:-12px">)</span></span></p>
	<br>
	<p class=eqn><img src="../../../en/randomMean/images/s_normalMean.gif" width="428" height="344"></p>




<h2 class="pageName">9.3.5 &nbsp; Large-sample normality of means</h2><!DOCTYPE HTML>


<p class="heading">Means from non-normal populations</p>
	<p>Irrespective of the shape of the population distribution,</p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
<p>If the population is not a normal distribution, the sample mean does not 
		have a normal distribution. However  the <strong>Central Limit 
		Theorem</strong> states that...</p>
<div class="centred"><div class="boxed"><p>For most non-normal population distributions, the distribution of 
				the sample mean becomes close to normal when the sample size increases.</p></div></div>
<p class="eqn"><img src="../../../en/randomMean/images/s_gammaMean.gif" width="428" height="344"></p>




<h2 class="pageName">9.3.6 &nbsp; Distribution of mean from a sample</h2><!DOCTYPE HTML>


<p class="heading">Need for multiple values to assess variability</p>
	<p>We usually need to make two or more measurements of a variable 
		to get any information about its variability. A single value contains no information about the quantity's variability.</p>
<p class="heading">Achieving the impossible?</p>
<p>Fortunately, <strong>we do not need multiple sample</strong> means to assess the variability of a sample mean. Its distribution can be estimated from a <strong>single sample</strong> using </p>
	<p class=eqn><span style="position:relative; top:6px"><img src="../../../en/../images/symbol.muXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=&nbsp; &mu;</span> </p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span> </p>
	<p>The distribution of the mean can be approximated with a normal distribution with this mean and standard deviation, if we replace µ 
	and σ with <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"> and <em>s</em>.</p>
	<p class="eqn"><img src="../../../en/randomMean/images/yamGrowth.gif" width="540" height="469" class="summaryPict"></p>




<h2 class="pageName">9.3.7 &nbsp; Requirement of independence</h2><!DOCTYPE HTML>


<p class="heading">Independent random samples</p>
	<p>The formula for the standard deviation of a sample mean,</p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
<p>is only accurate if the sample values are <strong>independent</strong>.</p>
<p class="heading">Dependent random samples</p>
	<p>When  sample values are correlated with each other, they are said to be <strong>dependent</strong> and the formula</p>
	<p class="eqn"><img class="gif" src="../../../en/randomMean/images/estSDMean.gif" id="gif_image_3_7_1" width="64" height="32"><iframe class="svg" src="../../../en/randomMean/images/estSDMean.svg" id="svg_image_3_7_1" width="64" height="32" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_3_7_1");</script></p>
	<p> can badly underestimate the variability (and hence accuracy) of the sample 
		mean of dependent random samples.</p>
	<p>Always check that a random sample is independently selected from the whole 
		population before using the formula for the standard deviation of the sample mean.</p>




<h2 class="pageName">9.3.8 &nbsp; Sampling from finite populations</h2><!DOCTYPE HTML>


<p class="heading">Sampling with replacement from finite populations</p>
	<p>When  a random sample is selected <strong>with replacement</strong> from a finite 
		population, the sample values are independent and the standard 
		deviation of the sample mean is again</p>
	<p class=eqn><span style="position:relative; top:5px"><img src="../../../en/../images/symbol.sigmaXbar.png" width="19" height="16" align="baseline"></span> &nbsp;<span class="black">=</span>&nbsp; <span style="position:relative; top:12px"><img src="../../../en/../images/symbol.sigmaOverRootN.png" width="26" height="31" align="baseline"></span></p>
	<p>Note however that the population  standard deviation, σ, 
		uses divisor <em>N</em>, the number of values in the population, rather than (<em>N</em>&nbsp;-&nbsp;1).	</p>
	<p class=eqn><img class="gif" src="../../../en/randomMean/images/popnSdFinite.gif" id="gif_image_3_8_2" width="114" height="49"><iframe class="svg" src="../../../en/randomMean/images/popnSdFinite.svg" id="svg_image_3_8_2" width="114" height="49" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_3_8_2");</script></p>
<p class="heading">Sampling without replacement from finite populations</p>
	<p>When a sample is selected <strong>without replacement</strong>, successive values are no longer independent &mdash; if a large value is selected, it cannot be selected again, so the next value will tend 
	to be lower.</p>
	<p>For sampling without replacement, a different formula should be used for the standard deviation of the sample mean:</p>
	<p class=eqn><img class="gif" src="../../../en/randomMean/images/sdMeanSWOR.gif" id="gif_image_3_8_1" width="138" height="45"><iframe class="svg" src="../../../en/randomMean/images/sdMeanSWOR.svg" id="svg_image_3_8_1" width="138" height="45" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_3_8_1");</script></p>
	<p>The quantity (<em>N </em>- <em>n</em>) / (<em>N</em> - 1) is called the <strong>finite 
		population correction factor</strong>. It can usually be ignored if only a small fraction of the population is sampled (say under 5%).</p>




<h1 class="sectionName breakBefore">9.4 &nbsp; Normal distributions</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Importance of normal distributions</li>
<li>Shape of normal distributions</li>
<li>Sketching a normal distribution</li>
<li>Some normal probabilities</li>
<li>Z-scores</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='6'>
<li>Finding normal probabilities</li>
<li>Other probabilities</li>
<li>Normal tables</li>
<li>Finding normal quantiles</li>
<li>Normal probability plots</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">9.4.1 &nbsp; Importance of normal distributions</h2><!DOCTYPE HTML>


<p class="heading">Normal distribution parameters</p>
	<p>The <a href="javascript:showNamedPage('probDensity5')">family of 
		normal distributions</a> consists of symmetric bell-shaped distributions that 
		are defined by two parameters, µ 
		and σ, the distribution's mean and standard deviation.</p>
	<p class="heading">Normal distributions as models for data</p>
	<p class="eqn"><img src="../../../en/probDensity/images/s_bestFit.gif" width="404" height="202"></p>
	<p>The sample data rarely gives enough information for us to be <strong>sure</strong> that the underlying population is  normal, but a normal model is often used unless there is <strong>obvious</strong> non-normality in the data.</p>
<p>Even if the sample data are obviously skew, a normal distribution may be a reasonable
model for a nonlinear transformation of the values (e.g. a log transformation).</p>
<p class="heading">Distribution of summary statistics</p>
	<p>A more important reason for the importance of the normal distribution in statistics 
		is that...</p>
	
<div class="centred"><div class="boxed">
<p>Many summary statistics have normal distributions (at least approximately).</p>
</div></div>

	<p>The Central Limit Theorem shows that the mean of a random sample has a distribution that is close to normal when the sample size is moderate or large, <strong>irrespective 
	of the shape of the distribution of the individual values</strong>. The following are also 
	approximately normal when the sample size is moderate or large...</p>
	<ul>
		<li>A sample proportion</li>
		<li>The slope and intercept of a least squares line</li>
		<li>The difference between the means of two samples</li>
	<li>The difference between two proportions</li></ul>




<h2 class="pageName">9.4.2 &nbsp; Shape of normal distributions</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Effect of normal parameters on distribution</p>
	<p>Distributions from the normal family have different locations and spreads, 
		but other aspects of their shape are the same. Indeed, if the scales on the horizontal 
		and vertical axes are suitably chosen, ...</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_normalAxes.gif" width="514" height="295"></p>





<h2 class="pageName">9.4.3 &nbsp; Sketching a normal distribution</h2><!DOCTYPE HTML>


<p class="heading">A common diagram for <span class="red">all</span> normal 
		distributions</p>
	<p>All normal distributions have basically the same shape.</p>
	<ul>
		<li>The distribution almost disappears at 3σ 
			from µ</li>
		<li>The probability (area) further than 2σ 
			from µ 
			is small &mdash; only about <sup>1</sup>/<sub>20</sub> of the total area.</li>
	</ul>
	<p>This should allow you to sketch a normal distribution, given any values of µ and σ.</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_sketch.gif" width="505" height="237"></p>




<h2 class="pageName">9.4.4 &nbsp; Some normal probabilities</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Some probabilities for normal distributions</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_empiricalRule.gif" width="499" height="553"></p>
	<p>A more precise version of the middle probability is</p>
	<ul>
		<li>P&nbsp;(within 1.96σ 
			of µ) 
			&nbsp;=&nbsp; 0.95</li>
	</ul>
	<p class="heading">70-95-100 rule of thumb and the normal distribution</p>
	<p>These probabilities are the basis of the <a href="javascript:showNamedPage('centerSpread7')">70-95-100 
		rule of thumb</a> for 
		'bell-shaped' data sets.</p>
	<ul>
		<li>About 70% of values are within <em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
		<li>About 95% of values are within 2<em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
		<li>Almost all values are within 3<em>s</em> of <img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></li>
	</ul>




<h2 class="pageName">9.4.5 &nbsp; Z-scores</h2><!DOCTYPE HTML>


<p class="heading">Standard deviations from the mean</p>
	<p>Any x-value can be expressed as a number of standard deviations from the mean &mdash; its <strong>z-score</strong>.</p>
	<p class=eqn><img src="../../../en/normalDistn/images/s_zScore.gif" width="498" height="145" alt="z = (x - mu) / sigma"> </p>
	<p>Mathematically, </p>
	<p class="eqn"><img class="gif" src="../../../en/normalDistn/images/standardiseEqn2.gif" id="gif_image_4_5_1" width="103" height="31"><iframe class="svg" src="../../../en/normalDistn/images/standardiseEqn2.svg" id="svg_image_4_5_1" width="103" height="31" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_4_5_1");</script></p>
	<p>or equivalently, </p>
	<p class=eqn><span class="black"><em>x</em> &nbsp;=&nbsp; &mu; &nbsp;+&nbsp; <em>z</em> &times; &sigma;</span> </p>
	<p class="heading">Probabilities and z-scores</p>
	<p>Any  probability (area) relating to a normally distributed random variable, <em>X</em>, can be expressed in terms of z-scores:</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_zProb.gif" width="502" height="298"></p>
	<p>Note in particular that:</p>
	<ul>
		<li>P(-1 &lt; z &lt; +1) &nbsp;is approx&nbsp;0.68</li>
		<li>P(-2 &lt; <em> z </em> &lt; +2) &nbsp;is approx&nbsp;0.95</li>
		<li>P(-3 &lt; <em>z</em> &lt; +3) &nbsp;is approx&nbsp;0.997</li>
	</ul>




<h2 class="pageName">9.4.6 &nbsp; Finding normal probabilities</h2><!DOCTYPE HTML>


<p class="heading">Distribution of z-scores</p>
	<p>Calculating a z-score from a value, <em>x</em>, is  called <strong>standardising</strong> it.</p>
<p class=eqn style="color:#000000; font-weight:bold">standardised value, &nbsp;&nbsp;&nbsp;&nbsp;<span style="position:relative; top:10px"><img class="gif" src="../../../en/normalDistn/images/standardiseEqn2.gif" id="gif_image_4_6_1" width="103" height="31"><iframe class="svg" src="../../../en/normalDistn/images/standardiseEqn2.svg" id="svg_image_4_6_1" width="103" height="31" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_4_6_1");</script></span></p>
<p>If <em>X</em> has a normal distribution, then <i>Z</i> has a <strong>standard 
		normal distribution</strong> with mean µ&nbsp;=&nbsp;0 
	and standard deviation σ&nbsp;=&nbsp;1. </p>
	<p class="heading">Probabilities for the standard normal distribution</p>
	<p>After translating a probability about <em>X</em> into one about a z-score, it is easier to evaluate it.</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_xProb.gif" width="509" height="436"></p>
	<p>Areas under the standard normal curve can be evaluated in Excel and most statistical programs. Statistical tables can also be used (see later).</p>




<h2 class="pageName">9.4.7 &nbsp; Other probabilities</h2><!DOCTYPE HTML>


<p class="heading">Evaluating other probabilities</p>
	<p>Other probabilities about normal distributions can be found using the following properties:</p>
	<ul>
		<li>The total area under a normal p.d.f. is 1</li>
		<li>The probability of a value in any interval is the area under the normal p.d.f. above this interval.</li>
	</ul>
	<p class="heading">Probability of higher value</p>
	<p class=eqn><img src="../../../en/normalDistn/images/normalAreas.gif" width="550" height="115"></p>
	<p class="heading">Probability of value between two others</p>
	<p class=eqn><img src="../../../en/normalDistn/images/normalAreas2.gif" width="506" height="241"></p>
	<p>In both cases, the conversion can be done either before or after translating the 
		required probability from x-values to z-scores.</p>




<h2 class="pageName">9.4.8 &nbsp; Normal tables</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Standard normal probabilities without a computer</p>
	<p>Probabilities about z-scores can be found <strong>without a computer</strong>. Most 
		introductory statistics textbooks contain printed tables with left-tail probabilities 
	for the standard normal distribution. </p>
	<p class=eqn><img src="../../../en/normalDistn/images/table.gif" width="482" height="176" alt="pdf = table"> </p>
	<p>These tables can be used after the required probability has been translated 
		into a problem relating to the standard normal distribution.</p>




<h2 class="pageName">9.4.9 &nbsp; Finding normal quantiles</h2><!DOCTYPE HTML>


<p class="heading">Finding an x-value from a probability</p>
	<dl>
		<dt>Quartiles</dt>
		<dd>The <strong>quartiles</strong> of a  distribution are the three values such that there is probability <sup>1</sup>/<sub>4</sub>, <sup>2</sup>/<sub>4</sub> and <sup>3</sup>/<sub>4</sub> of being lower.</dd>
		<dt>Percentiles</dt>
		<dd>The <em>r</em>'th percentile of the distribution is the value with probability <sup><em>r</em></sup>/<sub>100</sub> of being lower.</dd>
		<dt>Quantiles</dt>
		<dd>These are generalised by the term <strong>quantile</strong>. The value with probability <em>p</em> of being lower is called the quantile of the distribution corresponding to probability <em>p</em>.</dd>
	</dl>
	<p class="heading">Finding quantiles</p>
	<p>To find the x-value for which there is probability <em>p</em> of a normal distribution being lower,</p>
	<ul>
		<li>Find the z-score for which there is probability <em>p</em> of being less.</li>
		<li>Translate the z-score to an x-value</li>
	</ul>
	<p>The first step of this process can be done with Excel (or other statistical software) or statistical tables can be used. For example, the 
		diagram below shows how to find the z-score such that there is probability 
		0.9 of being less.</p>
	<p class=eqn><img src="../../../en/normalDistn/images/invTable.gif" width="482" height="176" class="summaryPict"> </p>
	<p>Translating from a z-score to the corresponding x-value is done with the 
		formula,</p>
	<p class=eqn><span class="black"><em>x</em> &nbsp;=&nbsp; &mu; &nbsp;+&nbsp; <em>z</em> &sigma;</span> </p>




<h2 class="pageName">9.4.10 &nbsp; Normal probability plots</h2><!DOCTYPE HTML>


<p class="heading">Do the data come from a normal distribution?</p>
	<p>A histogram  may indicate that a sample is unlikely to come from a normal distribution, but a <strong>normal probability plot</strong> can indicate more subtle departures from a normal distribution.</p>
	<ol>
		<li>Sort the data values into order, <em>x</em><sub>(1)</sub>&nbsp;&lt;&nbsp;<em>x</em><sub>(2)</sub>&nbsp;&lt;&nbsp;...&nbsp;&lt;&nbsp;<em>x</em><sub>(<em>n</em>)</sub></li>
		<li>Find ordered values that are spaced out as you would <strong>expect</strong> from a normal distribution, <em>q</em><sub>1</sub>&nbsp;&lt;&nbsp;<em>q</em><sub>2</sub>&nbsp;&lt;&nbsp;...&nbsp;&lt;&nbsp;<em>q</em><sub><em>n</em></sub>. 
			The quantiles of the normal distribution corresponding to probabilities <sup>1</sup>/<sub>(<em>n</em>+1)</sub>, <sup>2</sup>/<sub>(<em>n</em>+1)</sub>, ..., <sup><em>n</em></sup>/<sub>(<em>n</em>+1)</sub> are commonly used.</li>
		<li>Plot <em>x</em><sub>(<em>i</em>)</sub> against <em>q</em><sub><em>i</em></sub></li>
	</ol>
	<p>If the data set is from a normal distribution, the data should be spaced out in a similar way to the normal quantiles, so the crosses in the normal probability 
		plot should lie close to a straight line.</p>
	<p class="eqn"><img src="../../../en/normalDistn/images/s_probPlot.gif" width="359" height="357"></p>
	<p class="heading">How much curvature is needed to suggest non-normality?</p>
	<p>This is a difficult question to answer and we will not address it here.</p>





<h1 class="sectionName breakBefore">9.5 &nbsp; Distribution of sample proportion</h1>
<div class='leftTocCol'>
<ol class='toc'>
<li>Proportion and probability</li>
<li>Properties of counts and proportions</li>
<li>Binomial distribution</li>
</ol>
</div>
<div class='rightTocCol'>
<ol class='toc' start='4'>
<li>Binomial probability examples</li>
<li>Normal approximation to binomial</li>
<li>Normal approximation examples</li>
</ol>
</div>
<br clear='all'>
<h2 class="pageName">9.5.1 &nbsp; Proportion and probability</h2><!DOCTYPE HTML>


<p class="heading">A sample proportion has a distribution</p>
	<p>If a categorical data set is modelled as a random sample from a categorical 
		population, the sample proportions   must be treated as 
		random quantities &mdash; they vary from sample to sample.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_samplingPropns.gif" width="446" height="365"></p>
	<p>The population proportion in a category  is called 
		its <strong>probability</strong>, and is often denoted by π. 
		The corresponding sample proportion is usually denoted by <em>p</em>. </p>
	<div class="centred"><table class="centred" border="0" cellspacing="0" cellpadding="4">
<tr>
<th>&nbsp; </th>
<th align="CENTER">Sample Statistic</th>
<th align="CENTER">Population Parameter</th>
</tr>
<tr>
<th align="left">Mean</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top"><img src="../../../en/../images/symbol.xBar.png" width="10" height="10" align="baseline"></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top">µ</td>
</tr>
<tr>
<th align="left">Standard deviation</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top"><em>s</em></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top">σ</td>
</tr>
<tr>
<th align="left">Proportion/probability</th>
<td align="CENTER" bgcolor="#FFFFFF" class="top bottom"><em>p</em></td>
<td align="CENTER" bgcolor="#FFFFFF" class="top bottom">π</td>
</tr>
</table></div>
<p>In practice, we only have a single sample and must use it to get information about the underlying population.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_onePropn.gif" width="442" height="140"></p>




<h2 class="pageName">9.5.2 &nbsp; Properties of counts and proportions</h2><!DOCTYPE HTML>


<p class="heading">Properties of a sample proportion</p>
	<p>A sample proportion  from a random sample
	of size <i>n</i> has a distribution that ... </p>
	<ul>
		<li>is centred on the underlying population probability,
			π, and</li>
		<li>has a spread that decreases as the sample size <i>n</i> increases.</li>
	</ul>
	<p class="heading">Count and proportion of successes</p>
	<p>Although the sample proportion in a category, <span class="em black">p</span> , 
		is a good summary statistic, the raw count of sample values in the category, <span class="em black">x</span> <span class="black">&nbsp;=&nbsp;<em>np</em></span>, 
		contains equivalent information and is often easier to use. They have distributions with the same shape (other than the scaling constant <i><span class="black">n</span></i>).</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_propnDist.gif" width="550" height="286"></p>




<h2 class="pageName">9.5.3 &nbsp; Binomial distribution</h2><!DOCTYPE HTML>


<p class="heading">General notation</p>
	<p>In a categorical population, we choose one category of interest and call it <em><strong>success</strong></em>; all other categories are collectively called <em><strong>failures</strong></em>. The population proportion of successes is denoted by <span class="black">π</span>.</p>
	<p>When a random sample of <i><span class="black">n</span></i> values is selected, 
		we denote the number of successes by <span class="em black">x</span>  and the proportion of successes by <span class="em black">p</span> <span class="black">&nbsp;=&nbsp;<em><span style="vertical-align:20%">x</span></em>/<em><span style="vertical-align:-20%">n</span></em></span>. </p>
	<p class="heading">Distribution of a sample proportion</p>
	<p>The number of successes, <span class="em black">x</span> , 
		has a 'standard' discrete distribution called a <strong>binomial distribution</strong> which has two parameters, <span class="em black">n</span>  and <span class="black">π</span>.</p>
	<p>In practical applications, <span class="em black">n</span>  is a known constant, but <span class="black">π</span> may be unknown. The sample proportion, <span class="em black">p</span> , 
		has a distribution with the same shape, but is scaled by <span class="em black">n</span> .</p>
	<ul>
		<li>The distribution of <em>p</em> is centred on π.</li>
		<li>The spread of the distribution of <em>p</em> decreases as <em>n</em> increases.</li>
		<li>The distribution is symmetric when π 
			= 0.5, but becomes more skew as π 
			approaches 0 or 1.</li>
	</ul>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_binomials.gif" width="466" height="515"></p>




<h2 class="pageName">9.5.4 &nbsp; Binomial probability examples</h2><!DOCTYPE HTML>


<p class="heading">Assumptions underlying the binomial distribution</p>
	<ul>
		<li>Each observation has the same probability, <span class="black">π</span>, 
			of being a 'success'.</li>
		<li>Each of the <span class="em black">n</span>  observations is independently obtained.</li>
		<li>We record the number (or proportion) of successes in the <span class="em black">n</span>  observations.</li>
	</ul>
	<p class="heading">Evaluating binomial probabilities</p>
	<p>They may be obtained using ... </p>
	<ul>
		<li>a computer (preferred),</li>
		<li>a mathematical formula, or</li>
		<li>tables of binomial probabilites.</li>
	</ul>
	<p class="heading">A range of counts</p>
	<p>Finding the probability that the number of successes is within an interval involves adding the binomial probabilities for all integer values in the interval.</p>
	<p>Think carefully about the wording of the interval &mdash; does it include the values at the end? Adding or subtracting <sup>1</sup>/<sub>2</sub> to the endpoints of the interval makes it clearer. (This is also particularly useful when using the normal approximations that are described in the following pages.)</p>
	<div class="centred"><table border="0" class="centred" cellpadding="3" cellspacing="0">
<tr>
<th align="center" scope="col">In words...</th>
<th align="center" scope="col">&nbsp;&nbsp;&nbsp;&nbsp;In terms of X&nbsp;&nbsp;&nbsp;&nbsp;</th>
<th align="center" scope="col">&nbsp;&nbsp;&nbsp;&nbsp;Using <sup>1</sup>/<sub>2</sub>&nbsp;&nbsp;&nbsp;&nbsp;</th>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF" class="top">More than 5</td>
<td align="center" bgcolor="#FFFFFF" class="top">X &gt; 5</td>
<td align="center" bgcolor="#FFFFFF" class="top">X&nbsp;&gt;&nbsp;5.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">Greater than or equal to 5</td>
<td align="center" bgcolor="#FFFFFF">X ≥ 5</td>
<td align="center" bgcolor="#FFFFFF">X &gt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">No more than 5</td>
<td align="center" bgcolor="#FFFFFF">X ≤ 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 5.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">At least 5</td>
<td align="center" bgcolor="#FFFFFF">X ≥ 5</td>
<td align="center" bgcolor="#FFFFFF">X &gt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF">Fewer than 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 5</td>
<td align="center" bgcolor="#FFFFFF">X &lt; 4.5</td>
</tr>
<tr>
<td align="center" bgcolor="#FFFFFF" class="bottom">5 or fewer</td>
<td align="center" bgcolor="#FFFFFF" class="bottom">X ≤ 5</td>
<td align="center" bgcolor="#FFFFFF" class="bottom">X &lt; 5.5</td>
</tr>
</table></div>
<p>The following example illustrates the use of <sup>1</sup>/<sub>2</sub> in this way.</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_binomExample_b.gif" width="550" height="272"></p>





<h2 class="pageName">9.5.5 &nbsp; Normal approximation to binomial</h2><!DOCTYPE HTML>


<p class="heading">Mean and standard deviation of <em>x</em> and <em>p</em></p>
	<p>The mean and standard deviation are given below for the proportion of successes <span class="em black">p</span> , and number of successes,<span class="em black"> x</span> <span class="black">&nbsp;=&nbsp;<em>np</em></span></p>
	<p class=eqn><img class="gif" src="../../../en/randomPropn/images/normalApprox.gif" id="gif_image_5_5_1" width="389" height="126"><iframe class="svg" src="../../../en/randomPropn/images/normalApprox.svg" id="svg_image_5_5_1" width="389" height="126" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_5_1");</script> </p>
	<p>The fact that both <span class="em black">x</span>  and <span class="em black">p</span>  are approximately normally distributed in large samples is justified below.</p>
	<p class="heading">Proportions and means</p>
	<p>If we assign a code of '1' to the successes and '0' to the failures in the 
		random sample, then the resulting values are called an <strong>indicator variable</strong>. Its mean is identical to the proportion of successes. </p>
	<p class=eqn><img class="gif" src="../../../en/randomPropn/images/propnAsMeanEqn.gif" id="gif_image_5_5_2" width="397" height="100"><iframe class="svg" src="../../../en/randomPropn/images/propnAsMeanEqn.svg" id="svg_image_5_5_2" width="397" height="100" frameborder="0"></iframe><script type="text/javascript">showCorrectImage("image_5_5_2");</script> </p>
	<p>Since the proportion of successes in a sample is a kind of mean, its distribution is close to a normal distribution if the sample size is large enough.<br>
	</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_normalApprox.gif" width="502" height="375"></p>




<h2 class="pageName">9.5.6 &nbsp; Normal approximation examples</h2><!DOCTYPE HTML>


<p class="heading notPrinted">Use of the normal approximation to the binomial distribution</p>
	<p>To avoid adding large numbers of binomial probabilities, the normal approximation can be used to find the probability that a binomial variable is within a certain range when the sample size, <span class="em black">n</span> , is large.</p>
	<p>A common rule-of-thumb for when this kind of normal approximation can be used is:</p>
	
	<div class="centred"><div class="boxed percent50">
		<p><em>n</em>π &gt; 5 &nbsp; &nbsp;and &nbsp; &nbsp; <em>n</em>(1-π) &gt; 5 </p>
	</div></div>
	
	<p>An example is given below:</p>
	<p class="eqn"><img src="../../../en/randomPropn/images/s_normalApproxEx_b.gif" width="550" height="460"></p>
	<p>Note the translation of the range of values into one involving <sup>1</sup>/<sub>2</sub>. It is called a <strong>continuity correction</strong> in this context.</p>





</html>
